[["index.html", "Urban Transportation Planning Foreword", " Urban Transportation Planning Gregory Macfarlane, PhD, PE 2021-10-26 Foreword This book contains course notes and assignments for a senior / graduate class in transportation planning and elementary travel modeling. A description for this course is: An advanced course in urban transportation planning. Urban transportation as the outcome of an economic system, details and techniques for four-step travel model development, applications of travel models within a legal and regulatory context. The book is organized into six units: Building Blocks Trip Generation Trip Distribution Mode and Destination Choice Network Assignment and Validation The Planning Process It may seem strange to put the chapter covering the planning process at the end of the course, after students have learned the details of quantitative travel modeling. The purpose for this is that I assign a term project where the students build and calibrate a four-step model as they learn the techniques to do so, and then complete an alternatives analysis using their models. To create the time and space to do this project, we cover “softer” and conceptual topics in the second half of the course. The demonstration model the students calibrate and study is a model built in the Cube travel modeling software for the Roanoke, Virginia, metropolitan region. The model is a relatively advanced four-step, trip-based model with only 250 zones. The limited zone size means that the entire model system runs in approximately 15 minutes on a laptop computer. I am grateful to Virginia DOT for allowing my students the use of this model. Directions on how to use the Roanoke model are given in the Appendices. A handful of assignments require the students to write numerical programs or estimate statistical models. Some guidance on using R and RStudio to accomplish these assignments is also given in the Appendices. Acknowledgements Photographs in the textbook are the work of the author unless otherwise attributed. The vector art in the textbook uses icons from FontAwesome and the Noun Project distributed under creative commons licenses. Specific attributions are below: training wheels by Marco Fleseri from the Noun Project Cover image: TRAX by Ashton Bingham on Unsplash "],["chap-blocks.html", "Chapter 1 Building Blocks 1.1 Planning for Human Systems 1.2 The Four-Step Transportation Planning Process 1.3 Travel Model Building Blocks 1.4 Data Inputs 1.5 Statistical and Mathematical Techniques Homework Lab", " Chapter 1 Building Blocks This chapter contains concepts, definitions, and mathematical techniques that will be used throughout the semester. Critical terms to understand are given in bold. 1.1 Planning for Human Systems If you look out on any sufficiently busy road, you will see a steady stream of vehicles passing by. Each vehicle is largely indistinguishable from the others, and it is easy as an engineer responsible for that road to see the cars driving by as little more than an input to a problem. But the people inside the cars should not be indistinguishable from each other. Each person who is driving or riding in each of those cars has their own reasons to be driving on that road. One person might be driving to work; one person might be trying to get home to his or her family. Another car might hold a family going on vacation, or a group of friends heading to a movie. If you don’t recognize that each person who travels is different, with different needs and purposes, then it is easy to look only at the supply of transportation infrastructure. Is the road wide enough? Is the traffic signal timed appropriately? But as with anything in the economy transportation is a function of both supply and demand. Why are so many people trying to get down this one road right now? Why didn’t more people take transit? Why didn’t some people choose a different destination? Or why didn’t some people just stay home in the first place? Transportation planning therefore must be concerned with both the supply of infrastructure and the demand for travel. For the most part, economists consider travel a derived demand, which means people only go to the hassle of traveling somewhere if they have some other reason to be there. No one typically just drives around (with the possible exception of teenagers on a weekend night); they are going to work, or school, or a social engagement, or something. Travel demand has not been stable over time. The availability of inexpensive automobiles in the 20th Century created demand for inter-city and intra-urban roads that did not exist before. Rising labor force participation rates for women radically changed the number and types of trips the average household makes in an average day. Technological developments like teleconferencing and smartphone-enabled ridehailing could generate different trends. At the same time, populations in most regions continue to grow. Planning for future transportation infrastructure is difficult because of the uncertainty of the future, but it is necessary to keep economies rolling and preserve or improve quality of life. In the United States and most societies with some democratic process, decisions about what transportation facilities to build, which policies to implement, and how to build a city generally fall to decision makers. These decision makers consist of mayors, city councils, planning commissions, state legislatures, Congress, state and federal agencies, and innumerable others who are elected by the public, or who are accountable to others who have been. In making decisions about how to spend public money on civil infrastructure or enact tax or other policies, decision makers consult plans developed by professional engineers and planners. As engineers and planners, we are rarely in a position to make decisions, but we have a responsibility to provide accurate data and technical analysis to support decision makers. There is a misconception that transportation planners must accurately predict the future to be relevant. The purpose of transportation planning is not to perfectly envision what will happen under every scenario, it is to provide information that will help make good decisions now so that the future is at least as pleasant as the present. We all have hopes for what our lives and community will look like ten or twenty years from now; it may not be possible for anyone to provide analysis entirely free of all personal bias. But as you conduct your work as an engineer and planner, you owe the public your integrity and competence as you provide information to their representatives. 1.2 The Four-Step Transportation Planning Process How can you know what might happen in the future? And how might that change based on decisions that you make today? This basic question is at the heart of transportation planning: What might traffic look like if we build nothing and population still grows? Can we build less if we change land development patterns? How many people will use this new transit line? In many fields — including politics, meteorology, economics, etc. — professionals who seek answers to questions like this do so with the help of a model. A model is a mathematical representation of a real-world system. In any model, there are things that need to vary (called inputs), things that can be estimated or calibrated (called parameters), and results (called outputs). There are also things that are held fixed. The specific mathematical structure of the model, and which things get included and which things are excluded or abstracted away, determine what the model should and should not be used for. For example, we might try to predict something with a linear model, \\[\\begin{equation} y = \\alpha + \\beta x + \\varepsilon \\tag{1.1} \\end{equation}\\] In this case \\(y\\) is the output, \\(x\\) is the input, the \\(\\beta\\) parameter defines how the input influences the outcomes, \\(\\alpha\\) is a fixed value, and \\(\\varepsilon\\) accounts for the random influence of all the factors we did not include. We could add more \\(\\beta\\) and \\(x\\) terms to include more factors in the model. We could also change the mathematical format of the model to represent different types of outcomes, or chain several smaller models together to represent more complex relationships. If we wanted to see what might happen if \\(x\\) changed, we could put in a new value into this equation and the output result \\(y\\) might be a plausible prediction. The plausibility of the output is a function of how well the mathematical model actually represents the reality of the system. In this class you will learn the details of the travel demand modeling process, which is a chain of many models, each with different inputs and outputs. A travel demand model on the whole has two basic inputs: Socioeconomic data representing where people live and work and go to school and do other things. Transportation network data representing the roads and transit services and other methods people use to get between their activities. The basic outputs of a travel demand model are transportation volumes and levels of service. There are many ways to design and build travel demand models, but the traditional way most regions in the United States approach travel demand models is through a four step, trip-based1 process. The four steps are: Trip Generation Trip Distribution Mode Choice Route Assignment A trip generation model determines how many trips are produced in each zone (neighborhood), and how many trips are attracted to each zone. The inputs to this model is the socioeconomic data in each zone. Mathematically, trip generation can be represented as \\[\\begin{equation} P_i = f(SE_i), A_j = f(SE_j) \\tag{1.2} \\end{equation}\\] where \\(i\\) and \\(j\\) are the production and destination zone indexes. A trip distribution model seeks to pair the productions and attractions in the zones based on the travel costs \\(c\\) between the two zones. Mathematically, trip distribution can be represented as \\[\\begin{equation} T_{ij} = f(P_i, A_j, c_{ij}) \\tag{1.3} \\end{equation}\\] A mode choice model estimates how many of the trips from \\(i\\) to \\(j\\) will happen on each available mode \\(k\\), based on the travel time by each mode and other attributes of the origin and destination zones. Mathematically, \\[\\begin{equation} T_{ijk} = f(T_{ij}, c_{ijk}, SE_{i,j}) \\tag{1.4} \\end{equation}\\] A route assignment model determines the specific routes that the trips going between \\(i\\) and \\(j\\) take. This allows us to estimate the volume of level of service on each highway link and transit system \\(l\\). Mathematically, \\[\\begin{equation} LOS_l, V_l = f(T_{ijk}, c_{ijk}) \\tag{1.4} \\end{equation}\\] On the whole, the travel demand model can be represented mathematically as a single function where the output transportation volumes and levels of service are a function of the input socioeconomic information and travel costs. \\[\\begin{equation} LOS_l, V_l = \\mathcal{F}(SE_{i,j}, c_{ij}) \\tag{1.5} \\end{equation}\\] The details of each of these models will be the topic for the next several chapters. 1.3 Travel Model Building Blocks In this section, we present some of the terms used in transportation planning and modeling, as well as some of the data objects used in constructing travel demand models. 1.3.1 Travel Analysis Zones and SE Data The “people” in a model conduct activities: work, school, recreation, and other activities. Because travel is a derived demand, the purpose of travel is to move between these activities. So a travel model needs a way to represent where the households, persons, jobs, and activities are located in space. Activities in travel demand models happen in Travel Analysis Zones (TAZs). The model tries to represent trips between the TAZs. Because trips inside a TAZ — called intrazonal trips — are not included in the travel model, each TAZ should be sufficiently small such that these trips do not affect the models’ ability to forecast travel on roadways. The following rules are helpful when drawing TAZ’s: The TAZ should not stretch across major roadways The TAZ should contain principally one land use, though in some areas this is not possible. In areas with more dense population, the TAZ should be smaller. Each TAZ is associated with socioeconomic (SE) data, or information about the people, businesses, and other activities that are located in the TAZ. Households are a basic unit of analysis in many economic and statistical analyses. A household typically consists of one or more persons who reside in the same dwelling. Individuals living in the same dwelling can make up a family or other group of individuals; that is, a group of roommates is considered a household. Not everyone lives in households, however; some people live in what are called group quarters: military barracks, college dormitories, monasteries, prisons, etc. Travel models need to handle these people as well, but in this class we will focus on people who live in households. Households in travel models are often grouped into a classification scheme based on the number of people in the household, the number of children, the number of vehicles, etc. Households of different classifications will have different behavior in the rest of the model. Your lab activity for this unit will walk you through specifying a household classification model. Firms are another basic unit of analysis in many economic and statistical analyses. A firm is a profit-seeking person or entity that provides goods or services in exchange for monetary transactions. A firm can provide raw resources, manufactured resources, other services, or be a place of employment. In some cases, a firm may be another household. Each firm will have an industry type. Examples of industry types include office, service, manufacturing, retail, etc. In many SE data files, firms are simply represented as the total number of jobs in a TAZ belonging to each industry. Other Institutions including academic, government, and non-profit entities will also be represented in the SE data in terms of their jobs. It is important to be precise in our definitions when put all of these different items into an analysis. A typical socioeconomic data table for a small region is given in Table 1.1. Note the following relationships: Persons live in Households Workers are Persons who have a Job Firms have employees who work at a Jobs Figure 1.1: Travel Analysis Zones in Central Roanoke. When we talk about “how many jobs” are in a TAZ, we mean “How many people do the firms located in that TAZ employ,” and not “how many people who live in that TAZ are workers.” Table 1.1: Example SE Table taz persons hh workers retail office manufacturing 1 44 25 22 129 96 2 2 45 47 21 121 81 11 3 32 35 17 148 89 0 Alice lives with her husband in zone \\(A\\) and works as an accountant in zone \\(B\\). Her husband does not currently work. Fill out the SE table from Table 1.1 with just this household’s information. Two persons live in one household with one worker in zone A. The firm Alice works at has an office job for her in Zone B. taz persons hh workers retail office manufacturing A 2 1 1 B 1 1.3.2 Transportation Networks The purpose of a travel model is to understand how people are likely to use transportation infrastructure, so there has to be a way to represent roadway and transit systems. We do this with a network2. A network consists of two basic data structures: Nodes and Links Nodes are points in space. In a highway network, almost all nodes represent intersections between different roads. Some important nodes represent the TAZ Centroids, or the points where the households and jobs in the travel model are located. Links connect nodes, and represent roads. Links have many different attributes describing the characteristics of the roadway they represent. The two most important link attributes are the link’s speed and capacity, because they provide the travel costs (\\(c_{ij}\\) above) to the various steps of the model. But these attributes might not always be known at the outset, so instead we use attributes of the roadway that influence capacity and speed, and then calculate these other values. Functional Type or Functional Class describes the relative role each road plays in the transportation system (A Policy on Geometric Design of Highways and Streets, 7th Edition, 2018). Every street fills a role on a spectrum from mobility on one end to accessibility on the other: roads that are good at moving high volumes of vehicles are usually not good at providing access to homes and businesses. Common functional types include: Freeways are provided almost exclusively to enhance mobility for through traffic. Access to freeways is provided only at specific grade-separated interchanges, with no direct access to the freeway from adjacent land except by way of those interchanges. The primary function of major and minor arterials is to provide mobility of through traffic. However, arterials also connect to both collectors and local roads and streets and many arterials provide direct access to adjacent development. Major and minor collectors connect arterials to local roads and provide access to adjacent development. Mobility for through traffic is less important. Local streets exist primarily to serve adjacent development. Mobility for through traffic is not important, or even desired. Figure 1.2 shows roads in Provo and Orem classified by this scheme. Streets of a functional class below collector are almost never included in travel models, unless they provide essential connectivity between other roads. Entire neighborhoods of local streets may be represented by just a handful of special links called centroid connectors. Figure 1.2: UDOT Functional Classes. Why are local roads not included in travel models? Free-flow speed is the speed vehicles travel when the road is empty. Historically, travel modelers would use formulas in the Highway Capacity Manual to estimate the free-flow speed for roadways, or assert a basic calculation like 5 miles per hour over the speed limit. More recently, modelers use the speeds reported from GPS devices in the middle of the night to establish free-flow speeds. The number of lanes on a road is fairly self-explanatory, but it plays a major role in the road’s capacity. Roads located in different area types – urban, suburban, and rural – operate differently from each other. Sometimes travel models will assert this value, but more recent models will calculate the area type for each link based on the density of the surrounding TAZs. Link capacity is the maximum number of vehicles a can optimally transport between two nodes. The capacity is a function of functional type, lanes, free-flow speed, area type, etc. Usually travel models will calculate the capacity based on the given values for other roadway characteristics, but sometimes there are ways to override this feature, i.e., if engineers have developed specific capacity estimates for a new project. Centroid connectors are special links that connect centroids to a network. These are different from other links in that they usually don’t have a capacity or a speed (they don’t represent real roads). 1.3.3 Matrices Travel models need to represent travel times, costs, and flows between zones. Models store this data in matrices, special data structures developed for this purpose. Each matrix is a square table where the rows \\(i\\) represent origin zones and the columns \\(j\\) represent the destination zones. Each cell represents something about the relationship between the two zones. There are two kinds of information we typically represent with matrices: Cost matrices, or skims, are matrices where the cells contain estimates of travel time or cost. They are called skims because they are the results of skimming a network to find the shortest path between each pair of TAZ centroids. Flow matrices, represent flows of people or vehicles from each origin to each destination. The number in the corresponding cell \\(T_{ij}\\) is the total number of trips made, and represents the demand between two zones in a network. 1.4 Data Inputs In the last section we discussed data structures like highway networks and socioeconomic data files. In this section, we are going to talk about the data inputs that can be used for developing travel models. Besides highway networks — which usually have to be supplied by the transportation agency — modelers frequently gather data from household travel surveys and the US Census Bureau. Obtaining an accurate highway network is one of the most difficult tasks in travel modeling. It’s not conceptually or intellectually difficult, but it is very difficult to map model networks onto the linear referencing systems or GIS files used by other agency departments. This is made even more difficult by model networks needing to be routeable: common GIS formats like shapefiles have no way of representing routability. 1.4.1 Household Travel Surveys Travel demand models try to represent individual behavior. How many trips does the average household make per day? How do people respond to changes in transit fare? And how can a modeler know if the model accurately reflects total traffic? Household travel surveys are a critical component of much travel modeling practice and research, and are a primary way to answer some of these questions. In a travel survey, a regional planning agency3 will recruit households to participate in the survey. Often there is some kind of reward to encourage participation, like a gift card or raffle. Once recruited, household members fill out a diary of their activities on an assigned day; Figure 1.3 shows an example of one activity from a survey diary. From the example, you can see the kinds of data that are available: where the person traveled, which travel mode they used, and what was their reason for making the trip. Figure 1.3: Example travel survey diary entry. Not all travel surveys are filled in on forms; nowadays telephone interviews or mobile applications are more common (more on that below). But for decades, paper travel surveys were the basis of almost all transportation behavior science. Once the surveys are collected, the data is usually processed into several tables stored in different files or a database. A Households table has one row for each household in the dataset, including information about the number of people in the household, the number of vehicles, and the household income. A Persons table has one row for each person in the dataset — including which household they are a part of (to link with the households table) — and personal attributes like age, student or worker status. A Vehicles table has one row for each vehicle owned by the households in in the dataset, including attributes like model year, vehicle class, and fuel efficiency. A Trips table has one row for each trip taken by each person in the dataset. This table can be linked against the other tables if necessary, and contains information like the trip purpose and many other elements collected with the form in Figure 1.3. Tables 1.2 through 1.5 show data collected from one household in the 2017 National Household Travel Survey. The household contains four people, two of whom are working adults in their late thirties. (the other two are children, and the NHTS did not collect their trip data). The household has two vehicles, and on the survey travel day person 2 appeared to make a few very long trips. It’s impossible to know if this is a typical day for this person or not, but that’s the data that was collected. Table 1.2: NHTS Households File houseid hhsize numadlt wrkcount hhvehcnt hhfaminc wthhfin 30000082 4 2 2 2 $100,000 to $124,999 1148.809 Table 1.3: NHTS Persons File houseid personid r_age educ r_sex 30000082 01 39 Graduate degree or professional degree Female 30000082 02 38 Bachelor’s degree Male Table 1.4: NHTS Vehicles File houseid vehid vehyear make model fueltype od_read 30000082 01 2011 Mazda Mazda3 Gas 83644 30000082 02 2007 Toyota Yaris Gas 120615 Table 1.5: NHTS Trips File houseid personid strttime endtime trpmiles trptrans trippurp 30000082 01 2017-10-10 07:45:00 2017-10-10 07:52:00 2.710 Car Home-based trip (other) 30000082 01 2017-10-10 08:09:00 2017-10-10 08:13:00 1.432 Car Not a home-based trip 30000082 01 2017-10-10 08:24:00 2017-10-10 08:28:00 0.777 Car Not a home-based trip 30000082 01 2017-10-10 16:53:00 2017-10-10 16:57:00 1.075 Car Not a home-based trip 30000082 01 2017-10-10 17:18:00 2017-10-10 17:26:00 2.727 Car Home-based trip (other) 30000082 02 2017-10-10 07:30:00 2017-10-10 07:33:00 2.136 Car Home-based trip (shopping) 30000082 02 2017-10-10 07:38:00 2017-10-10 08:50:00 88.581 Car Not a home-based trip 30000082 02 2017-10-10 08:58:00 2017-10-10 09:49:00 45.341 Car Not a home-based trip 30000082 02 2017-10-10 10:51:00 2017-10-10 12:24:00 28.208 Car Not a home-based trip 30000082 02 2017-10-10 17:00:00 2017-10-10 17:05:00 0.239 Walk Not a home-based trip 30000082 02 2017-10-10 19:15:00 2017-10-10 19:26:00 0.267 Walk Not a home-based trip 30000082 02 2017-10-10 19:30:00 2017-10-10 20:43:00 29.293 Car Not a home-based trip Note that that the households data in Table 1.2 contains a numeric column called wthhfin. This is a survey weight. Because it is impossible to sample everyone in a population, there needs to be a way to expand the survey to the population. What this number means is that the selected household carries the same weight in this survey as approximately 1100 households in the general population. Also note that not every household’s weight will be equal; because some population groups have different survey response weights, some households will need to be weighted more heavily so that the survey reflects the general population. Most software packages have functions that allow you to calculate statistics or estimate models including weighted values. The code chunk below shows how to calculate the average number of workers per household with and without weights in R; as you can see, omitting the weights leads to a substantial change in the survey analysis. # Average workers per household with no weights mean(nhts_households$wrkcount) ## [1] 0.9891438 # Average workers per household, weighted weighted.mean(nhts_households$wrkcount, nhts_households$wthhfin) ## [1] 1.173206 Travel survey methodology is changing rapidly as a result of mobile devices with location capabilities. First, most travel surveys are now administered through a mobile application: respondents are invited to install an app on their smartphone that tracks the respondent’s position and occasionally asks questions about trip purpose or mode. This makes collecting and cleaning data considerably easier than traditional paper surveys, and it also lowers the response burden for the survey participants. Another change that mobile data has brought to travel surveys is the introduction of large datasets of location information that planners can purchase directly from cellular providers or third-party providers. Though these data do not have all the information on demographics and preferences a survey would provide, they provide a considerably larger and more detailed sample on things like overall trip flows. As a result, it may be possible to collect surveys less frequently, or to reduce survey sample sizes. 1.4.2 US Census Bureau The primary statistical agency of the United States is the U.S. Census Bureau, called Census. The need to collect statistics is established in the Constitution, Representatives and direct Taxes shall be apportioned among the several States which may be included within this Union, according to their respective Numbers… The actual Enumeration shall be made within three Years after the first Meeting of the Congress of the United States, and within every subsequent Term of ten Years, in such Manner as they shall by Law direct. Since the first census in 1790, Census has collected more data than simply the number of people in each state. Current programs that are especially important for travel modeling and other related demographic research include: The Decennial Census of Population and Housing is the thing most people think of when they think of Census. Every ten years (most recently in 2020), Census attempts to collect the number, age, and other key population attributes for every dwelling unit in the United States. The American Community Survey (ACS) is an annual 1% sample of the US population. This survey is considerably more detailed than the decennial census, and asks questions regarding the education and income of household members, how each worker traveled to work, whether people own or rent their home, etc. The ACS is a particularly useful data set, especially because the decennial census can become outdated as ten years go by between collections. To protect the individual identity of ACS respondents, Census engages in a number of different schemes. The simplest scheme is data aggregation. ACS data is usually obtained as tables representing the number of individuals or households in a geographic area that match a certain category. The data is aggregated in two ways: First, large geographic areas are aggregated each year, meaning that up-to-date numbers are always available; Second, smaller geographic areas contain groups of records collected over the last five years. In this way there is a tradeoff between temporal and spatial precision that the researcher needs to consider. Another basic scheme is top-coding, where numeric variables are capped at a common value. The ACS will report how many people in a neighborhood have an income over $250,000, but not how many have an income over $1 million. Census will also suppress small counts in a category; they will not reveal how many households have 8, 9, or 10 people, instead collapsing all of these households into a “seven or more” group. If too few individuals or households match that category, Census does not provide a count. For example, if only one household in a neighborhood makes more than $250,000, the ACS table for that cell will contain no information. That could mean there are zero, one, four, or some other small number of households in that category. Besides tables, the Census releases the ACS Public Use Micro-Sample (PUMS) containing ACS responses as disaggregate microdata. These data are geographically located to much larger areas than other ACS records, and Census does imputation and data swapping on the records to ensure that private information cannot be disclosed. But studies conducted on ACS PUMS records reach the same statistical conclusions as studies conducted on the unmodeled and raw data, making PUMS a useful tool in studying populations. 1.4.2.1 Geographies Census data are given in a geographical heirarchy: State County Tract Block Group Block The bottom three layers are shown in Figure 1.4. Each layer nests completely within the layer above it. More detailed data is available at less spatially detailed geographies. Figure 1.4: US Census Geographies in Central Provo. 1.5 Statistical and Mathematical Techniques Many elements of travel modeling and forecasting require complex numerical and quantitative techniques. In this section we will present some of these techniques. Many of the data tables are in the nhts2017 package. To install this package, follow the directions in the Appendix. 1.5.1 Continuous and Discrete Distributions In general, statistical variables can fall into one of two categories: Continuous variables can take any numeric value along some range Discrete variables can take some limited set of predetermined values A simplistic definition would be to say that continuous variables are numeric and discrete variables are non-numeric. A continuous variable has statistics such as a mean, but these statistics do not make sense on discrete variables. In the NHTS trips dataset, we can compute a mean trip miles, but we cannot compute a mean trip purpose. Or we can’t compute a mean that makes sense. # mean of continuous variable: trip length weighted.mean(nhts_trips$trpmiles, nhts_trips$wttrdfin) ## [1] 10.69119 # mean of categorical variable: trip purpose weighted.mean(nhts_trips$trippurp, nhts_trips$wttrdfin) ## Error in x * w: non-numeric argument to binary operator What we can do, however, is we can print a summary table showing the number of observations that fit in each trip purpose category. Note that sometimes there will be a category devoted to data that is missing or otherwise invalid. table(nhts_trips$trippurp) ## ## -9 HBO HBSHOP HBSOCREC HBW NHB ## 32 190022 195188 110235 117368 310727 Sometimes it is handy to split a continuous variable into categories so that you can treat it as a discrete variable. nhts_trips$miles_cat &lt;- cut(nhts_trips$trpmiles, breaks = c(0, 10, 20, 30, 50, 100, Inf)) table(nhts_trips$miles_cat) ## ## (0,10] (10,20] (20,30] (30,50] (50,100] (100,Inf] ## 719812 113383 38724 25064 14388 11060 When we visualize the distribution of a continuous variable, we might use a histogram or density plot, but with a discrete variable we would use a bar chart. ggplot(nhts_trips, aes(x = trpmiles, weight = wttrdfin)) + geom_histogram() + xlab(&quot;Trip Distance [Miles]&quot;) + ylab(&quot;Weighted Trips&quot;) + scale_x_continuous(limits = c(0, 50)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 1.5: Visualizing a continuous distribution with a histogram. ggplot(nhts_trips, aes(x = as_factor(trippurp, levels = &quot;labels&quot;), weight = wttrdfin)) + geom_bar() + xlab(&quot;Trip Purpose&quot;) + ylab(&quot;Weighted Trips&quot;) Figure 1.6: Visualizing a discrete distribution with a bar chart. To this point we’ve only looked at the distribution of one variable at a time. There are lots of cases where someone might want to consider the joint distribution of two variables. This joint distribution tells you what is happening with one variable while the other variable changes. In a table like the one below, the margins of the table (the row and column sums) contain the single variable distribution. So sometimes we call these the marginal distributions. table(nhts_trips$miles_cat, nhts_trips$trippurp) ## ## -9 HBO HBSHOP HBSOCREC HBW NHB ## (0,10] 23 156315 162602 84980 67162 248730 ## (10,20] 6 20856 19881 13054 28018 31568 ## (20,30] 1 5635 5469 4361 12087 11171 ## (30,50] 0 3592 3427 3267 7117 7661 ## (50,100] 2 1943 2150 2504 2332 5457 ## (100,Inf] 0 1231 1634 1844 597 5754 We can visualize joint distributions as well, and sometimes the results are quite nice. 1.5.2 Iterative Proportional Fitting There are times when we know two marginal distributions but do not know the joint distribution. This can happen for a number of reasons: We know how many households of different sizes and workers, but not how many large households have multiple workers. We have a forecast of truck volumes at external roads, but do not know how many trucks go between the roads. In these situations, a convenient technique is iterative proportional fitting. This technique updates a joint distribution to match two or more marginal distributions within a particular tolerance. IPF is complicated to explain but easy to demonstrate, so let’s go straight into an example. Let’s say we have a forecast for AADT at three external stations in the future. We can assume that the two-way AADT is roughly even in each direction, so the inbound volume at each station is half the AADT. Let’s also say we have an estimate of where the trips coming at those stations today go today. This matrix is called a seed. # AADT projections volumes &lt;- tibble( Station = LETTERS[1:3], Volume = c(20000, 30000, 35000), AADT = Volume * 2 ) volumes ## # A tibble: 3 x 3 ## Station Volume AADT ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 20000 40000 ## 2 B 30000 60000 ## 3 C 35000 70000 # observed distribution for a seed seed &lt;- matrix(c( 0, 7501 ,12500, 8956, 0, 11879, 9146, 21044, 4687), nrow = 3, ncol = 3, byrow = TRUE) rownames(seed) &lt;- colnames(seed) &lt;- volumes$Station seed ## A B C ## A 0 7501 12500 ## B 8956 0 11879 ## C 9146 21044 4687 Note that the row and column sums of the matrix do not match the forecast. But we can multiply each row in the matrix by the new volume estimate and the proportion of the seed matrix row that is in that cell. This gives us a new estimate of the cell’s value. Mathematically, \\[\\begin{equation} S_{n+1, ij} = \\frac{m_i * S_{n, ij} }{\\sum_{i}S_{ij}} \\tag{1.6} \\end{equation}\\] Where \\(S\\) is the seed matrix and \\(m\\) is the marginal vector. We then repeat with the other marginal, \\[\\begin{equation} S_{n+2, ij} = \\frac{m_j * S_{n, ij} }{\\sum_{j}S_{ij}} \\tag{1.7} \\end{equation}\\] In this example case, the first row is \\[\\begin{align*} S_{1, 1 1} &amp;= 20000 * 0 / 20001 &amp;= 0\\\\ S_{1, 1 2} &amp;= 20000 * 7501 / 20001 &amp;= 7500.62\\\\ S_{1, 1 2} &amp;= 20000 * 12500/ 20001 &amp;= 12499.38\\\\ \\end{align*}\\] And the whole row iteration is # get factor to multiply each row by row_factor &lt;- volumes$Volume / rowSums(seed) # multiply across rows, see ?sweep() sweep(seed, 1, row_factor, &quot;*&quot;) ## A B C ## A 0.000 7500.625 12499.38 ## B 12895.608 0.000 17104.39 ## C 9178.255 21118.215 4703.53 We can write a function that does a complete round of row, then column fitting. ipf_round &lt;- function(marginal1, marginal2, seed) { # multiply the first marginal through the rows (MARGIN = 1) seed_rows &lt;- sweep(seed, MARGIN = 1, marginal1 / rowSums(seed), &quot;*&quot;) # multiply the second marginal through the columns (MARGIN = 2) seed_cols &lt;- sweep(seed_rows, MARGIN = 2, marginal2 / colSums(seed_rows), &quot;*&quot;) # return seed_cols } ipf_round(volumes$Volume, volumes$Volume, seed) ## A B C ## A 0.000 7862.609 12751.752 ## B 11684.052 0.000 17449.749 ## C 8315.948 22137.391 4798.499 If we repeat this process for several iterations, we can see that the change between successive values shrinks. We can use this change to set a tolerance for when we want the process to stop. change &lt;- vector(&quot;numeric&quot;) joint &lt;- seed for(i in 1:5){ # update joint table new_joint &lt;- ipf_round(volumes$Volume, volumes$Volume, joint) # calculate absolute error at this iteration print(sum(abs(new_joint - joint))) # update joint joint &lt;- new_joint } ## [1] 10947.1 ## [1] 1550.274 ## [1] 325.7515 ## [1] 109.2708 ## [1] 37.10879 # final estimate new_joint ## A B C ## A 0.000 7748.922 12251.164 ## B 11945.229 0.000 18047.050 ## C 8054.771 22251.078 4701.787 A few notes: IPF is not guaranteed to progressively converge. Meaning, it is possible to get stuck in a loop where the successive change between iterations does not get smaller. It is important to set a maximum number of iterations. IPF can work in any number of dimensions. A two-dimensional matrix is easy to visualize and works as a good example, but that’s by no means an upper limit. Just keep repeating. If a cell in a seed matrix has a zero value, all successive iterations will have zero. If the seed table has a structural zero, keep it in. Otherwise, you might want to consider overriding the value with a small number. The process is not consistent: A different seed matrix will lead to a different outcome. If you are uncertain about your seed matrix, you could consider taking the average of multiple potential seed matrices. 1.5.3 Regression Analysis Consider the basic model in (1.1). This is a linear model, meaning that it creates a line that minimizes the error between the points. Figure 1.7 shows the linear relationship between engine displacement (in cubic centimeters) and engine miles per gallon in a sample of cars. ggplot(mtcars, aes(y = mpg, x = disp)) + geom_point() + stat_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + theme_bw() ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 1.7: Relationship between engine displacement and miles per gallon. How do we calculate this line? In R, the function to compute a basic linear regression is lm(). So to regress the mpg variable on the disp variable, # y ~ x + x1 + others, from data lm1 &lt;- lm(mpg ~ disp, data = mtcars) # print summary summary(lm1) ## ## Call: ## lm(formula = mpg ~ disp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.8922 -2.2022 -0.9631 1.6272 7.2305 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.599855 1.229720 24.070 &lt; 2e-16 *** ## disp -0.041215 0.004712 -8.747 9.38e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.251 on 30 degrees of freedom ## Multiple R-squared: 0.7183, Adjusted R-squared: 0.709 ## F-statistic: 76.51 on 1 and 30 DF, p-value: 9.38e-10 What this suggests is that for every additional cc of displacement, the engine miles per gallon changes by -0.0412151 miles per gallon. But how does R calculate this? Assume \\(y = X\\beta + \\epsilon\\), we want the estimate \\(\\hat{\\beta}\\) that minimizes the total error \\(\\epsilon = y - X\\hat(beta)\\). We can measure this total error as the “sum of squared residuals” \\[\\begin{equation} SSR(\\hat{\\beta}) = \\sum(y - X\\hat{\\beta})^2 \\tag{1.8} \\end{equation}\\] If we take derivative of this with respect to \\(\\hat(\\beta)\\), set to zero, and solve for \\(\\hat{\\beta}\\), we obtain the following estimate: \\[\\begin{equation} \\hat{\\beta} = (X&#39;X)^{-1}X&#39;y \\tag{1.9} \\end{equation}\\] We can calculate this manually in R, and verify that it matches the estimates obtained through lm(). y &lt;- mtcars$mpg X &lt;- model.matrix(mpg ~ disp, data = mtcars) (XTX1 &lt;- solve(t(X) %*% X)) ## (Intercept) disp ## (Intercept) 0.1430397593 -4.845217e-04 ## disp -0.0004845217 2.100025e-06 (beta &lt;- XTX1 %*% t(X) %*% y) ## [,1] ## (Intercept) 29.59985476 ## disp -0.04121512 1.5.3.1 Variance and Confidence Tests Because \\(\\hat{\\beta}\\) is an estimate rather than a population statistic, we have to consider that the estimate might be erroneous. That is, we need to construct a hypothesis test, \\[\\begin{align*} h_0:&amp; \\beta = 0\\\\ h_a:&amp; \\beta \\neq 0\\\\ \\end{align*}\\] Let’s look at the variance of our least-squares estimates: \\[\\begin{align*} \\hat{\\beta} &amp;= (X&#39;X)^{-1}X&#39;(X\\beta + \\epsilon)\\\\ \\hat{\\beta} &amp;= \\beta + (X&#39;X)^{-1}X&#39;\\epsilon\\\\ Var(\\hat{\\beta}) &amp;= 0 + (X&#39;X)^{-1}X&#39;\\Sigma_{\\epsilon} X(X&#39;X)^{-1}\\\\ \\end{align*}\\] If we assume that \\(\\epsilon\\) is independently and identically distributed (IID) with mean 0 and variance \\(\\sigma^2\\), \\[\\begin{align*} \\Sigma_\\epsilon &amp;= \\sigma^2I\\\\ Var(\\hat{\\beta}) &amp;= 0 + (X&#39;X)^{-1}X\\sigma^2X(X&#39;X)^{-1}\\\\ &amp;= \\sigma^2(X&#39;X)^{-1}X&#39;X(X&#39;X)^{-1}\\\\ &amp; = \\sigma^2(X&#39;X)^{-1} \\end{align*}\\] What does IID mean? It means that the distribution of \\(\\epsilon\\) does not depend on the values of \\(X\\). e &lt;- y - X %*% beta s &lt;- sum(e^2 / (nrow(X) - ncol(X))) ## SSR / degrees of freedom (se &lt;- sqrt(diag(s * XTX1))) ## (Intercept) disp ## 1.229719515 0.004711833 So, let’s talk about confidence tests using our new measure of the distribution of \\(\\hat{\\beta}\\). What is the probability of observing an estimate at least this extreme given the null hypothesis that the true value is 0? The \\(t\\)-statistic is the quotient of \\(\\beta\\) and its standard error, with the \\(p\\)-value determined based on the degrees of freedom of the model. (t = beta / se) ## [,1] ## (Intercept) 24.070411 ## disp -8.747152 # two-tailed P test with certain degrees of freedom 2*(pt(abs(t), nrow(X) - ncol(X), lower.tail = FALSE)) ## [,1] ## (Intercept) 3.576586e-21 ## disp 9.380327e-10 Of course, this test is only valid if the errors are IID. When you plot the results of a linear model, you get a series of diagnostic plots. The first one is a plot of the residuals at different values of \\(X\\beta\\), which allows you to visually inspect whether the IID assumption is valid. plot(lm1, which = 1) So, what can you do when the IID assumption is violated? Well, you could try to transform the data. What if the inverse of displacement is what matters? # transform the data lm2 &lt;- lm(log(mpg) ~ I(1 / disp), data = mtcars) plot(lm2, which = 1) There might also be other variables that matter. We could add the number of engine cylinders into our estimation. Both of these are better than the original model. lm3 &lt;- lm(mpg ~ disp + cyl, data = mtcars) plot(lm3, which = 1) 1.5.4 Numerical Optimization Let’s say you have a function with a Homework Some of these questions require a completed run of the demonstration model. For instructions on accessing and running the model, see the Appendix With the TAZ layer and socioeconomic data in the demonstration model, make a set of choropleth and / or bar chart maps showing the following socioeconomic variables: total households household density (per acre) total jobs job density share of manufacturing vs office vs retail employment Compare your maps with aerial imagery from Google Maps or OpenStreetMap. Describe the spatial patterns of the socioeconomic data in the model region. Identify which zones constitute the central business district, and identify any outlying employment centers. With the highway network layer, create maps showing: link functional type; link free flow speed; and link hourly capacity. Compare your maps with aerial imagery from Google Maps or OpenStreetMap. Note that the hourly capacity is not on the input network, so you will need to use a either the loaded highway network that is output from the model, or an intermediate network after the initial highway capacity calculations. Identify the major freeways and principal arterials in the model region. Find the shortest free-flow speed path along the network between two zones. Then find the shortest distance path between the same two zones. Are the paths the same? Do the paths match what an online mapping service shows for a trip in the middle of the night? Open the highway assignment report, which shows vehicle hours and miles traveled by facility type. What percent of the region’s VMT occurs on freeways? What percent of the region’s lane-miles are freeways? Create a map of the highway links showing PM period level of service based on the volume to capacity ratios in the table below. How would you characterize traffic in Roanoke? Which is the worst-performing major facility? LOS V/C Color A &lt; 0.35 Blue B 0.35 - 0.54 LightBlue C 0.55 - 0.77 Green D 0.78 - 0.93 Yellow E 0.94 - 0.99 Orange F \\(\\geq\\) 1.00 Red Lab Demographers for the Commonwealth of Virginia — like those in other states — forecast a certain number of residents, households, and jobs for many years into the future. But this data alone is not sufficient: We believe that larger households will make more trips than smaller households. But we only know how many households and persons there are, and not how many households of each different size. For example, let’s say that we know there are three households in a zone and nine persons. Does that mean that there are three 3-person households? Or two one-person households and one 7-person household? The number and types of trips generated by these two different scenarios could be very different. A household classification model turns these raw counts of households and persons into distributions of households by size, number of workers, etc. The Roanoke classification model works in two steps: The model makes a guess at the distribution by multiplying the average persons, vehicles, or workers per household in a zone by a marginal distribution determined from Census tables. The model then adjusts this initial guess (using IPF) so that the joint distribution of households matches what is currently there. The version of the Roanoke model you have installed contains a classification model, but the marginal and joint distribution tables the model uses contain nonsense. Your task for this lab is to correct the values in these tables so that the model will calculate reasonable estimates of the current and future population. Marginal Distributions The United States Census Bureau attempts to count every person in the United States every ten years. It also surveys a 1% sample of the population each year for additional statistical questions (in a program called the American Community Survey), like income and vehicle ownership. In order to protect respondent’s privacy, Census makes different types of information available at different geographic levels. For every Census tract in the country, we can get these marginal distribution tables: Number of households by household size Number of households by workers per household Number of households by vehicles available. Using these data, we can calculate how the average distribution of households changes based on the average household size in the tract. Figure 1.8 shows precisely this distribution, using a cubic polynomial to fit the average distribution. Each set of four points represents one census tract; for instance, the lowest average household size is in tract 51770001100 with 1.37 persons per household on average. This tract shows 71% of its households having 1 member, 25% having 2, and about 2 percent each of 3 and 4+ person households. Figure 1.8: Distribution of households by size, based on average households size. For the most part the distributions make sense, with a few exceptions. It makes sense that the proportion of 1-person households decreases with average size, and that the proportion of 4-person households increases. Each of these curves turns in a weird direction at the end; however, the wide margin on the distribution means that we’re not really confident about where the actual proportion is. We’ll need to do some manual adjusting to these curves. I have provided you with initial marginal distribution curves for the three marginal distributions of interest. These files are available on Box, and the starting values are shown graphically in Figure 1.9. Figure 1.9: Raw marginal distribution curves from Roanoke region. Rules you need to follow when adjusting the curves: If the average household size is one, then 100% of households must have one person. The same rule applies to zero workers and zero vehicles. If the average household size is four, there will still be some households with only one, two, or three people. This is because the 4 category really contains all households with four or more members. The total proportions across all four marginal curves at each \\(x\\) value must sum to 1. Joint Distribution In the section above we created curves to get the marginal distribution of household size, vehicles, and workers based on the average in a zone. In order to make sure that the joint distribution of all of these variables is correct, we will use IPF with a joint table as the seed. Census does not independently publish 3-dimensional tables, but the Census Transportation Planning Package (CTPP) is a partnership between AASHTO and the Census Bureau and it publishes key tables not available in other places. The CTPP data is available through the AASHTO website, or by searching “CTPP.” Find table A112305 for the residence counties where your group members are from. What do you see in this table? What makes sense and what does not? I have already gathered the data and done some preliminary preparation for you to compute the shares that belong in the household seed file. Run the code below in an R session (with the tidyverse library loaded) to calculate the necessary shares. Follow along with the code to understand what is happening. raw_counts &lt;- read_csv(&quot;https://byu.box.com/shared/static/03t5b6g9cw59aroqxarst55t2khnhmcz.csv&quot;) ## Parsed with column specification: ## cols( ## RESIDENCE = col_character(), ## persons = col_character(), ## workers = col_character(), ## vehicles = col_character(), ## output = col_character(), ## count = col_number() ## ) joint_distribution &lt;- raw_counts %&gt;% # the data contain both &quot;estimates&quot; and &quot;margin of error&quot;. We only want to # keep the estimates rows. filter(output == &quot;Estimate&quot;) %&gt;% # The data records size as text, with `0 workers`. We want to extract the # numeric information from these fields. This line of code says, for the variables # persons, workers, and vehicles, take all non-numeric characters and replace # them with nothing. It looks like gibberish, but it came from lots of Stack # Overflow searching. I can&#39;t come up with this stuff from thin air, you know. mutate_at( c(&quot;persons&quot;, &quot;workers&quot;, &quot;vehicles&quot;), function(x) {as.numeric(gsub(&quot;([0-9]+).*$&quot;, &quot;\\\\1&quot;, x))} ) %&gt;% # the rows that say &quot;total households&quot; get turned to NA by the above process, and we # don&#39;t want to keep those in anyways. filter(!is.na(persons), !is.na(workers), !is.na(vehicles)) %&gt;% # the roanoke model only includes households with 3+ workers, not 4+. So if a # row is for more than 3 workers, we group it as 3 mutate(workers = ifelse(workers &gt; 3, 3, workers)) %&gt;% # but this creates a problem where we now have multiple rows with # 3 workers. So we need to group and add up. group_by(persons, workers, vehicles) %&gt;% summarize(count = sum(count)) %&gt;% ungroup() %&gt;% # Finally, we want to turn the numbers into a share. So we divde the # counts by the total count in the whole table. mutate(share = count / sum(count)) ## Warning in (function (x) : NAs introduced by coercion ## Warning in (function (x) : NAs introduced by coercion ## Warning in (function (x) : NAs introduced by coercion ## `summarise()` regrouping output by &#39;persons&#39;, &#39;workers&#39; (override with `.groups` argument) Now we can see the joint distribution of size, workers, and vehicles. joint_distribution %&gt;% group_by(persons) %&gt;% select(-count) %&gt;% pivot_wider(names_from = vehicles, values_from = share, names_prefix = &quot;vehicles &quot;) %&gt;% knitr::kable() persons workers vehicles 0 vehicles 1 vehicles 2 vehicles 3 1 0 0.0424577 0.0937149 0.0193590 0.0040148 1 1 0.0129243 0.1195635 0.0273885 0.0073146 1 2 0.0000000 0.0000000 0.0000000 0.0000000 1 3 0.0000000 0.0000000 0.0000000 0.0000000 2 0 0.0079746 0.0269485 0.0441076 0.0211739 2 1 0.0088545 0.0369580 0.0531271 0.0283235 2 2 0.0019249 0.0120993 0.0742460 0.0378930 2 3 0.0000000 0.0000000 0.0000000 0.0000000 3 0 0.0031348 0.0067096 0.0040148 0.0029698 3 1 0.0041248 0.0192490 0.0178740 0.0114394 3 2 0.0008250 0.0051697 0.0323383 0.0255736 3 3 0.0002750 0.0005500 0.0038498 0.0146842 4 0 0.0015949 0.0033548 0.0010999 0.0024749 4 1 0.0025299 0.0140792 0.0231537 0.0153992 4 2 0.0015399 0.0087445 0.0382779 0.0257936 4 3 0.0001540 0.0021999 0.0027499 0.0237037 This table serves as the seed for the IPF process in the household classification Save this data table as a DBF file that you can put into the model at params/classification/hh_seed.dbf. The write.dbf() function is part of the foreign library. as.data.frame(joint_distribution) %&gt;% foreign::write.dbf(&quot;data/hh_seed.dbf&quot;) Report For your homework, you should have a complete run of the uncalibrated, “bare” RVTPO model. Open the “Trip Generation” submodule by double-clicking on the yellow box labeled Trip Generation. This submodule has two steps: the household classification model and the trip productions model. You can run just this step of the model (and not the entire travel model) by clicking the “Run Application” button in the top-left of the Cube window and then selecting the “Run current group only” option in the dialog. Open the classified socioeconomic data file that is an input to the trip productions step (the file is named se_classified_2012A.dbf), and make a tabulation report counting up the number of households in each district in the following three categories: W3V0 Households with three or more workers and no vehicles. W0V3 Households with no workers and three or more vehicles. P2V2 Households with two people and two vehicles. The marginal classification curves and the joint distribution seed table are stored in the params/classification folder. Each file is a .dbf with the following format: hh_size_lookup.dbf: [AVG, PERSONS_1, PERSONS_2, PERSONS_3, PERSONS_4] hh_workers_lookup.dbf: [AVG, WORKERS_0, WORKERS_1, WORKERS_2, WORKERS_3] hh_vehicles_lookup.dbf: [AVG, VEHICLES_0, VEHICLES_1, VEHICLES_2, VEHICLES_3] hh_seed.dbf: [WORKERS, PERSONS, VEHICLES, SHARE] Open these files in Cube, and observe that the data contained them is nonsense. Replace the files in the params/classification folder with the distributions you created above, including the marginal distribution curves you calibrated by hand and the household seed you constructed from CTPP. Run the trip generation submodule again, and recreate the tabulation report you previously created. Do these numbers make more sense? You can edit the DBF files in Cube by hand; its copy-paste feature is not what you would expect from a software program in 2020 (or 2005). A more efficient way might be to use the write.dbf() function in R. # read the file you edited; there is also a readxl() function for excel # spreadsheets read_csv(&quot;data/raw_size.csv&quot;) %&gt;% # need to make sure the names on the columns match what Cube is expecting to # get; you&#39;ll need to change this for the workers and vehicles rename(AVG = average, PERSONS_1 = `1`, PERSONS_2 = `2`, `PERSONS_3` = `3`, PERSONS_4 = `4`) %&gt;% # read_csv makes a modern data frame called a `tibble`. The old DBF function # has no idea what do to with this. So we need to convert the tibble to an # old-fashioned `data.frame` as.data.frame() %&gt;% foreign::write.dbf(&quot;data/hh_size_lookup.dbf&quot;) The classification model incorporates a zone-level IPF process executed in R; the script for this process is in R/classification.R. Open the script and find the place where the maximum IPF iterations are set. Change this parameter to 50, save the R file, and and re-run both the model and your tabulation report. Change it again to 100 and note the execution time and any changes in the tabulation report. How many iterations should you run? Write a lab report describing the household classification model you have developed. Describe how you developed your marginal disaggregation curves, including any assertions you employed in smoothing /adjusting the curves. Include plots of each curve. Describe how you determined the number of iterations of IPF to run in your model. Compare the distribution of households by classification category to the joint distribution you obtained from the CTPP. References "],["chap-tripgen.html", "Chapter 2 Trip Generation 2.1 Trip Production 2.2 Trip Attraction Homework Lab", " Chapter 2 Trip Generation The purpose of the trip generation model is to turn socioeconomic data into a certain number of trips. Every trip has two ends: an origin and a destination. But because we do not yet know where trips are going, at this stage of the model we instead forecast trip productions and trip attractions. Imagine a basic day where you travel from home to work, and then back to home. You made two trips. The first trip originated at your home and was destined for work: your second trip originated at work and was destined for home. But your home produced two trips, and your workplace attracted two trips. This distinction is critical. In a trip production model, we use the attributes of households to figure out how many trips each zone produces. In a trip attraction model, we use socioeconomic data to determine how many trips are attracted to the zone. It would be oversimplifying to say that all trip production happens at households: businesses produce commercial trips, and households can also attract trips from other households. But in this class, we are mostly going to concern ourselves with household trip production. Trip generation models are separated by trip purpose; trip purposes are defined by the kind of activity occurring at the production and attraction ends of a trip. Common trip purposes include: Home-based work (HBW): trips produced at a home and attracted to a work place. Home-based school / college (HBSchool): trips produced at a home and attracted to a school or college. Home-based shopping / recreational (HBShop): trips produced at a home and attracted to a shopping or recreational activity. Home-based other (HBO): trips produced at a home and attracted to any activity not-defined above. Non Home-based (NHB): trips produced somewhere other than a home and attracted to anywhere. The specific purposes included in a model are a function of the data available and the needs of the region. If there is no university or college in a region, then the HBSchool purpose might be sufficient; otherwise, there might need to be a HBSchool and a HBCollege. Or, if there is no data on trips by students, this purpose might just get folded into HBO. The next two sections give details of how to construct a cross-classification model of trip production as well as a regression model of trip attractions. More details of trip Generation models are given in Section 4.4 of NCHRP Report 716. 2.1 Trip Production The trip production model is a cross-classification model. What this means is that we will classify households into different bins based on their household size, income, vehicle ownership, etc. We will then calculate the average number of trips made by households in each group. We will need to get the data for households and trips. Use the records for households in MSA size 02, that completed the survey on a weekday, and then filter the trips to include only those records. We can also select only the data columns that have the information we will use to classify the model. We might need to create some or modify variables that we need to use to cross-classify; for instance we should cap the household size category at 4 people and the vehicles at 3. library(tidyverse) library(nhts2017) hh &lt;- nhts_households %&gt;% # filter to MSA size 2, travel on weekday filter(msasize == &quot;02&quot;, !travday %in% c(&quot;01&quot;, &quot;07&quot;)) %&gt;% # select the columns we care about. select(houseid, wthhfin, hhsize, hhvehcnt, numadlt, hhfaminc, wrkcount) %&gt;% mutate( hhsize = ifelse(hhsize &gt; 4, 4, hhsize), hhvehcnt = ifelse(hhvehcnt &gt; 3, 3, hhvehcnt), wrkcount = ifelse(wrkcount &gt; 2, 2, wrkcount) ) hh ## # A tibble: 10,381 × 7 ## houseid wthhfin hhsize hhvehcnt numadlt hhfaminc wrkcount ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr+lbl&gt; &lt;dbl&gt; ## 1 30000019 279. 2 2 2 03 [$15,000 to $24,999] 0 ## 2 30000288 103. 1 2 1 05 [$35,000 to $49,999] 0 ## 3 30000289 244. 3 3 2 07 [$75,000 to $99,999] 1 ## 4 30000463 348. 2 2 2 06 [$50,000 to $74,999] 2 ## 5 30000465 133. 4 2 2 08 [$100,000 to $124,999] 2 ## 6 30000478 120. 2 0 2 03 [$15,000 to $24,999] 0 ## 7 30000545 35.7 2 3 2 06 [$50,000 to $74,999] 2 ## 8 30000770 130. 1 1 1 06 [$50,000 to $74,999] 1 ## 9 30000983 147. 4 3 4 09 [$125,000 to $149,999] 1 ## 10 30001177 304. 2 0 2 04 [$25,000 to $34,999] 2 ## # … with 10,371 more rows The next step is we need to calculate how many trips the members of each household in the data took. To do this, we can use summarise to count the number of trip rows for each household. Then, we can pivot_wider to spread the trips out by purpose. trips &lt;- nhts_trips %&gt;% # filter to households in the data filter(houseid %in% hh$houseid) %&gt;% group_by(houseid, trippurp) %&gt;% # count up how many trips each household took summarise(trips = n()) %&gt;% # &quot;spread&quot; the data, filling zero if no trips were taken pivot_wider(id_cols = houseid, names_from = trippurp, values_from = trips, values_fill = 0) ## `summarise()` has grouped output by &#39;houseid&#39;. You can override using the `.groups` argument. trips ## # A tibble: 9,518 × 7 ## # Groups: houseid [9,518] ## houseid HBO HBSHOP NHB HBSOCREC HBW `-9` ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 30000019 4 0 0 0 0 0 ## 2 30000288 1 1 4 0 0 0 ## 3 30000289 2 4 3 1 1 0 ## 4 30000463 7 2 6 0 0 0 ## 5 30000465 8 2 9 0 1 0 ## 6 30000478 2 0 0 0 0 0 ## 7 30000545 0 2 2 4 2 0 ## 8 30000770 0 3 1 0 1 0 ## 9 30000983 4 0 2 1 2 0 ## 10 30001177 3 2 0 2 2 0 ## # … with 9,508 more rows Now, we will join the trips data frame to the households data frame so that everything is in one place. Note that when we do this, there will be some households that never made any trips; we need to change their trip counts from NA to 0. # function to change NA to 0 nato0 &lt;- function(x) {ifelse(is.na(x), 0, x)} tripprod &lt;- hh %&gt;% # join tables by id field left_join(trips, by = &quot;houseid&quot;) %&gt;% # change all NA values in columns from the trips data to 0.a mutate_at(vars(names(trips)), nato0) tripprod ## # A tibble: 10,381 × 13 ## houseid wthhfin hhsize hhvehcnt numadlt hhfaminc wrkcount HBO HBSHOP NHB ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr+lb&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 30000019 279. 2 2 2 03 [$15… 0 4 0 0 ## 2 30000288 103. 1 2 1 05 [$35… 0 1 1 4 ## 3 30000289 244. 3 3 2 07 [$75… 1 2 4 3 ## 4 30000463 348. 2 2 2 06 [$50… 2 7 2 6 ## 5 30000465 133. 4 2 2 08 [$10… 2 8 2 9 ## 6 30000478 120. 2 0 2 03 [$15… 0 2 0 0 ## 7 30000545 35.7 2 3 2 06 [$50… 2 0 2 2 ## 8 30000770 130. 1 1 1 06 [$50… 1 0 3 1 ## 9 30000983 147. 4 3 4 09 [$12… 1 4 0 2 ## 10 30001177 304. 2 0 2 04 [$25… 2 3 2 0 ## # … with 10,371 more rows, and 3 more variables: HBSOCREC &lt;dbl&gt;, HBW &lt;dbl&gt;, ## # -9 &lt;dbl&gt; Now we can count up the number of trips by grouping the variables we care about and taking the average. For instance, we can get the average HBO trip rate for households by size and vehicle count. Remember to weight! hbo_tripprod &lt;- tripprod %&gt;% group_by(hhsize, hhvehcnt) %&gt;% summarise( n = n(), # number of households in category HBO = weighted.mean(HBO, wthhfin), # average HBO trips per hh ) ## `summarise()` has grouped output by &#39;hhsize&#39;. You can override using the `.groups` argument. hbo_tripprod ## # A tibble: 16 × 4 ## # Groups: hhsize [4] ## hhsize hhvehcnt n HBO ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 0 345 0.414 ## 2 1 1 2263 0.558 ## 3 1 2 541 0.540 ## 4 1 3 207 0.339 ## 5 2 0 55 0.420 ## 6 2 1 782 1.55 ## 7 2 2 2472 1.15 ## 8 2 3 1282 1.12 ## 9 3 0 25 3.98 ## 10 3 1 161 1.90 ## 11 3 2 422 2.04 ## 12 3 3 561 1.93 ## 13 4 0 12 3.32 ## 14 4 1 120 5.28 ## 15 4 2 556 4.31 ## 16 4 3 577 4.19 2.2 Trip Attraction Trip attraction models estimate how many trips will be attracted to a particular zone. This is a function of how many jobs of different kinds are in a zone, in addition to other elements of a zone. Trip attraction models are often a linear regression model. The NHTS cannot be used to estimate trip attraction models because we do not know how many trips went to each TAZ; we only see the household side of the survey. So we will use a file that I have prepared from the Puget Sound Regional Council (PSRC, Seattle) household travel survey. This file is available on Box. You can download it and read it directly into an R session with the read_csv() function, psrc_attractions &lt;- read_csv(&quot;https://byu.box.com/shared/static/7ci8vomip719bdno7xl5ftjj940dausm.csv&quot;) ## Rows: 643 Columns: 12 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (12): attr_tract, HBO, HBShop, HBW, NHB, tothh, retl, manu, offi, gved, ... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. psrc_attractions ## # A tibble: 643 × 12 ## attr_tract HBO HBShop HBW NHB tothh retl manu offi gved othr ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 53033000100 8918. 1024. 6151. 4.70e2 3779 508 121 680 28 199 ## 2 53033000200 25787. 148. 0 1.03e4 3654 146 0 605 264 117 ## 3 53033000300 39.0 13.0 0 8.50e2 1187 50 0 574 0 0 ## 4 53033000401 1215. 478. 83.1 2.79e3 3760 385 0 773 211 99 ## 5 53033000402 5079. 353. 0 5.30e3 2437 97 53 981 27 124 ## 6 53033000500 0 0 0 9.89e1 1251 0 0 96 2 93 ## 7 53033000600 9787. 4217. 1937. 3.22e4 3413 997 0 4017 527 101 ## 8 53033000700 5659. 32.4 178. 4.20e3 2222 315 0 967 19 89 ## 9 53033000800 1653. 1.08 0 2.22e3 1067 0 0 33 61 15 ## 10 53033000900 2075. 0 0 4.25e0 868 0 0 80 0 5 ## # … with 633 more rows, and 1 more variable: totemp &lt;dbl&gt; This file has, for every tract in the Seattle metro region, how many trips were attracted to the tract by purpose as well as the households and jobs by type in that tract. Let’s look at the relationship between HBW trips and total employment: ggplot(psrc_attractions, aes(x = totemp, y = HBW)) + geom_point() + stat_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 1 rows containing non-finite values (stat_smooth). ## Warning: Removed 1 rows containing missing values (geom_point). We can estimate a linear regression model with the lm function. In this function we specify the model as y ~ x + .... lm_hbw_totemp &lt;- lm(HBW ~ totemp, data = psrc_attractions) summary(lm_hbw_totemp) ## ## Call: ## lm(formula = HBW ~ totemp, data = psrc_attractions) ## ## Residuals: ## Min 1Q Median 3Q Max ## -41447 -1955 -1180 -699 56593 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 530.34960 300.96059 1.762 0.0785 . ## totemp 0.98197 0.04386 22.388 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6902 on 640 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.4392, Adjusted R-squared: 0.4383 ## F-statistic: 501.2 on 1 and 640 DF, p-value: &lt; 2.2e-16 Let’s estimate a more complex for HBO trips with many predictors. hbo_rates &lt;- lm(HBO ~ tothh + retl + manu + offi + gved + othr, data = psrc_attractions) summary(hbo_rates) ## ## Call: ## lm(formula = HBO ~ tothh + retl + manu + offi + gved + othr, ## data = psrc_attractions) ## ## Residuals: ## Min 1Q Median 3Q Max ## -24039 -6216 -4083 -818 227788 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -394.4522 2045.5441 -0.193 0.84715 ## tothh 3.0939 0.9421 3.284 0.00108 ** ## retl 2.8846 1.4143 2.040 0.04180 * ## manu 1.3558 1.5214 0.891 0.37320 ## offi 0.3982 0.2207 1.804 0.07165 . ## gved 0.5974 0.4671 1.279 0.20137 ## othr 0.0726 0.9776 0.074 0.94082 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17030 on 635 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.07254, Adjusted R-squared: 0.06378 ## F-statistic: 8.278 on 6 and 635 DF, p-value: 1.199e-08 The \\(R^2\\) statistic is not particularly good, but it really never will be with this kind of data. More important is the relationship with the residuals. It’s also not very good; there are a few outliers and quite a bit of heteroskedasticity. But there may be things we can try to make it better. plot(hbo_rates, which = 1) Homework Calculate the trip rates for each purpose by household size, and by income group. Do the rates make sense? Why or why not? Calculate the trip rates for each purpose by the number of household workers and the vehicle availability. Do the rates make sense? Why or why not? Calculate the variance or standard deviation in work trip rates by household size / vehicles and by number of workers / vehicles. Which classification should be used for work trips? Calculate the number of households in each classification (size / vehicles), (workers / vehicles). What information does this give you about the estimated trip rates? Estimate trip rate attraction models for all the trip purposes. Present models with only significant or influential factors (try a few different specifications until you are satisfied with your models’ performance) Explain your attraction rate models; do the rates make sense? Which models have the best fit in terms of \\(R^2\\) value? Why? Remove the intercept from your model estimations. In R, you can do this by adding a -1 to the formula, as in lm(y ~ x - 1). Do the rates change? By how much? Should you keep the intercept in or remove it? Lab In this lab you will implement and calibrate trip generation rates for the RVTPO model. 2.2.1 Trip Production The trip production rates are stored in the params/trip_prod/ folder, with a dbf file for each trip purpose in the model. We are going to calibrate the following trip purposes: HBW: cross-classification model of workers and vehicles available HBO: cross-classification model of household persons and vehicles available HBShop: cross-classification model of household persons and vehicles available The household travel survey for the RVTPO region reported the following total weighted trips in these trip purposes: Purpose Weighted Survey Trips HBW 118,653 HBO 267,987 HBShop 129,614 Begin by running the RVTPO model through the Trip Generation submodel. The trip productions for each trip purpose are recorded in Base/outputs/HH_PROD.dbf. Using this file, create a report that sums the trips produced in each of these three purposes. You can read this file in R using the read.dbf() function in the foreign library, and then sum all columns in this table using the summarize_all function. # read roanoke household trip productions rvtpo_productions &lt;- foreign::read.dbf(&quot;data/HH_PROD.DBF&quot;) %&gt;% as_tibble() # show first 10 rows rvtpo_productions ## # A tibble: 267 × 7 ## TAZ HBWP NHBWP HBOP HBSCP HBSHP NHBOP ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 199. 113. 199. 112. 199. 384. ## 2 2 38.6 22.0 38.6 21.8 38.6 74.5 ## 3 3 88 50.2 88 49.7 88 170. ## 4 4 265. 151. 265. 150. 265. 512. ## 5 5 116. 65.9 116. 65.3 116. 223. ## 6 6 69.0 39.4 69.0 39.0 69.0 133. ## 7 7 26.7 15.2 26.7 15.1 26.7 51.7 ## 8 8 174. 99.2 174. 98.2 174. 336. ## 9 9 82.2 46.9 82.2 46.5 82.2 159. ## 10 10 177. 101. 177. 100. 177. 343. ## # … with 257 more rows rvtpo_productions %&gt;% summarize_all( sum ) ## # A tibble: 1 × 7 ## TAZ HBWP NHBWP HBOP HBSCP HBSHP NHBOP ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 35778 28200. 16093. 28200. 15933. 28200. 54513. Obviously the bare model is not very close to the targets. Replace the bare model rates with rates you estimated from the NHTS during your homework assignment. Run your tabulation report again, and compare the total productions to the regional targets. Adjust the trip rates so that they replicate the regional survey targets within an acceptable margin of error. Ensure that the comparative relationship between the trip rates is maintained (i.e. households with more workers must make at least as many work trips). 2.2.2 Trip Attraction The trip attraction rates enter into the destination choice model, but it is helpful to compare the total forecasted attractions using the rates you estimated. Apply the rates to the zonal socioeconomic data file and calculate the total number of attracted trips for each of the three purposes. Adjust the rates so that the total attractions for your three estimated purposes match the regional totals. 2.2.3 Report Prepare a technical report describing the process by which you estimated household trip production and attraction rates, and calibrated the trip rates to reproduce regional totals. Calculate the margin of error for all purposes (not just the purposes you calibrated). Discuss how well the calibrated trip generation models replicates the survey targets, and justify your residual error. Use formatted tables to display results instead of screenshots of output. You will be graded on the overall readability, flow, formatting and grammar in addition to how clearly you articulate the process of your work. "],["chap-distribution.html", "Chapter 3 Trip Distribution Homework", " Chapter 3 Trip Distribution The trip distribution model is the second component in a traditional 4-step model. The purpose of the trip distribution model is to produce a trip table with the estimated number of trips from each TAZ to every other TAZ in the study area. To do this, predicted productions at each origin TAZ, and predicted attractions at each destination TAZ are combined to create a production-attraction model. Your homework this week will focus on a basic, three zone system which you will calibrate using a Gravity Model. The Gravity Model assumes that the number of trips between any two zones is directly proportional to the trips produced and attracted, controlling for the proportional to the travel time between two zones. Please reference this article from the Travel Forecasting Resource for more information about about trip distribution. Section 4.5 of NHCRP report 716 also provides good background. Homework Figure 3.1: Simple 3-Zone System. Figure 3.14 presents a simple three zone system, the link travel times for this system (for internal trips, \\(t_{ii}=2\\) globally), and the zonal productions and attractions. In this homework you will develop, calibrate, and apply a doubly-constrained gravity model for this system. First Problem: Gravity Model Development In this step, you will develop a gravity model. Assume a gravity model of the form \\[T_{ij}=\\frac{P_iA_j^*(t_{ij})^{-b}}{\\sum_{j&#39;\\in J}A^*_{j&#39;}(t_{ij&#39;})^{-b}}\\] where \\(A_j^*\\) is a “modified attraction term” defined by the algorithm shown in Figure 3.2 below. This is an iterative algorithm; in each iteration the \\(A^*\\) vector gets updated based on the difference between the predicted trips \\(T_{ij}\\) and the input attractions \\(A\\). As the number of iterations increases, the ratio \\(A_j / \\sum_i T_{ij}\\) gets closer to 1. At the same time, the difference between successive predicted trip matrices becomes close to zero. This algorithm therefore “doubly-constrains” the predicted trips to a given zone to equal the input zonal attractions \\(A_j\\), within some tolerance represented by the value \\(\\varepsilon\\). Figure 3.2: Trip balancing algorithm. Why might we want to doubly-constrain a gravity model? Why might we not? Write a program function or develop a spreadsheet to apply this doubly-constrained gravity model. The function code below shows how to calculate one iteration of a gravity model in R. The function takes the production and attractions vector, in addition to the cost matrix and the impedance parameter, and calculates the \\(T_{ij}\\) matrix. Once you have this function (as well as the necessary vectors and cost matrix), you can calculate the gravity model with arbitrary values of the impedance coefficient. #&#39; Gravity Model #&#39; @param p vector of productions, length n #&#39; @param A vector of attractions, lenth n #&#39; @param C matrix of impedances, dim n x n #&#39; @param b impedance parameter gravity &lt;- function(p, a, C, b){ # output matrix (all 0 here) trips &lt;- matrix(0, nrow = length(p), ncol = length(a)) # loop over all rows (production) for (i in 1:length(p)) { bottomA &lt;- sum(a * C[i, ]^(-b)) # denominator # loop over all columns (attraction) for (j in 1:length(a)) { # calculate gravity model for trips from i to j topA &lt;- a[j] * C[i,j]^(-b) trips[i, j] &lt;- p[i] * topA / bottomA } } return(trips) } # calculate one round of gravity model with p &lt;- c(100, 200, 100) a &lt;- c(200, 50, 150) C &lt;- matrix(c(2,5,4,5,2,3,4,3,2), nrow=3, byrow=TRUE) gravity(p, a, C, b = 0.5) ## [,1] [,2] [,3] ## [1,] 59.22613 9.364473 31.40940 ## [2,] 84.61917 33.448665 81.93216 ## [3,] 42.56523 12.287524 45.14725 gravity(p, a, C, b = 1.5) ## [,1] [,2] [,3] ## [1,] 75.27793 4.760994 19.96108 ## [2,] 55.52540 54.870858 89.60374 ## [3,] 28.52074 10.977638 60.50162 Note that were we not going to doubly-constrain this model, we would be done. But we now need to implement the algorithm presented in 3.2. We now have a model that at least generates a trip matrix that is constrained to the attractions. #&#39; Function to balance gravity model #&#39; @param p vector of productions, length n #&#39; @param A vector of attractions, lenth n #&#39; @param C matrix of impedances, dim n x n #&#39; @param b impedance parameter #&#39; @param tolerance Acceptable change in trips matrix balance_gravity &lt;- function(p, a, C, b, tolerance) { # define starting values k &lt;- 0 #iteration counter astar &lt;- a # starting unadjusted attractions trips0 &lt;- matrix(0, nrow = length(p), ncol = length(a)) #initial T is 0&#39;s error &lt;- Inf # first time through, error is Infinite # loop through algorithm while(error &gt; tolerance){ # compute gravity model with adjusted attractions, using your function trips &lt;- gravity(p, astar, C, b) # calculate the error as the change in trips in successive iterations error &lt;- sum(abs(trips - trips0)) # protect against infinite loops, increment values if (k &gt; 100) break # maximum of 100 iterations k &lt;- k + 1 trips0 &lt;- trips astar &lt;- astar * a / colSums(trips) # next iteration astar } return(trips) } double_constrained_trips &lt;- balance_gravity(p, a, C, 0.5, 0.01) double_constrained_trips ## [,1] [,2] [,3] ## [1,] 62.50975 8.329009 29.16124 ## [2,] 91.54031 30.492846 77.96684 ## [3,] 45.94993 11.178146 42.87193 colSums(double_constrained_trips) ## [1] 200 50 150 Second Problem: Model Calibration The three-zone system defined in Figure 3.1 has the observed trip matrix listed below. What we need to do now is find the value of \\(b\\) that minimizes the difference between the observed and predicted trip matrix. You could solve this manually by trying different values of beta, or by using a goal seek / optimization program. The choice is yours, but you should document what you do in your homework response. You also need to determine how you are calculating the “difference”: sum of absolute error? Root mean squared error, etc. \\(i\\) 1 2 3 1 80 5 15 2 80 40 80 3 40 5 55 Third Problem: Model Application The region represented in Problem 1 has begun improvements to the link between zones 1 and 2 that will reduce the travel cost from 5 to 3. Using your algorithm and the value of \\(b\\) that you calibrated above, determine the effect of this improvement on the predicted trip distribution matrix. That is, change the cost matrix to represent the forecasted travel costs and re-run your doubly-constrained gravity model algorithm. Is the response reasonable? This problem is adapted with permission from Urban Transportation Planning: Second Edition by Michael D. Meyer and Eric J. Miller.↩︎ "],["chap-modechoice.html", "Chapter 4 Mode and Destination Choice 4.1 Mathematical Derivation 4.2 Utility Equations 4.3 MNL Estimation 4.4 IIA and Nested Logit 4.5 Accessibility and Log Sums 4.6 Destination Choice Homework Lab", " Chapter 4 Mode and Destination Choice Assume that person has a set of mode choices in front of them. They could pick to drive, walk, take transit, or some combination of those. Which one they pick for which kind of trip is not a simple question, but travel models need a mathematical way to represent this choice. 4.1 Mathematical Derivation We’ll say that person \\(n\\) is choosing between alternatives \\(i, j, \\ldots \\in J\\). In economic theory, people pick the alternative that brings them the most utility, \\(U\\). So person \\(n\\) will choose \\(i\\) if \\(U_i&gt;U_j, \\forall j \\neq i \\in J\\). The problem we face is that \\(U\\) contains things we can measure and model (like the travel time and the fare) as well as things we can’t (like how much this person hates riding the bus). So really \\(U_i\\) has two parts: a measurable portion \\(V_i\\) and an unmeasurable portion \\(\\varepsilon_i\\). \\[\\begin{equation} U_i = V_i + \\varepsilon_i \\tag{4.1} \\end{equation}\\] So in our model, people will choose \\(i\\) if \\[\\begin{align*} V_i + \\varepsilon_i &amp;&gt; V_j + \\varepsilon_j, \\forall j \\neq i \\in J\\\\ V_i - V_j &amp;&gt; \\varepsilon_j - \\varepsilon_i, \\forall j \\neq i \\in J\\\\ \\end{align*}\\] or, if the difference in observed utility is greater than the difference in the unobserved error. Because this difference in error is unobserved, we can treat it as a random variable with an assumed distribution. If we make a particular assumption5, then the probability that person \\(n\\) chooses option \\(i\\) is \\[\\begin{equation} P_n(i | V_i, V_j \\forall j \\neq i \\in J) = \\frac{e^{V_i}}{\\sum_{j\\in J} e^{V_j}} \\tag{4.2} \\end{equation}\\] There are two important things to note that result from this derivation: Only the differences in Utility matter. This will have a number of implications for the interpretation and use of the model. The model formula depends entirely on the assumed distribution of the unobserved utility. If that assumption isn’t met (and it usually isn’t!) a different model is needed. 4.2 Utility Equations The observed utility usually takes a linear form, \\(V_i = \\alpha_i + X_i\\beta\\). This means that each alternative will have a linear equation, \\[\\begin{align*} V_{\\text{Auto}} =&amp; - 0.05(\\text{time}_a) - 0.3(\\text{cost}_a)\\\\ V_{\\text{Bus}} =&amp; -4.5 - 0.05(\\text{time}_b) - 0.3(\\text{cost}_b) - 1.4(\\text{high income})\\\\ V_{\\text{Walk}} =&amp; -6.0 - 0.05(\\text{time}_w) - 0.3(\\text{cost}_w) - 1.8(\\text{high income})\\\\ \\end{align*}\\] Recall that only the difference in the utility matters, which we showed theoretically above. you can also see it numerically because \\(e^0 / (e^1 + e^0) = e^1 / (e^2 + e^1) = 0.269\\). So the utility equations have to reflect this difference in the way that they get structured. There are three basic basic values that show up in these equations, and the difference between them is how they create and use difference in the utility: Alternative-Specific Constants: These function like intercepts in a linear regression model. The constants control for the observed difference between alternatives resulting from the unobserved utility; if all else is equal, the constants describe what the utility of each mode will be. One alternative is selected as the reference alternative, with \\(\\alpha = 0\\). Generic Coefficients: These coefficients have a single estimated parameter. That is, the \\(\\hat{\\beta}\\) coefficient for these variables has the same value in the utility equation for every alternative. These estimates come from variables that vary naturally across the alternatives, like the cost of travel. Alternative-Specific Coefficient: This type of coefficient has a unique estimate for each alternative. That is, \\(\\hat{\\beta}_{DA}\\) is different from \\(\\hat{\\beta}_{Walk}\\). This type of estimate comes from variables that are constant across alternatives, like the distance of the trip. 4.3 MNL Estimation Ordinary Least Squares does not work on multinomial logit models, so we need another way of estimating the model coefficients. We instead use a numerical procedure called maximum likelihood estimation.6 The utility equation is \\(V_i = \\alpha_i + X_i\\beta\\). We want to find an estimate \\(\\hat\\beta\\) that are most likely given the observed data. For one person, the probability that our model picks an alternative \\(i\\) is given by the MNL formula in Equation (4.2). Let’s simplify this to a function of \\(\\beta\\), \\(P_{in}(\\beta)\\). We want to pick the value of \\(\\beta\\) that gives a high probability to the chosen alternative and low probability to the non-chosen alternatives. Let’s create a power value \\(\\delta_{in}\\) equal to 1 if chosen and zero if not. The likelihood of our model for one alternative is therefore \\[\\begin{equation} \\mathcal{L} = P_{in}(\\beta)^{\\delta_{in}} \\tag{4.3} \\end{equation}\\] But we want to find values that maximize this likelihood for all alternatives for all people. Recall from basic statistics that the probability of multiple independent events occuring simultaneously is the product of the probabilities of each event. So the likelihood equation for our entire dataset is \\[\\begin{equation*} \\mathcal{L} = \\prod_{n=1}^N \\prod_{i \\in J} P_{in}(\\beta)^{\\delta_{in}} \\end{equation*}\\] We encounter a problem here where each \\(P_{in}\\) is going to be a number much smaller than 1. If we multiply too many of these numbers together, our computer won’t be able to keep track of a number that small. So we instead use the logarithm of the likelihood function which turns products into sums, \\[\\begin{equation} \\log(\\mathcal{L}) = \\sum_{n=1}^N \\sum_{i \\in J} \\delta_{in} \\log(P_{in}(\\beta)) \\tag{4.4} \\end{equation}\\] We can also compare the log-likelihood of our estimated model \\(\\log\\mathcal{L}(\\beta)\\) against the log-likelihood of some reference models to create a statistic that works kind of like \\(R^2\\) in a linear regression model. First, imagine that we have a perfect model. That would mean the \\(P_{in} = 1\\) for the chosen alternative and zero for the others. The log-likelihood of this model is \\[\\begin{align*} \\log\\mathcal{L}(*) &amp;= \\sum_{n=1}^N \\log(1)\\\\ &amp;= \\sum_{n=1}^N 0\\\\ \\end{align*}\\] Now, what happens if we have a model that is literally no better than random chance? This could be called a null model where each option has equal probability. That would also imply that every utility value is exactly the same, \\[\\begin{align*} \\log\\mathcal{L}(0) &amp;= \\sum_{n=1}^N \\sum_{i \\in J} \\delta_{in} \\log(\\frac{e^0}{\\sum_J e^0})\\\\ &amp;= \\sum_{n=1}^N \\log(\\frac{1}{J})\\\\ &amp;= N \\log(\\frac{1}{J})\\\\ \\end{align*}\\] This value depends on the number of observations and alternatives in the data set, and so will be different for each problem. But because the logarithm of a fraction is a negative number, \\(\\log\\mathcal{L}(0)\\) will be a large negative number. The last special likelihood is what would happen if we just used the observed choice probabilities, or the market shares. This is equivalent to a model estimated with only the constants and no other variables included. \\[\\begin{align*} \\log\\mathcal{L}(C) &amp;= \\sum_{n=1}^N \\sum_{i \\in J} \\delta_{in} \\log(\\frac{e^\\alpha}{\\sum_J e^\\alpha})\\\\ &amp;= \\sum_{n=1}^N \\sum_{i \\in J} \\log(P_i)\\\\ &amp;= N_i \\sum_{i \\in J} \\log(P_i)\\\\ \\end{align*}\\] We can create a measure of goodness-of-fit using these measures. Let’s call it \\(\\rho^2\\). Let’s consider how much more likely our estimated model is than the null model: \\[\\begin{equation} \\rho_0^2 = 1 - \\frac{\\log\\mathcal{L}(\\beta)}{\\log\\mathcal{L}(0)} \\tag{4.5} \\end{equation}\\] If our model is no better than random chance, then \\(\\rho_0^2 = 0\\). If our model is perfect (log-likelihood of zero), then \\(\\rho_0^2 = 1\\). In practice, you will not ever see \\(\\rho_0^2\\) values anywhere near 1. You can build an analogous \\(\\rho_C^2\\) metric by using the market shares likelihood as a reference. 4.4 IIA and Nested Logit The MNL model has a property referred to as the independence of irrelevant alternatives (IIA). This simply means that the ratio of the probabilities of two alternatives is only based on the utility of those two alternatives. Mathematically, \\[\\begin{equation} \\frac{P_i}{P_k} = \\frac{e^{V_i}}{\\sum_{j\\in J} e^{V_j}} \\times \\frac{\\sum_{j\\in J} e^{V_j}}{} = \\frac{e^{V_i}}{e^{V_k}} \\tag{4.6} \\end{equation}\\] This property plays an important role in mode choice, because it introduces some unreasonable behavior when we add a new mode. Consider if we have two modes, car and bus, with utilities of \\(V_{bus} = 0\\) and \\(V_{car} = 1\\). The choice probabilities of these two modes are \\[\\begin{align*} P_{car} &amp;= \\frac{exp(1)}{exp(0) + exp(1)} &amp;= 0.731\\\\ P_{bus} &amp;= \\frac{exp(0)}{exp(0) + exp(1)} &amp;= 0.269\\\\ \\frac{P_{car} }{P_{bus}} &amp;= 0.368 &amp; \\end{align*}\\] Now imagine add a light rail alternative to this scenario, with \\(V_{LRT} = 0.5\\). The choice probabilities are now \\[\\begin{align*} P_{car} &amp;= \\frac{exp(1) }{exp(0) + exp(1) + exp(0.5)} &amp;= 0.506\\\\ P_{bus} &amp;= \\frac{exp(0) }{exp(0) + exp(1) + exp(0.5)} &amp;= 0.186\\\\ P_{LRT} &amp;= \\frac{exp(0.5)}{exp(0) + exp(1) + exp(0.5)} &amp;= 0.307\\\\ \\frac{P_{car} }{P_{bus}} &amp; = 0.368 &amp; \\end{align*}\\] The ratio of the choice probabilities of car and bus stay exactly the same. A consequence of this is that 73% of the new light rail passengers were formerly automobile passengers. But this doesn’t makes intuitive sense; we would expect light rail and bus to have more unobserved things in common. Surely, some automobile riders will decide to take transit when there is a light rail instead of just a bus, but not most of the new light rail riders to be converts. So we need a way to represent the fact that some alternatives are “closer” to other alternatives. When we derived the MNL, we assumed that the unobserved error \\(\\varepsilon_i\\) was distributed independently. That is, if we were to make a matrix of the correlation of the error between our three alternatives for car, bus, and LRT, it would be the matrix below: \\[\\begin{equation*} \\varepsilon \\sim \\begin{matrix} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1\\\\ \\end{matrix} \\end{equation*}\\] But what is more likely is that the two alternatives for bus and LRT have correlated error terms, or the correlation between them is somewhere between 0 and 1. Let’s assume that there is instead some coefficient that dictates the degree to which these error terms are allowed to be correlated. \\[\\begin{equation*} \\varepsilon \\sim \\begin{matrix} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; \\lambda_{transit}\\\\ 0 &amp; 1 - \\lambda_{transit} &amp; 1\\\\ \\end{matrix} \\end{equation*}\\] This is equivalent to saying that the choice of mode doesn’t look like an equal choice among three alternatives, But instead a two-stage choice, where people first choose whether they will take transit, and then which transit mode they will choose. This results in a nested logit model. The choice probability of an alternative in this model is now \\[\\begin{equation} P_{i} = \\frac{e^{V_i/\\lambda_k}(\\lambda_{j \\in B_{k}} e^{V_j/\\lambda_k})^{\\lambda_l - 1}} {\\sum_{l\\in K}(\\sum_{j \\in B_l} e^{V_j/\\lambda_l})^{\\lambda_l - 1}} \\tag{4.7} \\end{equation}\\] It is important to note that if the estimated \\(\\lambda\\) value equals one, then there is no observable correlation between the error terms, and the model reduces to the MNL. If \\(\\lambda&gt;1\\), then it implies negative correlation between the alternatives. This violates utility theory, because it would cause more people to choose auto when the utility of a transit mode increases. So valid values for the estimated correlation are \\(0 &lt; \\lambda &lt; 1\\). 4.5 Accessibility and Log Sums 4.6 Destination Choice Homework In this unit’s assignment, you will use data from the 2000 Bay Area Travel Survey to estimate multinomial and nested logit models that predict mode choice for work trips. The data is available on on Box. The data are named as worktrips_dfidx.rds. You will also need to load the mlogit library package, which contains the tools necessary to estimate multinomial logit models. library(mlogit) worktrips &lt;- read_rds(&quot;data/worktrips_dfidx.rds&quot;) Because multinomial logit models are so different from other models, the data are stored in a special type of data frame. You can see the first several rows of this data below; Person 1 in household 2 has five alternatives, and they chose to drive alone. Person 3 chose to take transit. head(worktrips[,1:8], n=12) ## HHID PERID CASE ALTNUM NUMALTS CHOSEN IVTT OVTT ## 2-1. Drive Alone 2 1 1 Drive Alone 5 TRUE 13.38 2.0 ## 2-1. Share 2 2 1 1 Share 2 5 FALSE 18.38 2.0 ## 2-1. Share 3+ 2 1 1 Share 3+ 5 FALSE 20.38 2.0 ## 2-1. Transit 2 1 1 Transit 5 FALSE 25.90 15.2 ## 2-1. Bike 2 1 1 Bike 5 FALSE 40.50 2.0 ## 3-1. Drive Alone 3 1 2 Drive Alone 5 FALSE 29.92 10.0 ## 3-1. Share 2 3 1 2 Share 2 5 FALSE 34.92 10.0 ## 3-1. Share 3+ 3 1 2 Share 3+ 5 FALSE 21.92 10.0 ## 3-1. Transit 3 1 2 Transit 5 TRUE 22.96 14.2 ## 3-1. Bike 3 1 2 Bike 5 FALSE 58.95 10.0 ## 5-1. Drive Alone 5 1 3 Drive Alone 4 TRUE 8.60 6.0 ## 5-1. Share 2 5 1 3 Share 2 4 FALSE 13.60 6.0 In the first section, you will estimate a multinomial logit mode choice model for work trips using the data collected in the San Francisco Bay. Then, you will calculate nested logit models on the same data set. In the third step, you will calculate mode choice model log sums and estimate a destination choice model. First Problem Set: Multinomial Logit Model Estimation To do this, use the mlogit() function, in a manner sort of like you would use the lm() command. One thing to look out for: the difference between generic and alternative-specific variables, described above. To specify the model, we use the following construction. fit.mnl &lt;- mlogit ( CHOSEN ~ Generic | Alt.Specific, data = worktrips ) To examine the model output, the standard summary() command will produce a coefficients table and key test statistics. The modelsummary packages will produce convenient model comparison tables that can be included in a report or pasted into Excel for further formatting. For your homework, please include a formatted model comparison table rather than a print out of each model summary. Question 1: Calculate the likelihood of a model with no covariates (equal-shares) and a model with constants only (market shares). Estimate a model with only the travel time, and calculate the \\(\\rho^2\\) statistics with respect to the equal shares model and the market shares model. Which statistic is reported by the summary() command? Why is this important? Question 2: Estimate a model with just the total travel time (TVTT) and the cost of the trip (COST). These two parameter estimates will allow you to calculate the value of time for the sample population as \\[VOT = \\frac{60\\hat{\\beta}_{TVTT}}{100 \\hat{\\beta}_{COST}}\\] Report the value of time you calculate. Is this reasonable? Question 3: Estimate a model with the out-of-vehicle travel time (OVTT), and the in-vehicle travel time (IVTT). What is the ratio of these parameters? What does this tell you about how people feel waiting for the bus? Question 4: Estimate a model with the residential population density (RSPOPDEN) and the workplace employment density (WKEMPDEN), controlling for the affordability of the trip (COSTINC). Does land use at the production or the attraction end of the trip affect the mode choice problem more? Is it different by mode? Second Problem Set: Nested Logit Models In this set of problems, you will estimate nested logit models. To specify nests in an mlogit estimation, you supply a list of alternatives to the nests argument. The code below puts all auto-based alternatives in one nest called auto and the other alternatives into another nest called nonauto. nl &lt;- mlogit(CHOSEN ~ COST + TVTT + OVTT | WKEMPDEN, data = worktrips, nests = list(auto = c(&#39;Drive Alone&#39;, &#39;Share 2&#39;, &#39;Share 3+&#39;), nonauto = c(&#39;Bike&#39;, &#39;Walk&#39;, &#39;Transit&#39;)) Question 5: Estimate a nested logit model including cost, travel time, out-of-vehicle travel time, and workplace employment density. Group car alternatives into one nest, and non-car alternatives into another. Constrain the nesting parameter to a single value (set the parameter un.nest.el = TRUE). What is the estimated value of the nesting parameter? What are the implications of this parameter value for across-nest substitution? Question 6: Estimate another nested logit model with the same nests, but this time segment the data on income; include households making less than $50k/year in one segment and households making at least $50k in the other. Add vehicles per worker as a covariate (VEHBYWRK). Comment on how the two segments respond to the different covariates. Which matters more to which group? Question 7. Of all the models you estimated (including in the previous segment), which is the preferred in terms of model likelihood? What about in terms of behavioral sensitivity / reasonableness? Third Problem Set: Log Sums and Destination Choice Mode Choice (Intercept) × Share 2 -2.405 (0.063) (Intercept) × Share 3++ -3.863 (0.107) (Intercept) × Transit -1.535 (0.134) (Intercept) × Bike -3.595 (0.187) (Intercept) × Walk -2.598 (0.105) IVTT -0.006 (0.006) OVTT -0.052 (0.006) COST -0.003 (0.000) WKEMPDEN × Share 2 0.001 (0.000) WKEMPDEN × Share 3++ 0.002 (0.000) WKEMPDEN × Transit 0.003 (0.000) WKEMPDEN × Bike 0.001 (0.001) WKEMPDEN × Walk 0.002 (0.001) Num.Obs. 5029 AIC 7329.0 BIC Log.Lik. -3651.489 rho2 0.248 rho20 0.595 Question 8. An MNL mode choice model is given above. Calculate the utility, choice probabilities, and the choice model logsum for the individual in the dataset below. IVTT OVTT COST WKEMPDEN alternative 13.4 2 70.6 3.48 Drive Alone 18.4 2 35.3 3.48 Share 2 20.4 2 20.2 3.48 Share 3+ 25.9 15.2 116. 3.48 Transit 40.5 2 0 3.48 Bike Question 9 The same person is choosing which of two destinations to travel to. The travel times to Zone 1 are given in question 8, and the travel times to Zone 2 are given below. The destination choice utility equation is \\(V_j = 0.35 * \\text{MCLS}_{ij} + 2.56 * \\ln(\\text{office}_j) + 1.45 * ln(\\text{service}_j)\\). IVTT OVTT COST WKEMPDEN alternative 29.92 10.0 390.81 764.19 Drive Alone 34.92 10.0 195.40 764.19 Share 2 21.92 10.0 97.97 764.19 Share 3+ 22.96 14.2 185.00 764.19 Transit 58.95 10.0 0.00 764.19 Bike The socioeconomic data for both zones is given below. Calculate the destination utilities, choice probabilities, and the destination choice logsum. TAZ office service 1 126 742 2 321 140 Question 10. An improvement to the transit system is proposed that will lower the above individual’s transit out-of-vehicle time to zone 1 to 5 minutes (from 15.2). Calculate the mode choice utility, probabilities, and the mode choice model logsum. What is the monetary value of this transit improvement to the mode choice of this person? Question 11. Using the same improvement, calculate the destination choice utility, probabilities, and destination choice logsum. What is the monetary value of the transit improvement to the destination choice? Does this include the mode choice benefit? Lab In practice, small-sample surveys have a difficult time generating estimates of choice parameters that are both precise and rational. As a result, it is common to assert choice coefficients that have worked well in comparable cities and then calibrate the mode-specific constants and distance decay parameters to match your targets. In this lab you will calibrate the mode choice models and the destination choice models for the following trip purposes: Home-based Work Home-based Other Home-based Shopping In the Roanoke mode choice model, HBO and HBShopping get combined for mode choice. So you will calibrate three purposes in the destination choice model, but only two in the mode choice model. Even though trip distribution comes first in the standard four-step process, we end up using the mode choice logsums mode choice to inform destination choice. Mathematically they happen simultaneously, but in the model it goes skims &gt; mode choice logsums &gt; destination choice &gt; mode choice As a result you will need to calibrate the models iteratively: first adjust the mode choice constants, then the distance decay parameters, then mode choice, etc., until you are satisfied that the model meets the targets. Mode Choice Calibration The coefficients are found in the ./Params/mc/MC_Coefficients.csv file. Record the coefficients in a table in your lab report accompanying a description of the purpose of each coefficient and any notable values. These coefficients are fixed; they should not change as part of this exercise. read_csv(&quot;data/rvtpo_data/MC_Coefficients.csv&quot;) %&gt;% kbl() %&gt;% kable_styling() ## Rows: 20 Columns: 6 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): Name ## dbl (5): ;N, HBW, HBO, NHB, HBSC ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ;N Name HBW HBO NHB HBSC 1 CIVTT -0.02500 -0.01500 -0.02000 -0.01500 2 CSWAIT -0.05625 -0.03375 -0.04500 -0.03375 3 CLWAIT -0.02500 -0.01500 -0.02000 -0.01500 4 CXWAIT -0.06250 -0.03750 -0.05000 -0.03750 5 CCOST -0.00158 -0.00237 -0.00253 -0.18000 6 CDRIVE -0.05625 -0.03375 -0.04500 -0.03375 7 CTERML -0.06250 -0.03750 -0.05000 -0.06250 8 CWALK -0.06250 -0.03750 -0.05000 -0.03750 9 CWALK1 -0.06250 -0.03750 -0.05000 -0.03750 10 CWALK2 -0.09375 -0.05625 -0.07500 -0.05625 11 CBIKE1 -0.06250 -0.03750 -0.05000 -0.03750 12 CBIKE2 -0.09375 -0.05625 -0.07500 -0.05625 13 DWalkBIKE 1.00000 1.00000 1.00000 1.00000 14 NC1 0.50000 0.50000 0.50000 0.50000 15 NC2 0.50000 0.50000 0.50000 0.50000 16 NC3 0.50000 0.50000 0.50000 0.50000 17 CBD 0.00000 0.00000 0.00000 0.00000 18 NXFER 0.00000 0.00000 0.00000 0.00000 19 AUTOCOST 13.60000 13.60000 13.60000 13.60000 20 SHAREFAC 2.00000 2.00000 2.00000 2.00000 The mode-specific constants are in a separate file, ./Params/mc/MC_Constants.csv. The reference alternative in the choice model is Drive Alone, and the alternatives with their nesting structure are: Trips |-- Auto | |--Drive | |--Share | |-- Transit | |--Local | |--Premium | |-- Non-motorized read_csv(&quot;data/rvtpo_data/MC_Constants.csv&quot;) %&gt;% kbl() %&gt;% kable_styling() ## Rows: 10 Columns: 6 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): Name ## dbl (5): ;N, HBW, HBO, NHB, HBSC ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ;N Name HBW HBO NHB HBSC 1 K_SR 1 1 -0.0336 -1.1685 2 K_TRN 1 1 -2.2714 0.3261 3 K_NMOT 1 1 -0.8655 -1.2505 4 K_PREM 0 0 0.0000 0.0000 5 NA 0 0 0.0000 0.0000 6 NA 0 0 0.0000 0.0000 7 NA 0 0 0.0000 0.0000 8 NA 0 0 0.0000 0.0000 9 NA 0 0 0.0000 0.0000 10 NA 0 0 0.0000 0.0000 There is currently no premium service in the model, so changing its constant will not result in more people taking it. But the structure for it exists, if the MPO wants to look at some future transit options. For initial values for the alternative-specific constants, you can use parameters you estimated in the homework. After you run the destination choice model and adjust its calibration (see below), run the mode choice model and look at the mode choice report. Calculate new alternative specific constants using the bias adjustment formula, \\[\\alpha_{n+1} = \\alpha_n + \\log(A_j / S_j) \\] Where \\(A_j\\) is the population share (target) and \\(S_j\\) is the model share. The population shares (targets) are given below. # Population shares (targets) read_xlsx(&quot;data/rvtpo_data/mc.xlsx&quot;) %&gt;% knitr::kable(digits = 1) Mode HBW HBO Drive Alone 87.5 44.1 Share 8.4 45.5 Local Bus 2.7 0.5 Premium 0.0 0.0 Non-motorized 1.4 9.9 Destination Choice Calibration A destination choice model has three basic components: A size term: these are the trip attraction rates An impedance term: this is the mode choice logsum Calibration constants Return to your trip generation lab where you estimated and adjusted attraction rates for different land uses. These attraction rates are the size terms of the destination choice model; replace the dummy rates in the model with your estimated rates. The coefficient on the logsum term should match the nesting parameter in the mode choice model. This link is what allows for a simultaneous mode and destination choice. Instead of alternative-specific constants, a destination choice model includes calibration constants based on distance-decay functions. By adjusting these parameters, we can make the modeled trip length frequency distribution match the observed distribution. The roanoke model has three basic versions that can be combined (but really don’t need to be) A distance polynomial (\\(\\beta_d d + \\beta_{d2} d^2 + \\beta_{d3} d^3\\)) A logarithmic decay function (\\(\\beta_d \\log(d)\\)) A set of bins for trips within specific mile ranges, 0-1, 1-2, etc. There is also an intrazonal constant, which we can leave alone for this lab. Remember that most of the work should be done by the logsum and the size term, and that the constants shouldn’t generally violate basic travel behavior theory. These constants are really just here to nudge the distribution in one direction or another. The table and figure below show the observed trip length frequency distributions for the purposes you need to calibrate. An Excel file with the values is on LearningSuite. tlfd &lt;- read_xlsx(&quot;data/rvtpo_data/tlfd.xlsx&quot;) knitr::kable(tlfd) HIGH MID LOW HBW HBO HBSH 1 0.5 0 3.9039039 19.6131359 13.9606519 2 1.5 1 5.0050050 7.9734108 10.0849391 3 2.5 2 8.3083083 11.9601161 23.5531056 4 3.5 3 10.0100100 12.9567925 13.8690052 5 4.5 4 10.3103103 8.9700871 8.5542211 6 5.5 5 9.0090090 6.9767344 7.1484507 7 6.5 6 13.0130130 7.5098897 6.7094465 8 7.5 7 9.5095095 4.7840465 4.3326161 9 8.5 8 7.6076076 5.0830494 2.8884107 10 9.5 9 5.8058058 4.9833817 3.0809714 11 10.5 10 4.2042042 4.0863730 2.4070089 12 11.5 11 2.0020020 1.7940174 0.4464608 13 12.5 12 1.9019019 1.8936851 2.2144482 14 13.5 13 3.0030030 0.3986705 0.0000000 15 14.5 14 1.9019019 0.6976734 0.3851214 16 15.5 15 0.5005005 0.0996676 0.1093873 17 16.5 16 1.0010010 0.0897009 0.0772576 18 17.5 17 0.3003003 0.0797341 0.0590860 19 18.5 18 0.8008008 0.0000000 0.0402657 20 19.5 19 0.0000000 0.0498338 0.0304333 21 20.5 20 1.0010010 0.0000000 0.0210982 22 21.5 21 0.9009009 0.0000000 0.0276142 ggplot(tlfd %&gt;% gather(purpose, share, HBW:HBSH), aes(x = LOW, y = share, color = purpose)) + geom_line() + xlab(&quot;Distance&quot;) + ylab(&quot;%Trips&quot;) The modeled TLFD for each purpose can be had on the model home screen as an output of the trip distribution model. Calculate the error between your observed and modeled TLFD. Determine which calibration curve you will use (you may decide to use different curves for different purposes). Find the coefficients of the curve that will compensate for the error in the model. Re-run the destination choice model with your new coefficients, run the mode choice model, and adjust the coefficients over there. Report Your lab report should describe the trip distribution and mode choice models; include discussions of the coefficients and your process to calibrate the mode and distance constants. Provide results of the calibration including observed and modeled mode choice and trip length frequency distributions. Specifically, that \\(\\varepsilon_i\\) is distributed IID with a Gumbel extreme value distribution↩︎ You can estimate linear regression models with maximum likelihood estimation as well, but it can be proven the MLE and OLS result in exactly the same answers.↩︎ "],["chap-assignment.html", "Chapter 5 Network Assignment and Validation 5.1 Volume - Delay Functions 5.2 Assignment Algorithms Homework Laboratory Assignment", " Chapter 5 Network Assignment and Validation The purpose of network assignment is to estimate the traffic flows that will occur on each highway link, given the highway network and the trips flowing from all origins to all destinations (determined by the other three steps of the four-step model). On average, we start from the assumption that people will take the shortest path available to them. The travel time on a particular road is a function of the road’s capacity as well as its volume. Because the volume is not known when we start a traffic assignment process, we will have to find the solution iteratively. 5.1 Volume - Delay Functions A volume-delay function (VDF) calculates the increase of travel time on a roadway based on the ratio of volume \\(V\\) to capacity \\(C\\). A popular equation in travel modeling is a function developed by the Buruea of Public Roads (the predecessor to the US Department of Transportation). The BPR VDF is \\[\\begin{equation} t = t_0[1 + \\alpha * (V/C)^\\beta] \\tag{5.1} \\end{equation}\\] Where \\(t\\) is the travel time on the link, \\(t_0\\) is the base travel time, and \\(\\alpha, \\beta\\) are calibrated parameters. Figure 5.1 shows average values for BPR functions obtained from a sample of MPO travel models of different sizes. As roads become more heavily loaded, the travel time increases and other routes become more attractive. Figure 5.1: Average BPR VDF curves in a sample of MPO models. 5.2 Assignment Algorithms Consider that we have the network below, with two routes between nodes \\(A\\) and \\(B\\). The bypass is longer initially, but its travel time will grow less quickly with added volume. In general, the operating theory of network assignment is called static user equilibrium, A network is in static user equilibrium if a person cannot find a shorter path between their origin and destination. That is, all paths that are used have the same travel cost, and all longer paths are unused. In a small and simple network, we could just generate a system of equations that represent the SUE conditions, and solve for the values that will give us that loading. In our example, we can write the system of equations as \\[\\begin{align*} &amp; t_b &amp;- 0.005 V_b &amp; &amp;= 15 \\\\ t_t &amp; &amp; &amp;- 0.02 V_t &amp;= 10 \\\\ t_t &amp; - t_b &amp; &amp; &amp;= 0\\\\ &amp; &amp; V_b &amp; + V_t &amp;= 1000\\\\ \\end{align*}\\] We can solve this using our matrix calculation skills from linear algebra. The SUE assignment is reached when 600 vehicles take the bypass and 400 vehicles take the through road, because when that happens both the routes have an equal travel time of 18 minutes. (A &lt;- matrix(c(0, 1, -0.005, 0, 1, 0, 0, -0.02, 1, -1, 0, 0, 0, 0, 1, 1), byrow = TRUE, ncol = 4)) ## [,1] [,2] [,3] [,4] ## [1,] 0 1 -0.005 0.00 ## [2,] 1 0 0.000 -0.02 ## [3,] 1 -1 0.000 0.00 ## [4,] 0 0 1.000 1.00 b &lt;- c(15, 10, 0, 1000) # Ax = b -&gt; x = A^-1 b solve(A) %*% b ## [,1] ## [1,] 18 ## [2,] 18 ## [3,] 600 ## [4,] 400 It is not going to be feasible to construct this UE matrix for more complex networks. So engineers have developed heuristic algorithms that iterate to find a solution that replicates the UE conditions. 5.2.1 All-or-Nothing The most basic way to assign trips is with an “all-or-nothing” (AON) assignment. This simply puts all the trips between \\(i\\) and \\(j\\) on the shortest route. This is obviously not great in a lot of ways, because it will overload some roads while leaving other roads completely empty. So if we assign 1000 trips to this network, the volumes and travel times become We could repeat this process many times, assigning new AON loads to the updated travel times. This won’t converge to anything, but we could take the average of all the different AON loadings and run with that. It’s not perfect, but it’s easy. 5.2.2 Incremental Assignment Instead of assigning the flows all at once, we might be able to get a more realistic loading by loading the flows in increments. Here’s how this algorithm works: Select increments \\(p_n\\) that sum to 1 (e.g., 0.4, 0.3, 0.2, and 0.1) Calculate the travel times on all links Assign \\(V_n * p_n\\) trips to the network via All-or-Nothing Return to step 2 with the next increment Assigning the 1000 trips to our two-route network using the increment values results in the following successive assignments. Iteration Increment Vb tb Vt tt 0 0 15 0 10 1 0.4 0 15 400 18 2 0.3 300 16.5 400 18 3 0.2 500 17.5 400 18 4 0.1 600 18 400 18 5.2.3 Successive Averages (FHWA) Assignment A big problem with the AON assignment (and with incremental assignment) is the large jump in travel times between iterations. It also is not guaranteed to converge to any particular solution, and the outcome is determined by the assumptions of the number of increments applied. This can be improved with a method developed by FHWA that is designed to repeatedly load the network and update the travel times by a diminishing rate. In this method, the volume on any particular link after iteration is given by \\[\\begin{equation} V_{n} = (1-\\phi) * V_{n-1} + \\phi F \\tag{5.2} \\end{equation}\\] Where \\(\\phi = 1 / n\\) and F is the load of an All-or-Nothing assignment. As \\(n\\) increases, the relative amount of weight given to the previous assignment increases relative to the new AON assignment. The following table shows ten iterations of this algorithm. Iterations Load phi Vt tt Vb tb 1 F 1 1000 V 1000 30 15 2 F 0.5 1000 V 500 20 500 17.5 3 F 0.33333333 1000 V 333.33 16.67 666.67 18.3333333 4 F 0.25 1000 V 500 20 500 17.5 5 F 0.2 1000 V 400 18 600 18 6 F 0.16666667 1000 0 V 500 20 500 17.5 7 F 0.14285714 1000 V 428.571429 18.5714286 571.428571 17.8571429 8 F 0.125 10 1000 20 V 375 17.5 625 18.125 9 F 0.11111111 1000 V 444.44 18.89 555.56 17.78 10 F 0.1 1000 V 400.00 18.00 600.00 18.00 5.2.4 Frank-Wolfe We can represent SUE traffic assignment as a nonlinear optimization problem. We want to find the loading of links that minimizes the total delay in the system, subject to the UE constraints. We can represent the total vehicle travel time on a single link as an integral of the travel time function: \\[\\begin{equation} \\int_0^{v} S(x) dx \\tag{5.3} \\end{equation}\\] where \\(v\\) is the volume assigned to the link and \\(S(x)\\) is the volume-delay function for the link. As the volume increases, the total delay experienced by each vehicle increases as well. The R function integrate computes a numerical integral for a function on a finite interval. We can compute the total vehicle travel time for each link at an equal loading of 500 vehicles as # delay on bypass vdb &lt;- function(v){15 + 0.005*v} # volume-delay for bypass integrate(vdb, 0, 500)$value ## [1] 8125 # delay on town vdt &lt;- function(v){10 + 0.02*v} # volume-delay for town integrate(vdt, 0, 500)$value ## [1] 7500 If we plot these two integrals on the same graph (opposing each other), it is pretty easy to see that the 600, 400 loading condition minimizes the area under each curve, and therefore the total travel time on the system. # reverse vdt function rvdt &lt;- function(x){30 - 0.02 * x} ggplot() + stat_function(fun = vdb, xlim = c(0,1000), color = &quot;#ECCBAE&quot;) + stat_function(fun = rvdt, xlim = c(0,1000), color = &quot;#046C9A&quot;) + stat_function(fun = vdb, xlim = c(0, 600), fill = &quot;#ECCBAE&quot;, alpha = 0.2, geom = &quot;area&quot;) + stat_function(fun = rvdt, xlim = c(600, 1000), fill = &quot;#046C9A&quot;, alpha = 0.2, geom = &quot;area&quot;) + xlab(&quot;Volume on Bypass (Town = 1000 - Bypass)&quot;) + ylab(&quot;Travel Time&quot;) + theme_bw() Keep in mind that though the travel time is the same on both links for the last vehicle, the total delay is different on each link because more people use the bypass than the town route. We can now set up the rest of the optimization. Let \\(v_a =\\) vehicles assigned to link \\(a\\). \\(S_a(v_a) =\\) the travel cost on link \\(a\\) as a function of its volume (VDF function) \\(X_{ij}^r =\\) the total number of vehicles traveling from \\(i\\) to \\(j\\) on the sum of links that represent route \\(r\\). We want to minimize the total travel cost for all users \\[\\begin{equation} \\sum_a \\int_0^{v_a} S_a(x) dx \\tag{5.4} \\end{equation}\\] subject to the constraints \\[\\begin{align*} v_a &amp;= \\sum_i \\sum_j \\sum_r \\delta_{ij}^{ar}X_{ij}{r}\\\\ \\sum_r X_{ij}^r &amp;= T_{ij}\\\\ X_{ij}^r &amp;\\geq 0 \\end{align*}\\] In text, the constraints are as follow: the volume on a link is a sum of the volume on all routes that use that link (\\(\\delta\\) is indicator), the total of all routes has to equal the total number of trips assigned, and the paths on a route are not allowed to be negative. This is a nonlinear programming problem. A number of libraries exist that will find the optimal solutions to these problems. The Rsolnp library finds our solution in 2 iterations. library(Rsolnp) # nonlinear programming solver library # total travel time objective function # x is a vector of volumes on each link total_time &lt;- function(x){ # integrate from 0 to estimated volume sum(integrate(vdb, 0, x[1])$value, integrate(vdt, 0, x[2])$value) } opt &lt;- solnp( c(500, 500), # starting values total_time, # objective function eqfun = sum, eqB = 1000, #the sum of volumes must be 1000 LB = c(0, 0), # flows must be positive UB = c(1000, 1000) #flows cannot exceed total volume ) ## ## Iter: 1 fn: 15500.0000 Pars: 599.99719 400.00281 ## Iter: 2 fn: 15500.0000 Pars: 599.99967 400.00033 ## solnp--&gt; Completed in 2 iterations Various algorithms can be used to find the values of \\(v_a\\) that minimize this objective function subject to these constraints. A popular algorithm is the Frank-Wolfe algorithm, though other algorithms have been developed that converge more quickly under different scenarios. With these algorithms, it is essential to allow the algorithm to converge appropriately. A measure of the convergence is a statistic called the “relative gap,” or the difference between the assignment at that iteration and an AON assignment made with the calculated travel times. As this gap becomes smaller, it means that the difference between travel times on the routes are becoming closer to each other. The figure below shows the value of the relative gap after several thousand iterations in the Washington, D.C. travel model. Figure 5.2: Relative gap after several thousand iterations. Large networks may take many hours to reach convergence that is acceptable for policy analysis. There is a large incentive to “cut corners” by shrinking the maximum number of iterations that are run, but this can lead to strange behavior. Homework Network The figure above7 represents a simple four-node network where 7000 vehicles travel from A to D, and 5000 travel from B to D (there are no additional trips from C to D). Link travel times for the network are given by the functions below. \\[\\begin{align*} t_{AD} =&amp; 20 + 0.01 q_{AD}\\\\ t_{AC} =&amp; 10 + 0.005 q_{AC}\\\\ t_{CD} =&amp; 12 + 0.005 q_{CD}\\\\ t_{BC} =&amp; 7.25 + 0.005 q_{BC}\\\\ t_{BD} =&amp; 20 + 0.01 q_{BD} \\end{align*}\\] Question 1: Solve for the user equilibrium (UE) link flows and travel times by solving a set of simultaneous equations that explicitly define the UE conditions. Demonstrate that your solution is the user equilibrium by showing through example that all UE conditions are satisfied. Question 2: Perform four iterations of All Or Nothing (AON) assignment on the network and O/D volumes. Show the link flows and travel times at the end of each iteration and compute the average link loads and travel times. Question 3: Perform four iterations of an incremental assignment assignment using the increment values 0.4, 0.3, 0.2, and 0.1. Show the link flows and travel times at the end of each iteration. Question 4: Assign trips using the successive averages (FHWA) heuristic. Show the link flows and travel times for five successive assignments, and the final assignment. Question 5: Compare these three traffic assignment heuristic approaches to the UE assignment and to each other. How do the resulting flow patterns differ (cite specific differences)? Which one comes closest to the UE flows? Question 6: You are considering a road widening project in a suburb of a large metropolitan area (indicated with the blue circle). The difference in loaded volumes between your base scenario (no-build) and the widening is given in the figure below. What is a likely explanation for the patterns shown in the figure? Figure 5.3: Difference in assigned volumes when adding a lane in area with blue circle. Laboratory Assignment The highway volume-to-capacity curves in the Roanoke Model have already been largely calibrated8. They use the Bureau of Public Roads (BPR) format, \\[T_c = T_0 * (1 + \\alpha (V / C)^\\beta)\\] Create a plot showing the values of these curves for varying VOC ratios and discuss the implications of the different curves on different facility types in your report. Note that there are 5 facility types in the BPR table but 11 facility types in the model network. The assignment script files have comments that build a crosswalk between the two facility type definitions. For this lab, you will create a model validation report where you examine the following: Root mean squared error (RMSE) by facility type, area type, volume group, and by screenline. Are there certain classes that are outperforming others? Observed vs Modeled link volume scatterplots: an X-Y fit line by facility type as well as a maximum desirable deviation plot defined in NCHRP 765. Geographic distribution of link error. Comment on the Roanoke model’s calibration. This is an adaptation of a homework assignment from Dr. John Ivan at the University of Connecticut.↩︎ To be specific, VDOT has values that they assert for all of their models↩︎ "],["chap-process.html", "Chapter 6 The Planning Process", " Chapter 6 The Planning Process "],["app-demomodel.html", "A Demonstration Model A.1 Running the Model A.2 Files and Reports A.3 Writing Custom Scripts A.4 Network and Zone Maps A.5 Editing Transit Lines", " A Demonstration Model In this class we will use the model for the Roanoke Valley Transportation Planning Organization (RVTPO), the MPO responsible for transportation planning in Roanoke, Virginia. The model is written for the CUBE travel modeling software package, the same software used by the Wasatch Front Regional Council model. The model code and files are available on Box. A few key parameters files have been reset to default values, rather than the calibrated values used in the actual model. The homework assignments and lab activities in this course will walk you through re-calibrating the model to use in your term assignments. A.1 Running the Model The model files are available from Box as a compressed file called rvtpo_bare.zip. Extract this file to a folder on your local computer. I prefer to keep my models in a folder on the C:\\ drive called C:\\projects. It may be that the C drive is not available to you, but you should place the model at a path that makes sense and that will not change from session to session. It is possible that the J: drive will not have enough space for multiple runs of the model. The path that you choose must have no spaces, from the drive letter to the final folder, i.e., C:\\folder\\folder\\rvtpo_bare. If there are any spaces your model will crash. Figure A.1: RVTPO model home folder Double-click on the roanoke.cat Cube catalog file. This will open the model application interface in Cube. On this interface you can see the steps the model will execute, as well as access the input / output files for each step. Some steps actually contain several sub-steps, and double-clicking the yellow step box will expand that application. Figure A.2: RVTPO model application interface Run the base scenario of the model by pushing the large blue “Run” button in the upper left-hand corner of the Cube application. A window will appear first asking you to confirm which scenario you are running, and then showing you the progress. This model takes approximately 15 minutes to run on my laptop.9 Complete instructions are included in the model user’s guide (in the usersguide/ folder). I have also made a YouTube video showing these steps. Note that the video shows you getting the model from Canvas; get it from Box. A.2 Files and Reports You can access files in the model in multiple ways: Through the application manager in Cube (input / output boxes) In the catalog windows on the side Directly in the File Explorer The model has a few prepared reports that you can run at any time. These are found in the “Reports” drop-down in the catalog along the left-hand side of the Cube window. These reports include: Highway vehicle miles traveled and vehicle hours traveled by facility type Mode choice by purpose Transit route-level boardings in Peak and Off-peak periods You can also make tabulations of any report. The video below has an example. A.3 Writing Custom Scripts Sometimes what you would like to look at is not calculated directly from the model, but you can write scripts that will compute what you need. Two common script types are for matrix manipulation and for network manipulation. A.3.1 Network Bandwidth The script below will compute the difference between 2040 and 2012 highway volumes. The script can be adapted for similar applications. The video shows how to use this script and calculate what you need to. RUN PGM=NETWORK NETI[1] = &quot;C:\\projects\\rvtpo-master\\Base\\Output\\LOADED_2012A.net&quot; NETI[2] = &quot;C:\\projects\\rvtpo-master\\Base\\EC_2040\\Output\\LOADED_2040A.net&quot; NETO = &quot;C:\\projects\\rvtpo-master\\Base\\Output\\Growth.net&quot; INCLUDE = base_vol, ec2040_vol, diff PROCESS PHASE=INPUT ;Use this phase to modify data as it is read, such as recoding node numbers. ENDPROCESS PROCESS PHASE=NODEMERGE ; Use this phase to make computations and selections of any data on the NODEI files. ENDPROCESS PROCESS PHASE=LINKMERGE ; Use this phase to make computations and selections of any data on the LINKI files. base_vol = li.1.TOTAL_VOL ec2040_vol = li.2.TOTAL_VOL diff = ec2040_vol - base_vol ENDPROCESS PROCESS PHASE=SUMMARY ; Use this phase for combining and reporting of working variables. ENDPROCESS ENDRUN A.4 Network and Zone Maps You can use Cube to create maps of network and zone data that you can use for debugging and analysis. You can also include these graphics in reports and presentations. For example, Figure A.3 shows the network links by facility type. The video below shows how to do this. Figure A.3: Facility types in the Roanoke region. A.4.1 Shortest Paths You can use Cube to measure the shortest path between two points in your model network. You can also make isochrone maps of the travel time to various destinations from a specific origin point, like the map shown in Figure A.4. The video below shows how to do this. Figure A.4: Isochrone map using network speed information. A.4.2 Mode Choice Logsum Maps The mode choice model logsums are an important accessibility component in the model that you may want to visualize. This video shows how to create these datasets and visualize them. The script is here: /* Average logsum calculator This script reads in a matrix (ideally a logsum) matrix and writes out the row averages. */ RUN PGM=MATRIX ; Input matrix 1 FILEI MATI[1] = &quot;C:\\projects\\rvtpo-master\\Base\\Output\\HBW_MCLS.MAT&quot; ; Can add additional input matrices FILEI MATI[2] = &quot;C:\\projects\\rvtpo-master\\Base\\Output\\HBO_MCLS.MAT&quot; ; Output DBF FILEO RECO[1] = &quot;C:\\projects\\rvtpo-master\\Base\\Output\\average_mcls.dbf&quot;, FIELDS= Z HBW HBO ; can add additional fields ; specify which working matrices are which purpose ; MW[n] = MI.[which mati].[which table] FILLMW MW[1] = MI.1.1 ; HBW FILLMW MW[2] = MI.2.1 ; HBO ; can add additional fields ; The MATRIX program has an implicit row loop. So for each row, we restart the N and sum calculations n = 0 hbw = 0 hbo = 0 ; loop through destinations LOOP JJ=1, ZONES hbw = hbw + MW[1][JJ] hbo = hbo + MW[2][JJ] ; add other fields n = n + 1 ; increment number of zones ENDLOOP ; write output record RO.Z = I RO.HBW = hbw / n RO.HBO = hbo / n ; other fields WRITE RECO = 1 ENDRUN A.5 Editing Transit Lines It’s a small model with only about 250 zones; a larger model like WFRC will take many hours. Generally model run time increases with the square of the zones.↩︎ "],["app-rstudio.html", "B R and RStudio Help B.1 Installing B.2 RStudio Orientation B.3 R Packages B.4 Working with Tables B.5 Graphics with ggplot2", " B R and RStudio Help R is a powerful, open-source statistical programming language used by both professional and academic data scientists. It is among the computer languages most suited to modern data science, and is growing rapidly in its user base and available packages. Some students may not feel comfortable working in a programming language like R or a console-based application like RStudio, especially if they have used applications primarily through a GUI. This appendix provides a basic bootcamp for R and Rstudio, but cannot be a comprehensive manual on RStudio, and it certainly cannot be one for R. Good places to get more detailed help include: R help manuals Stack Overflow Some of the sections in this appendix are text-based, and some contain little more than links to YouTube videos created by me or someone else. B.1 Installing There are two pieces of software you should install: R https://cran.r-project.org/: this contains the system libraries necessary to run R commands in a terminal on your computer, and contains a few additional helper applications. Install the most recent stable release for your operating system. RStudio https://rstudio.com/products/rstudio/download/ is an integrated application that makes using R considerably easier with text completion, file management, and some GUI features. Both software are available for Windows, MacOS, and Linux. The videos and screenshots of the application I post will use MacOS; the R code for all systems is the same, and the RStudio interface all systems is very similar with minor differences. B.2 RStudio Orientation The video below gives a very basic introduction to RStudio. There is also a very useful cheat sheet for working with RStudio on the Rstudio website. B.3 R Packages One of the strengths of R is the ability for anyone to write packages. These packages make it easier to read manipulate, and vizualize data; to estimate statistical models; or to communicate results. There are a number of ways to install additional packages. The most straightforward is to use the install.packages() function in the console. The problems in this book are solved with two additional packages10: install.packages(&quot;tidyverse&quot;) # a suite of tools for data manipulation install.packages(&quot;mlogit&quot;) # discrete choice modeling RStudio also contains a GUI interface to install and update packages. Sometimes you want to use a package that has not yet been pushed to CRAN, the international repository of “approved” R packages. This may be because the package is in development, or for one reason or another does not meet CRAN’s standards for completeness, etc. Oftentimes, the package has been made available on GitHub. You can install a package directly from GitHub with the remotes library. One package you will want for the problems in the book is the nhts2017 package on the BYU Transportation GitHub account. This package contains datasets from the 2017 National Household Travel Survey. install.packages(&quot;remotes&quot;) # tools for installing development packages remotes::install_github(&quot;byu-transpolab/nhts2017&quot;) You only need to install a package once on your computer. But every time you want to use a function in a package, you need to load the package with the library() function. To load the tidyverse packages, for instance, library(tidyverse) ## ── Attaching packages ─────────────────────────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.2.9000 ✓ purrr 0.3.4 ## ✓ tibble 3.0.3 ✓ dplyr 1.0.0 ## ✓ tidyr 1.1.0 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0 ## ── Conflicts ────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() If you get errors when you run the command above, it means that for some reason you did not install the package correctly. And if you ever get an error like kable(tibble(x = 1:2, y = c(&quot;blue&quot;, &quot;red&quot;))) ## Error in kable(tibble(x = 1:2, y = c(&quot;blue&quot;, &quot;red&quot;))): could not find function &quot;kable&quot; It often means you didn’t load the library. In this case, the kable() function to make pretty tables is part of the knitr package. library(knitr) kable(tibble(x = 1:2, y = c(&quot;blue&quot;, &quot;red&quot;))) x y 1 blue 2 red You can also use a function from a package without loading the library if you use the :: operator, like you did in the remotes::install_github() command earlier. This is handy if you only want to use one function from a package, or if you have two functions from different packages with the same name. For example, when you loaded the tidyverse package, R told you that dplyr::filter() would mask stats::filter(). So if for some reason you wanted to use the filter function from the stats package, you would need to use stats::filter(). B.4 Working with Tables Most data you will work with comes in a tabular form, meaning that the data is formatted in columns of variables and rows of observations. B.4.1 Reading Data Tabular data is often stored in a comma-separated values .csv file. To read a data file like this in R, you can use the read_csv() function included in tidyverse. trips &lt;- read_csv(&quot;data/demo_trips.csv&quot;) ## Parsed with column specification: ## cols( ## houseid = col_double(), ## personid = col_character(), ## trpmiles = col_double(), ## trippurp = col_character() ## ) print(trips) ## # A tibble: 924 x 4 ## houseid personid trpmiles trippurp ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 30182694 01 13.6 HBW ## 2 40532989 02 9.38 HBSOCREC ## 3 40729475 01 5.06 HBSHOP ## 4 40290784 01 0.509 HBSOCREC ## 5 30118876 02 0.599 NHB ## 6 30352119 01 20.3 HBO ## 7 30085077 01 22.5 NHB ## 8 30180962 02 0.581 HBSOCREC ## 9 40155356 01 2.74 HBO ## 10 30069734 01 4.65 NHB ## # … with 914 more rows This function will make a guess as to what the columns types should be. Often we want to keep ID values as characters, even if they are numeric (this preserves leading 0 values, etc.). We can tell read_csv() what types we expect with the col_types argument. trips &lt;- read_csv(&quot;data/demo_trips.csv&quot;, col_types = list(houseid = col_character())) print(trips) ## # A tibble: 924 x 4 ## houseid personid trpmiles trippurp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 30182694 01 13.6 HBW ## 2 40532989 02 9.38 HBSOCREC ## 3 40729475 01 5.06 HBSHOP ## 4 40290784 01 0.509 HBSOCREC ## 5 30118876 02 0.599 NHB ## 6 30352119 01 20.3 HBO ## 7 30085077 01 22.5 NHB ## 8 30180962 02 0.581 HBSOCREC ## 9 40155356 01 2.74 HBO ## 10 30069734 01 4.65 NHB ## # … with 914 more rows You can also write tables back to .csv with the write_csv() command. B.4.2 Modifying and Summarizing Tables In much of this section, we will work with the nhts_trips dataset of trips from the 2017 National Household Travel Survey in the nhts2017 package you installed from GitHub above. library(nhts2017) trips &lt;- nhts_trips trips ## # A tibble: 923,572 x 62 ## houseid personid tdtrpnum strttime endtime trvlcmin ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;dbl+lb&gt; ## 1 300000… 01 1 2017-10-10 10:00:00 2017-10-10 10:15:00 15 ## 2 300000… 01 2 2017-10-10 15:10:00 2017-10-10 15:30:00 20 ## 3 300000… 02 1 2017-10-10 07:00:00 2017-10-10 09:00:00 120 ## 4 300000… 02 2 2017-10-10 18:00:00 2017-10-10 20:30:00 150 ## 5 300000… 03 1 2017-10-10 08:45:00 2017-10-10 09:00:00 15 ## 6 300000… 03 2 2017-10-10 14:30:00 2017-10-10 14:45:00 15 ## 7 300000… 01 1 2017-10-10 11:15:00 2017-10-10 11:30:00 15 ## 8 300000… 01 2 2017-10-10 23:30:00 2017-10-10 23:40:00 10 ## 9 300000… 01 1 2017-10-10 05:50:00 2017-10-10 06:05:00 15 ## 10 300000… 01 2 2017-10-10 07:00:00 2017-10-10 07:15:00 15 ## # … with 923,562 more rows, and 56 more variables: trpmiles &lt;dbl+lbl&gt;, ## # trptrans &lt;chr+lbl&gt;, trpaccmp &lt;dbl+lbl&gt;, trphhacc &lt;dbl+lbl&gt;, ## # vehid &lt;chr+lbl&gt;, trwaittm &lt;dbl+lbl&gt;, numtrans &lt;dbl+lbl&gt;, tracctm &lt;dbl+lbl&gt;, ## # drop_prk &lt;chr+lbl&gt;, tregrtm &lt;dbl+lbl&gt;, whodrove &lt;chr+lbl&gt;, ## # whyfrom &lt;chr+lbl&gt;, loop_trip &lt;chr+lbl&gt;, trphhveh &lt;chr+lbl&gt;, ## # hhmemdrv &lt;chr+lbl&gt;, hh_ontd &lt;dbl+lbl&gt;, nonhhcnt &lt;dbl+lbl&gt;, ## # numontrp &lt;dbl+lbl&gt;, psgr_flg &lt;chr+lbl&gt;, pubtrans &lt;chr+lbl&gt;, ## # trippurp &lt;chr+lbl&gt;, dweltime &lt;dbl+lbl&gt;, tdwknd &lt;chr+lbl&gt;, ## # vmt_mile &lt;dbl+lbl&gt;, drvr_flg &lt;chr+lbl&gt;, whytrp1s &lt;chr+lbl&gt;, ## # ontd_p1 &lt;chr+lbl&gt;, ontd_p2 &lt;chr+lbl&gt;, ontd_p3 &lt;chr+lbl&gt;, ontd_p4 &lt;chr+lbl&gt;, ## # ontd_p5 &lt;chr+lbl&gt;, ontd_p6 &lt;chr+lbl&gt;, ontd_p7 &lt;chr+lbl&gt;, ontd_p8 &lt;chr+lbl&gt;, ## # ontd_p9 &lt;chr+lbl&gt;, ontd_p10 &lt;chr+lbl&gt;, ontd_p11 &lt;chr+lbl&gt;, ## # ontd_p12 &lt;chr+lbl&gt;, ontd_p13 &lt;chr+lbl&gt;, tdcaseid &lt;chr&gt;, ## # tracc_wlk &lt;chr+lbl&gt;, tracc_pov &lt;chr+lbl&gt;, tracc_bus &lt;chr+lbl&gt;, ## # tracc_crl &lt;chr+lbl&gt;, tracc_sub &lt;chr+lbl&gt;, tracc_oth &lt;chr+lbl&gt;, ## # tregr_wlk &lt;chr+lbl&gt;, tregr_pov &lt;chr+lbl&gt;, tregr_bus &lt;chr+lbl&gt;, ## # tregr_crl &lt;chr+lbl&gt;, tregr_sub &lt;chr+lbl&gt;, tregr_oth &lt;chr+lbl&gt;, ## # whyto &lt;chr+lbl&gt;, gasprice &lt;chr&gt;, wttrdfin &lt;dbl&gt;, whytrp90 &lt;chr+lbl&gt; B.4.2.1 Select, Filter, and Chains This table is pretty overwhelming. But there are two functions that can help us pare it down: select() lets you select columns in a table using the names of the columns. filter() lets you select rows in a table that meet a certain condition. Let’s practice this by selecting our trips dataset to only include the id columns, the trip length, and the trip purpose. select(trips, houseid, personid, trpmiles, trippurp) ## # A tibble: 923,572 x 4 ## houseid personid trpmiles trippurp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl+lbl&gt; &lt;chr+lbl&gt; ## 1 30000007 01 5.24 HBO [Home-based trip (other)] ## 2 30000007 01 5.15 HBO [Home-based trip (other)] ## 3 30000007 02 84.0 HBW [Home-based trip (work)] ## 4 30000007 02 81.6 HBW [Home-based trip (work)] ## 5 30000007 03 2.25 HBO [Home-based trip (other)] ## 6 30000007 03 2.24 HBO [Home-based trip (other)] ## 7 30000008 01 8.02 HBW [Home-based trip (work)] ## 8 30000008 01 8.02 HBW [Home-based trip (work)] ## 9 30000012 01 3.40 HBSOCREC [Home-based trip (social/recreational)] ## 10 30000012 01 3.40 HBSOCREC [Home-based trip (social/recreational)] ## # … with 923,562 more rows Let’s also practice filtering the trips dataset to only include trips of the purpose “HBO” (home-based other). Notice how the number of rows in the table trips is much smaller. filter(trips, trippurp == &quot;HBW&quot;) # use double equals as comparison ## # A tibble: 117,368 x 62 ## houseid personid tdtrpnum strttime endtime trvlcmin ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;dbl+lb&gt; ## 1 300000… 02 1 2017-10-10 07:00:00 2017-10-10 09:00:00 120 ## 2 300000… 02 2 2017-10-10 18:00:00 2017-10-10 20:30:00 150 ## 3 300000… 01 1 2017-10-10 11:15:00 2017-10-10 11:30:00 15 ## 4 300000… 01 2 2017-10-10 23:30:00 2017-10-10 23:40:00 10 ## 5 300000… 01 5 2017-10-10 09:00:00 2017-10-10 09:20:00 20 ## 6 300000… 01 7 2017-10-10 15:30:00 2017-10-10 16:05:00 35 ## 7 300000… 01 1 2017-10-10 08:00:00 2017-10-10 08:20:00 20 ## 8 300000… 01 2 2017-10-10 18:00:00 2017-10-10 20:00:00 120 ## 9 300000… 02 3 2017-10-10 09:00:00 2017-10-10 11:00:00 120 ## 10 300000… 02 4 2017-10-10 18:30:00 2017-10-10 20:30:00 120 ## # … with 117,358 more rows, and 56 more variables: trpmiles &lt;dbl+lbl&gt;, ## # trptrans &lt;chr+lbl&gt;, trpaccmp &lt;dbl+lbl&gt;, trphhacc &lt;dbl+lbl&gt;, ## # vehid &lt;chr+lbl&gt;, trwaittm &lt;dbl+lbl&gt;, numtrans &lt;dbl+lbl&gt;, tracctm &lt;dbl+lbl&gt;, ## # drop_prk &lt;chr+lbl&gt;, tregrtm &lt;dbl+lbl&gt;, whodrove &lt;chr+lbl&gt;, ## # whyfrom &lt;chr+lbl&gt;, loop_trip &lt;chr+lbl&gt;, trphhveh &lt;chr+lbl&gt;, ## # hhmemdrv &lt;chr+lbl&gt;, hh_ontd &lt;dbl+lbl&gt;, nonhhcnt &lt;dbl+lbl&gt;, ## # numontrp &lt;dbl+lbl&gt;, psgr_flg &lt;chr+lbl&gt;, pubtrans &lt;chr+lbl&gt;, ## # trippurp &lt;chr+lbl&gt;, dweltime &lt;dbl+lbl&gt;, tdwknd &lt;chr+lbl&gt;, ## # vmt_mile &lt;dbl+lbl&gt;, drvr_flg &lt;chr+lbl&gt;, whytrp1s &lt;chr+lbl&gt;, ## # ontd_p1 &lt;chr+lbl&gt;, ontd_p2 &lt;chr+lbl&gt;, ontd_p3 &lt;chr+lbl&gt;, ontd_p4 &lt;chr+lbl&gt;, ## # ontd_p5 &lt;chr+lbl&gt;, ontd_p6 &lt;chr+lbl&gt;, ontd_p7 &lt;chr+lbl&gt;, ontd_p8 &lt;chr+lbl&gt;, ## # ontd_p9 &lt;chr+lbl&gt;, ontd_p10 &lt;chr+lbl&gt;, ontd_p11 &lt;chr+lbl&gt;, ## # ontd_p12 &lt;chr+lbl&gt;, ontd_p13 &lt;chr+lbl&gt;, tdcaseid &lt;chr&gt;, ## # tracc_wlk &lt;chr+lbl&gt;, tracc_pov &lt;chr+lbl&gt;, tracc_bus &lt;chr+lbl&gt;, ## # tracc_crl &lt;chr+lbl&gt;, tracc_sub &lt;chr+lbl&gt;, tracc_oth &lt;chr+lbl&gt;, ## # tregr_wlk &lt;chr+lbl&gt;, tregr_pov &lt;chr+lbl&gt;, tregr_bus &lt;chr+lbl&gt;, ## # tregr_crl &lt;chr+lbl&gt;, tregr_sub &lt;chr+lbl&gt;, tregr_oth &lt;chr+lbl&gt;, ## # whyto &lt;chr+lbl&gt;, gasprice &lt;chr&gt;, wttrdfin &lt;dbl&gt;, whytrp90 &lt;chr+lbl&gt; One extremely useful feature of the tidyverse functions is the chain operator, %&gt;%. This operator basically does the opposite of the assigment operator &lt;-. While assignment says “take the thing on the right and put it in the thing on the left,” chain says “take the thing on the left and pass it as the first argument of the function on the right.” What this means in practice is we can chain R commands together. So we can do the select and the filter statements in sequence, trips %&gt;% select(houseid, personid, trpmiles, trippurp) %&gt;% filter(trippurp == &quot;HBW&quot;) ## # A tibble: 117,368 x 4 ## houseid personid trpmiles trippurp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl+lbl&gt; &lt;chr+lbl&gt; ## 1 30000007 02 84.0 HBW [Home-based trip (work)] ## 2 30000007 02 81.6 HBW [Home-based trip (work)] ## 3 30000008 01 8.02 HBW [Home-based trip (work)] ## 4 30000008 01 8.02 HBW [Home-based trip (work)] ## 5 30000012 01 4.29 HBW [Home-based trip (work)] ## 6 30000012 01 6.82 HBW [Home-based trip (work)] ## 7 30000039 01 11.5 HBW [Home-based trip (work)] ## 8 30000041 01 73.7 HBW [Home-based trip (work)] ## 9 30000041 02 77.9 HBW [Home-based trip (work)] ## 10 30000041 02 77.8 HBW [Home-based trip (work)] ## # … with 117,358 more rows Notice that we didn’t have to tell the select and filter functions the name of the table we were selecting or filtering. The %&gt;% chain operator did that for us. Once we have the table we want, we can assign it to a new object called mytrips In this case, let’s get HBO and HBW trips. mytrips &lt;- trips %&gt;% select(houseid, personid, trpmiles, trippurp) %&gt;% filter(trippurp %in% c(&quot;HBW&quot;, &quot;HBO&quot;)) # use %in% for multiple comparisons. B.4.3 Mutate, Summarize, and Group Sometimes we want to calculate a new column in a table, or recompute an existing column. We can do that with the mutate function, and we can put more than one calculation in a single mutate statement. mytrips %&gt;% mutate( tripkm = trpmiles * 1.60934, # convert miles to km. longtrip = ifelse(tripkm &gt; 50, TRUE, FALSE) # is trip longer than 50 km? ) ## # A tibble: 307,390 x 6 ## houseid personid trpmiles trippurp tripkm longtrip ## &lt;chr&gt; &lt;chr&gt; &lt;dbl+lbl&gt; &lt;chr+lbl&gt; &lt;dbl+lbl&gt; &lt;lgl&gt; ## 1 30000007 01 5.24 HBO [Home-based trip (other)] 8.44 FALSE ## 2 30000007 01 5.15 HBO [Home-based trip (other)] 8.29 FALSE ## 3 30000007 02 84.0 HBW [Home-based trip (work)] 135. TRUE ## 4 30000007 02 81.6 HBW [Home-based trip (work)] 131. TRUE ## 5 30000007 03 2.25 HBO [Home-based trip (other)] 3.62 FALSE ## 6 30000007 03 2.24 HBO [Home-based trip (other)] 3.61 FALSE ## 7 30000008 01 8.02 HBW [Home-based trip (work)] 12.9 FALSE ## 8 30000008 01 8.02 HBW [Home-based trip (work)] 12.9 FALSE ## 9 30000012 01 4.29 HBW [Home-based trip (work)] 6.91 FALSE ## 10 30000012 01 6.82 HBW [Home-based trip (work)] 11.0 FALSE ## # … with 307,380 more rows Other times we want to calculate summary statistics like means. For this we can use the summarize() function. mytrips %&gt;% summarize( mean_trip = mean(trpmiles), sd_trip = sd(trpmiles), max_trip = max(trpmiles), min_trip = min(trpmiles) ) ## # A tibble: 1 x 4 ## mean_trip sd_trip max_trip min_trip ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9.81 32.1 5699. -9 Finally, we sometimes want to calculate summary statistics for different groups. We can tell tidyverse to group our tables with the group_by() function. mytrips %&gt;% group_by(trippurp) %&gt;% summarize( mean_trip = mean(trpmiles), sd_trip = sd(trpmiles), max_trip = max(trpmiles), min_trip = min(trpmiles) ) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 5 ## trippurp mean_trip sd_trip max_trip min_trip ## &lt;chr+lbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 HBO [Home-based trip (other)] 7.73 32.0 5699. -9 ## 2 HBW [Home-based trip (work)] 13.2 31.9 2927. -9 As you might expect, work trips are on average longer than other kinds of trips. But some people report very long trips! You might want to filter your data more carefully for real analyses. B.5 Graphics with ggplot2 The ggplot2 package included in the tidyverse is a very powerful graphics engine with a relatively easy-to-learn grammar. In fact, the gg stands for “grammar of graphics” as it implements the grammar defined by Wilkinson (2012). The basic structure of a ggplot2 call is constructed as follows: ggplot(data, aes(data aesthetics like x and y coordinates, fill color, etc.)) + geom_(geometry style like point, bar, or histogram) + other things like theme, color, and labels For instance, we can create a histogram of trip lengths in the NHTS by giving the x aesthetic as the trpmiles column in the mytrips dataset. ggplot(mytrips, aes(x = trpmiles)) + geom_histogram() ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. This ends up not being very informative because some trips are very long. We could filter out the long trips within the data argument (Note that we still have the -9 values from the missing information). ggplot(mytrips %&gt;% filter(trpmiles &lt; 50), aes(x = trpmiles)) + geom_histogram() ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. If we wanted to see the difference between lengths of different trip purposes, we could add a color aesthetic to the plot. By default this stacks the two categories on top of each other. ggplot(mytrips %&gt;% filter(trpmiles &lt; 50), aes(x = trpmiles, fill = factor(trippurp))) + geom_histogram() ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. You could also show this with a statistical density (the integral of a density function is 1). Note that the alpha statement for fill opacity is not included as an aesthetic, because it doesn’t vary based on any data elements in the way that the x and fill variables do. ggplot(mytrips %&gt;% filter(trpmiles &lt; 50), aes(x = trpmiles, fill = factor(trippurp))) + geom_density(alpha = 0.5) ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. ggplot2 also excels at building statistical analysis on top of visualization. For example, we can see the odometer reading for cars still on the road in 2017 by make. set.seed(15) # so that we pull the same random records each time # sample 15k vehicles built after 1980 with 0 to 500k miles vehicles &lt;- nhts_vehicles %&gt;% # convert numeric make to its labeled name, and then group into manufacturers mutate( make = as_factor(make, levels = &quot;labels&quot;), vehtype = as_factor(vehtype, levels = &quot;labels&quot;), make = case_when( make %in% c(&quot;Toyota&quot;, &quot;Lexus&quot;, &quot;Subaru&quot;) ~ &quot;Toyota&quot;, make %in% c(&quot;Ford&quot;, &quot;Lincoln&quot;, &quot;Mercury&quot;) ~ &quot;Ford&quot;, make %in% c(&quot;Chevrolet&quot;, &quot;GMC&quot;, &quot;Pontiac&quot;, &quot;Buick&quot;, &quot;Cadillac&quot;, &quot;Saturn&quot;) ~ &quot;GM&quot;, make %in% c(&quot;Volkswagen&quot;, &quot;Audi&quot;, &quot;Porsche&quot;) ~ &quot;VW&quot;, grepl(&quot;Jeep&quot;, make) | grepl(&quot;Chrysler&quot;, make) | make %in% c(&quot;Ram&quot;, &quot;Dodge&quot;, &quot;Plymouth&quot;) ~ &quot;Chrysler&quot;, make %in% c(&quot;Honda&quot;, &quot;Acura&quot;) ~ &quot;Honda&quot;, make %in% c(&quot;Nissan/Datsun&quot;, &quot;Infiniti&quot;) ~ &quot;Nissan&quot;, TRUE ~ &quot;Other&quot; # all other makes ) , vehtype = case_when( grepl(&quot;Car&quot;, vehtype) ~ &quot;Car&quot;, grepl(&quot;Van&quot;, vehtype) ~ &quot;Van&quot;, grepl(&quot;SUV&quot;, vehtype) ~ &quot;SUV&quot;, grepl(&quot;Pickup&quot;, vehtype) ~ &quot;Pickup&quot;, TRUE ~ &quot;Other&quot;, ) ) %&gt;% filter(vehtype != &quot;Other&quot;) %&gt;% filter(vehyear &gt; 1980) %&gt;% filter(od_read &gt; 0, od_read &lt; 500000) %&gt;% sample_n(15000) ggplot(vehicles, aes(x = vehyear, y = od_read, color = make)) + geom_point() ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. This is pretty unreadable. But we can add a few things to the figure to make it a little bit easier to understand, like smooth average lines and point transparency. ggplot(vehicles, aes(x = vehyear, y = od_read, color = make)) + geom_point(alpha = 0.5) + stat_smooth(method = &quot;loess&quot;) ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. ## `geom_smooth()` using formula &#39;y ~ x&#39; Let’s break this out by vehicle type. ggplot(vehicles, aes(x = vehyear, y = od_read, color = make)) + geom_point(alpha = 0.5) + stat_smooth(method = &quot;loess&quot;) + facet_wrap(~vehtype) ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. ## `geom_smooth()` using formula &#39;y ~ x&#39; And let’s clean it up a little bit. This is a figure that you could put in a published journal article or thesis, if it showed something you cared to show. ggplot(vehicles, aes(x = vehyear, y = od_read, color = make)) + geom_point(alpha = 0.5) + scale_color_discrete(&quot;Manufacturer&quot;) + stat_smooth(method = &quot;loess&quot;) + facet_wrap(~vehtype) + xlab(&quot;Vehicle Model Year&quot;) + ylab(&quot;Odometer Reading&quot;) + theme_bw() ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. ## `geom_smooth()` using formula &#39;y ~ x&#39; "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

[
["index.html", "Urban Transportation Planning Foreword", " Urban Transportation Planning Gregory Macfarlane, PhD, PE 2020-09-12 Foreword This book contains course notes and assignments for a senior / graduate class in transportation planning and elementary travel modeling. A description for this course is: An advanced course in urban transportation planning. Urban transportation as the outcome of an economic system, details and techniques for four-step travel model development, applications of travel models within a legal and regulatory context. The book is organized into six units: Building Blocks Trip Generation Trip Distribution Mode and Destination Choice Network Assignment and Validation The Planning Process It may seem strange to put the chapter covering the planning process at the end of the course, after students have learned the details of quantitative travel modeling. The purpose for this is that I assign a term project where the students build and calibrate a four-step model as they learn the techniques to do so, and then complete an alternatives analysis using their models. To create the time and space to do this project, we cover “softer” and conceptual topics in the second half of the course. The demonstration model the students calibrate and study is a model built in the Cube travel modeling software for the Roanoke, Virginia, metropolitan region. The model is a relatively advanced four-step, trip-based model with only 250 zones. The limited zone size means that the entire model system runs in approximately 15 minutes on a laptop computer. I am grateful to Virginia DOT for allowing my students the use of this model. Directions on how to use the Roanoke model are given in the Appendices. A handful of assignments require the students to write numerical programs or estimate statistical models. Some guidance on using R and RStudio to accomplish these assignments is also given in the Appendices. Acknowledgements Photographs in the textbook are the work of the author unless otherwise attributed. The vector art in the textbook uses icons from FontAwesome and the Noun Project distributed under creative commons licenses. Specific attributions are below: training wheels by Marco Fleseri from the Noun Project Cover image: TRAX by Ashton Bingham on Unsplash "],
["chap-blocks.html", "Chapter 1 Building Blocks 1.1 Planning for Human Systems 1.2 The Four-Step Transportation Planning Process 1.3 Travel Model Building Blocks 1.4 Data Inputs 1.5 Statistical and Mathematical Techniques Homework Lab", " Chapter 1 Building Blocks This chapter contains concepts, definitions, and mathematical techniques that will be used throughout the semester. Critical terms to understand are given in bold. 1.1 Planning for Human Systems If you look out on any sufficiently busy road, you will see a steady stream of vehicles passing by. Each vehicle is largely indistinguishable from the others, and it is easy as an engineer responsible for that road to see the cars driving by as little more than an input to a problem. But the people inside the cars should not be indistinguishable from each other. Each person who is driving or riding in each of those cars has their own reasons to be driving on that road. One person might be driving to work; one person might be trying to get home to his or her family. Another car might hold a family going on vacation, or a group of friends heading to a movie. If you don’t recognize that each person who travels is different, with different needs and purposes, then it is easy to look only at the supply of transportation infrastructure. Is the road wide enough? Is the traffic signal timed appropriately? But as with anything in the economy transportation is a function of both supply and demand. Why are so many people trying to get down this one road right now? Why didn’t more people take transit? Why didn’t some people choose a different destination? Or why didn’t some people just stay home in the first place? Transportation planning therefore must be concerned with both the supply of infrastructure and the demand for travel. For the most part, economists consider travel a derived demand, which means people only go to the hassle of traveling somewhere if they have some other reason to be there. No one typically just drives around (with the possible exception of teenagers on a weekend night); they are going to work, or school, or a social engagement, or something. Travel demand has not been stable over time. The availability of inexpensive automobiles in the 20th Century created demand for inter-city and intra-urban roads that did not exist before. Rising labor force participation rates for women radically changed the number and types of trips the average household makes in an average day. Technological developments like teleconferencing and smartphone-enabled ridehailing could generate different trends. At the same time, populations in most regions continue to grow. Planning for future transportation infrastructure is difficult because of the uncertainty of the future, but it is necessary to keep economies rolling and preserve or improve quality of life. In the United States and most societies with some democratic process, decisions about what transportation facilities to build, which policies to implement, and how to build a city generally fall to decision makers. These decision makers consist of mayors, city councils, planning commissions, state legislatures, Congress, state and federal agencies, and innumerable others who are elected by the public, or who are accountable to others who have been. In making decisions about how to spend public money on civil infrastructure or enact tax or other policies, decision makers consult plans developed by professional engineers and planners. As engineers and planners, we are rarely in a position to make decisions, but we have a responsibility to provide accurate data and technical analysis to support decision makers. There is a misconception that transportation planners must accurately predict the future to be relevant. The purpose of transportation planning is not to perfectly envision what will happen under every scenario, it is to provide information that will help make good decisions now so that the future is at least as pleasant as the present. We all have hopes for what our lives and community will look like ten or twenty years from now; it may not be possible for anyone to provide analysis entirely free of all personal bias. But as you conduct your work as an engineer and planner, you owe the public your integrity and competence as you provide information to their representatives. 1.2 The Four-Step Transportation Planning Process How can you know what might happen in the future? And how might that change based on decisions that you make today? This basic question is at the heart of transportation planning: What might traffic look like if we build nothing and population still grows? Can we build less if we change land development patterns? How many people will use this new transit line? In many fields — including politics, meteorology, economics, etc. — professionals who seek answers to questions like this do so with the help of a model. A model is a mathematical representation of a real-world system. In any model, there are things that need to vary (called inputs), things that can be estimated or calibrated (called parameters), and results (called outputs). There are also things that are held fixed. The specific mathematical structure of the model, and which things get included and which things are excluded or abstracted away, determine what the model should and should not be used for. For example, we might try to predict something with a linear model, \\[\\begin{equation} y = \\alpha + \\beta x + \\varepsilon \\tag{1.1} \\end{equation}\\] In this case \\(y\\) is the output, \\(x\\) is the input, the \\(\\beta\\) parameter defines how the input influences the outcomes, \\(\\alpha\\) is a fixed value, and \\(\\varepsilon\\) accounts for the random influence of all the factors we did not include. We could add more \\(\\beta\\) and \\(x\\) terms to include more factors in the model. We could also change the mathematical format of the model to represent different types of outcomes, or chain several smaller models together to represent more complex relationships. If we wanted to see what might happen if \\(x\\) changed, we could put in a new value into this equation and the output result \\(y\\) might be a plausible prediction. The plausibility of the output is a function of how well the mathematical model actually represents the reality of the system. In this class you will learn the details of the travel demand modeling process, which is a chain of many models, each with different inputs and outputs. A travel demand model on the whole has two basic inputs: Socioeconomic data representing where people live and work and go to school and do other things. Transportation network data representing the roads and transit services and other methods people use to get between their activities. The basic outputs of a travel demand model are transportation volumes and levels of service. There are many ways to design and build travel demand models, but the traditional way most regions in the United States approach travel demand models is through a four step, trip-based1 process. The four steps are: Trip Generation Trip Distribution Mode Choice Route Assignment A trip generation model determines how many trips are produced in each zone (neighborhood), and how many trips are attracted to each zone. The inputs to this model is the socioeconomic data in each zone. Mathematically, trip generation can be represented as \\[\\begin{equation} P_i = f(SE_i), A_j = f(SE_j) \\tag{1.2} \\end{equation}\\] where \\(i\\) and \\(j\\) are the production and destination zone indexes. A trip distribution model seeks to pair the productions and attractions in the zones based on the travel costs \\(c\\) between the two zones. Mathematically, trip distribution can be represented as \\[\\begin{equation} T_{ij} = f(P_i, A_j, c_{ij}) \\tag{1.3} \\end{equation}\\] A mode choice model estimates how many of the trips from \\(i\\) to \\(j\\) will happen on each available mode \\(k\\), based on the travel time by each mode and other attributes of the origin and destination zones. Mathematically, \\[\\begin{equation} T_{ijk} = f(T_{ij}, c_{ijk}, SE_{i,j}) \\tag{1.4} \\end{equation}\\] A route assignment model determines the specific routes that the trips going between \\(i\\) and \\(j\\) take. This allows us to estimate the volume of level of service on each highway link and transit system \\(l\\). Mathematically, \\[\\begin{equation} LOS_l, V_l = f(T_{ijk}, c_{ijk}) \\tag{1.4} \\end{equation}\\] On the whole, the travel demand model can be represented mathematically as a single function where the output transportation volumes and levels of service are a function of the input socioeconomic information and travel costs. \\[\\begin{equation} LOS_l, V_l = \\mathcal{F}(SE_{i,j}, c_{ij}) \\tag{1.5} \\end{equation}\\] The details of each of these models will be the topic for the next several chapters. 1.3 Travel Model Building Blocks In this section, we present some of the terms used in transportation planning and modeling, as well as some of the data objects used in constructing travel demand models. 1.3.1 Travel Analysis Zones and SE Data The “people” in a model conduct activities: work, school, recreation, and other activities. Because travel is a derived demand, the purpose of travel is to move between these activities. So a travel model needs a way to represent where the households, persons, jobs, and activities are located in space. Activities in travel demand models happen in Travel Analysis Zones (TAZs). The model tries to represent trips between the TAZs. Because trips inside a TAZ — called intrazonal trips — are not included in the travel model, each TAZ should be sufficiently small such that these trips do not affect the models’ ability to forecast travel on roadways. The following rules are helpful when drawing TAZ’s: The TAZ should not stretch across major roadways The TAZ should contain principally one land use, though in some areas this is not possible. In areas with more dense population, the TAZ should be smaller. Each TAZ is associated with socioeconomic (SE) data, or information about the people, businesses, and other activities that are located in the TAZ. Households are a basic unit of analysis in many economic and statistical analyses. A household typically consists of one or more persons who reside in the same dwelling. Individuals living in the same dwelling can make up a family or other group of individuals; that is, a group of roommates is considered a household. Not everyone lives in households, however; some people live in what are called group quarters: military barracks, college dormitories, monasteries, prisons, etc. Travel models need to handle these people as well, but in this class we will focus on people who live in households. Households in travel models are often grouped into a classification scheme based on the number of people in the household, the number of children, the number of vehicles, etc. Households of different classifications will have different behavior in the rest of the model. Your lab activity for this unit will walk you through specifying a household classification model. Firms are another basic unit of analysis in many economic and statistical analyses. A firm is a profit-seeking person or entity that provides goods or services in exchange for monetary transactions. A firm can provide raw resources, manufactured resources, other services, or be a place of employment. In some cases, a firm may be another household. Each firm will have an industry type. Examples of industry types include office, service, manufacturing, retail, etc. In many SE data files, firms are simply represented as the total number of jobs in a TAZ belonging to each industry. Other Institutions including academic, government, and non-profit entities will also be represented in the SE data in terms of their jobs. It is important to be precise in our definitions when put all of these different items into an analysis. A typical socioeconomic data table for a small region is given in Table 1.1. Note the following relationships: Persons live in Households Workers are Persons who have a Job Firms have employees who work at a Jobs Figure 1.1: Travel Analysis Zones in Central Roanoke. When we talk about “how many jobs” are in a TAZ, we mean “How many people do the firms located in that TAZ employ,” and not “how many people who live in that TAZ are workers.” Table 1.1: Example SE Table taz persons hh workers retail office manufacturing 1 44 25 22 129 96 2 2 45 47 21 121 81 11 3 32 35 17 148 89 0 Alice lives with her husband in zone \\(A\\) and works as an accountant in zone \\(B\\). Her husband does not currently work. Fill out the SE table from Table 1.1 with just this household’s information. Two persons live in one household with one worker in zone A. The firm Alice works at has an office job for her in Zone B. taz persons hh workers retail office manufacturing A 2 1 1 B 1 1.3.2 Transportation Networks The purpose of a travel model is to understand how people are likely to use transportation infrastructure, so there has to be a way to represent roadway and transit systems. We do this with a network2. A network consists of two basic data structures: Nodes and Links Nodes are points in space. In a highway network, almost all nodes represent intersections between different roads. Some important nodes represent the TAZ Centroids, or the points where the households and jobs in the travel model are located. Links connect nodes, and represent roads. Links have many different attributes describing the characteristics of the roadway they represent. The two most important link attributes are the link’s speed and capacity, because they provide the travel costs (\\(c_{ij}\\) above) to the various steps of the model. But these attributes might not always be known at the outset, so instead we use attributes of the roadway that influence capacity and speed, and then calculate these other values. Functional Type or Functional Class describes the relative role each road plays in the transportation system (A Policy on Geometric Design of Highways and Streets, 7th Edition, 2018). Every street fills a role on a spectrum from mobility on one end to accessibility on the other: roads that are good at moving high volumes of vehicles are usually not good at providing access to homes and businesses. Common functional types include: Freeways are provided almost exclusively to enhance mobility for through traffic. Access to freeways is provided only at specific grade-separated interchanges, with no direct access to the freeway from adjacent land except by way of those interchanges. The primary function of major and minor arterials is to provide mobility of through traffic. However, arterials also connect to both collectors and local roads and streets and many arterials provide direct access to adjacent development. Major and minor collectors connect arterials to local roads and provide access to adjacent development. Mobility for through traffic is less important. Local streets exist primarily to serve adjacent development. Mobility for through traffic is not important, or even desired. Figure 1.2 shows roads in Provo and Orem classified by this scheme. Streets of a functional class below collector are almost never included in travel models, unless they provide essential connectivity between other roads. Entire neighborhoods of local streets may be represented by just a handful of special links called centroid connectors. Figure 1.2: UDOT Functional Classes. Why are local roads not included in travel models? Free-flow speed is the speed vehicles travel when the road is empty. Historically, travel modelers would use formulas in the Highway Capacity Manual to estimate the free-flow speed for roadways, or assert a basic calculation like 5 miles per hour over the speed limit. More recently, modelers use the speeds reported from GPS devices in the middle of the night to establish free-flow speeds. The number of lanes on a road is fairly self-explanatory, but it plays a major role in the road’s capacity. Roads located in different area types – urban, suburban, and rural – operate differently from each other. Sometimes travel models will assert this value, but more recent models will calculate the area type for each link based on the density of the surrounding TAZs. Link capacity is the maximum number of vehicles a can optimally transport between two nodes. The capacity is a function of functional type, lanes, free-flow speed, area type, etc. Usually travel models will calculate the capacity based on the given values for other roadway characteristics, but sometimes there are ways to override this feature, i.e., if engineers have developed specific capacity estimates for a new project. Centroid connectors are special links that connect centroids to a network. These are different from other links in that they usually don’t have a capacity or a speed (they don’t represent real roads). 1.3.3 Matrices Travel models need to represent travel times, costs, and flows between zones. Models store this data in matrices, special data structures developed for this purpose. Each matrix is a square table where the rows \\(i\\) represent origin zones and the columns \\(j\\) represent the destination zones. Each cell represents something about the relationship between the two zones. There are two kinds of information we typically represent with matrices: Cost matrices, or skims, are matrices where the cells contain estimates of travel time or cost. They are called skims because they are the results of skimming a network to find the shortest path between each pair of TAZ centroids. Flow matrices, represent flows of people or vehicles from each origin to each destination. The number in the corresponding cell \\(T_{ij}\\) is the total number of trips made, and represents the demand between two zones in a network. 1.4 Data Inputs In the last section we discussed data structures like highway networks and socioeconomic data files. In this section, we are going to talk about the data inputs that can be used for developing travel models. Besides highway networks — which usually have to be supplied by the transportation agency — modelers frequently gather data from household travel surveys and the US Census Bureau. Obtaining an accurate highway network is one of the most difficult tasks in travel modeling. It’s not conceptually or intellectually difficult, but it is very difficult to map model networks onto the linear referencing systems or GIS files used by other agency departments. This is made even more difficult by model networks needing to be routeable: common GIS formats like shapefiles have no way of representing routability. 1.4.1 Household Travel Surveys Travel demand models try to represent individual behavior. How many trips does the average household make per day? How do people respond to changes in transit fare? And how can a modeler know if the model accurately reflects total traffic? Household travel surveys are a critical component of much travel modeling practice and research, and are a primary way to answer some of these questions. In a travel survey, a regional planning agency3 will recruit households to participate in the survey. Often there is some kind of reward to encourage participation, like a gift card or raffle. Once recruited, household members fill out a diary of their activities on an assigned day; Figure 1.3 shows an example of one activity from a survey diary. From the example, you can see the kinds of data that are available: where the person traveled, which travel mode they used, and what was their reason for making the trip. Figure 1.3: Example travel survey diary entry. Not all travel surveys are filled in on forms; nowadays telephone interviews or mobile applications are more common (more on that below). But for decades, paper travel surveys were the basis of almost all transportation behavior science. Once the surveys are collected, the data is usually processed into several tables stored in different files or a database. A Households table has one row for each household in the dataset, including information about the number of people in the household, the number of vehicles, and the household income. A Persons table has one row for each person in the dataset — including which household they are a part of (to link with the households table) — and personal attributes like age, student or worker status. A Vehicles table has one row for each vehicle owned by the households in in the dataset, including attributes like model year, vehicle class, and fuel efficiency. A Trips table has one row for each trip taken by each person in the dataset. This table can be linked against the other tables if necessary, and contains information like the trip purpose and many other elements collected with the form in Figure 1.3. Tables 1.2 through 1.5 show data collected from one household in the 2017 National Household Travel Survey. The household contains four people, two of whom are working adults in their late thirties. (the other two are children, and the NHTS did not collect their trip data). The household has two vehicles, and on the survey travel day person 2 appeared to make a few very long trips. It’s impossible to know if this is a typical day for this person or not, but that’s the data that was collected. Table 1.2: NHTS Households File houseid hhsize numadlt wrkcount hhvehcnt hhfaminc wthhfin 30000082 4 2 2 2 $100,000 to $124,999 1148.809 Table 1.3: NHTS Persons File houseid personid r_age educ r_sex 30000082 01 39 Graduate degree or professional degree Female 30000082 02 38 Bachelor’s degree Male Table 1.4: NHTS Vehicles File houseid vehid vehyear make model fueltype od_read 30000082 01 2011 Mazda Mazda3 Gas 83644 30000082 02 2007 Toyota Yaris Gas 120615 Table 1.5: NHTS Trips File houseid personid strttime endtime trpmiles trptrans trippurp 30000082 01 2017-10-10 07:45:00 2017-10-10 07:52:00 2.710 Car Home-based trip (other) 30000082 01 2017-10-10 08:09:00 2017-10-10 08:13:00 1.432 Car Not a home-based trip 30000082 01 2017-10-10 08:24:00 2017-10-10 08:28:00 0.777 Car Not a home-based trip 30000082 01 2017-10-10 16:53:00 2017-10-10 16:57:00 1.075 Car Not a home-based trip 30000082 01 2017-10-10 17:18:00 2017-10-10 17:26:00 2.727 Car Home-based trip (other) 30000082 02 2017-10-10 07:30:00 2017-10-10 07:33:00 2.136 Car Home-based trip (shopping) 30000082 02 2017-10-10 07:38:00 2017-10-10 08:50:00 88.581 Car Not a home-based trip 30000082 02 2017-10-10 08:58:00 2017-10-10 09:49:00 45.341 Car Not a home-based trip 30000082 02 2017-10-10 10:51:00 2017-10-10 12:24:00 28.208 Car Not a home-based trip 30000082 02 2017-10-10 17:00:00 2017-10-10 17:05:00 0.239 Walk Not a home-based trip 30000082 02 2017-10-10 19:15:00 2017-10-10 19:26:00 0.267 Walk Not a home-based trip 30000082 02 2017-10-10 19:30:00 2017-10-10 20:43:00 29.293 Car Not a home-based trip Note that that the households data in Table 1.2 contains a numeric column called wthhfin. This is a survey weight. Because it is impossible to sample everyone in a population, there needs to be a way to expand the survey to the population. What this number means is that the selected household carries the same weight in this survey as approximately 1100 households in the general population. Also note that not every household’s weight will be equal; because some population groups have different survey response weights, some households will need to be weighted more heavily so that the survey reflects the general population. Most software packages have functions that allow you to calculate statistics or estimate models including weighted values. The code chunk below shows how to calculate the average number of workers per household with and without weights in R; as you can see, omitting the weights leads to a substantial change in the survey analysis. # Average workers per household with no weights mean(nhts_households$wrkcount) ## [1] 0.9891438 # Average workers per household, weighted weighted.mean(nhts_households$wrkcount, nhts_households$wthhfin) ## [1] 1.173206 Travel survey methodology is changing rapidly as a result of mobile devices with location capabilities. First, most travel surveys are now administered through a mobile application: respondents are invited to install an app on their smartphone that tracks the respondent’s position and occasionally asks questions about trip purpose or mode. This makes collecting and cleaning data considerably easier than traditional paper surveys, and it also lowers the response burden for the survey participants. Another change that mobile data has brought to travel surveys is the introduction of large datasets of location information that planners can purchase directly from cellular providers or third-party providers. Though these data do not have all the information on demographics and preferences a survey would provide, they provide a considerably larger and more detailed sample on things like overall trip flows. As a result, it may be possible to collect surveys less frequently, or to reduce survey sample sizes. 1.4.2 US Census Bureau The primary statistical agency of the United States is the U.S. Census Bureau, called Census. The need to collect statistics is established in the Constitution, Representatives and direct Taxes shall be apportioned among the several States which may be included within this Union, according to their respective Numbers… The actual Enumeration shall be made within three Years after the first Meeting of the Congress of the United States, and within every subsequent Term of ten Years, in such Manner as they shall by Law direct. Since the first census in 1790, Census has collected more data than simply the number of people in each state. Current programs that are especially important for travel modeling and other related demographic research include: The Decennial Census of Population and Housing is the thing most people think of when they think of Census. Every ten years (most recently in 2020), Census attempts to collect the number, age, and other key population attributes for every dwelling unit in the United States. The American Community Survey (ACS) is an annual 1% sample of the US population. This survey is considerably more detailed than the decennial census, and asks questions regarding the education and income of household members, how each worker traveled to work, whether people own or rent their home, etc. The ACS is a particularly useful data set, especially because the decennial census can become outdated as ten years go by between collections. To protect the individual identity of ACS respondents, Census engages in a number of different schemes. The simplest scheme is data aggregation. ACS data is usually obtained as tables representing the number of individuals or households in a geographic area that match a certain category. The data is aggregated in two ways: First, large geographic areas are aggregated each year, meaning that up-to-date numbers are always available; Second, smaller geographic areas contain groups of records collected over the last five years. In this way there is a tradeoff between temporal and spatial precision that the researcher needs to consider. Another basic scheme is top-coding, where numeric variables are capped at a common value. The ACS will report how many people in a neighborhood have an income over $250,000, but not how many have an income over $1 million. Census will also suppress small counts in a category; they will not reveal how many households have 8, 9, or 10 people, instead collapsing all of these households into a “seven or more” group. If too few individuals or households match that category, Census does not provide a count. For example, if only one household in a neighborhood makes more than $250,000, the ACS table for that cell will contain no information. That could mean there are zero, one, four, or some other small number of households in that category. Besides tables, the Census releases the ACS Public Use Micro-Sample (PUMS) containing ACS responses as disaggregate microdata. These data are geographically located to much larger areas than other ACS records, and Census does imputation and data swapping on the records to ensure that private information cannot be disclosed. But studies conducted on ACS PUMS records reach the same statistical conclusions as studies conducted on the unmodeled and raw data, making PUMS a useful tool in studying populations. 1.4.2.1 Geographies Census data are given in a geographical heirarchy: State County Tract Block Group Block The bottom three layers are shown in Figure 1.4. Each layer nests completely within the layer above it. More detailed data is available at less spatially detailed geographies. Figure 1.4: US Census Geographies in Central Provo. 1.5 Statistical and Mathematical Techniques Many elements of travel modeling and forecasting require complex numerical and quantitative techniques. In this section we will present some of these techniques. Many of the data tables are in the nhts2017 package. To install this package, follow the directions in the Appendix. 1.5.1 Continuous and Discrete Distributions In general, statistical variables can fall into one of two categories: Continuous variables can take any numeric value along some range Discrete variables can take some limited set of predetermined values A simplistic definition would be to say that continuous variables are numeric and discrete variables are non-numeric. A continuous variable has statistics such as a mean, but these statistics do not make sense on discrete variables. In the NHTS trips dataset, we can compute a mean trip miles, but we cannot compute a mean trip purpose. Or we can’t compute a mean that makes sense. # mean of continuous variable: trip length weighted.mean(nhts_trips$trpmiles, nhts_trips$wttrdfin) ## [1] 10.69119 # mean of categorical variable: trip purpose weighted.mean(nhts_trips$trippurp, nhts_trips$wttrdfin) ## Error in x * w: non-numeric argument to binary operator What we can do, however, is we can print a summary table showing the number of observations that fit in each trip purpose category. Note that sometimes there will be a category devoted to data that is missing or otherwise invalid. table(nhts_trips$trippurp) ## ## -9 HBO HBSHOP HBSOCREC HBW NHB ## 32 190022 195188 110235 117368 310727 Sometimes it is handy to split a continuous variable into categories so that you can treat it as a discrete variable. nhts_trips$miles_cat &lt;- cut(nhts_trips$trpmiles, breaks = c(0, 10, 20, 30, 50, 100, Inf)) table(nhts_trips$miles_cat) ## ## (0,10] (10,20] (20,30] (30,50] (50,100] (100,Inf] ## 719812 113383 38724 25064 14388 11060 When we visualize the distribution of a continuous variable, we might use a histogram or density plot, but with a discrete variable we would use a bar chart. ggplot(nhts_trips, aes(x = trpmiles, weight = wttrdfin)) + geom_histogram() + xlab(&quot;Trip Distance [Miles]&quot;) + ylab(&quot;Weighted Trips&quot;) + scale_x_continuous(limits = c(0, 50)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 1.5: Visualizing a continuous distribution with a histogram. ggplot(nhts_trips, aes(x = as_factor(trippurp, levels = &quot;labels&quot;), weight = wttrdfin)) + geom_bar() + xlab(&quot;Trip Purpose&quot;) + ylab(&quot;Weighted Trips&quot;) Figure 1.6: Visualizing a discrete distribution with a bar chart. To this point we’ve only looked at the distribution of one variable at a time. There are lots of cases where someone might want to consider the joint distribution of two variables. This joint distribution tells you what is happening with one variable while the other variable changes. In a table like the one below, the margins of the table (the row and column sums) contain the single variable distribution. So sometimes we call these the marginal distributions. table(nhts_trips$miles_cat, nhts_trips$trippurp) ## ## -9 HBO HBSHOP HBSOCREC HBW NHB ## (0,10] 23 156315 162602 84980 67162 248730 ## (10,20] 6 20856 19881 13054 28018 31568 ## (20,30] 1 5635 5469 4361 12087 11171 ## (30,50] 0 3592 3427 3267 7117 7661 ## (50,100] 2 1943 2150 2504 2332 5457 ## (100,Inf] 0 1231 1634 1844 597 5754 We can visualize joint distributions as well, and sometimes the results are quite nice. 1.5.2 Iterative Proportional Fitting There are times when we know two marginal distributions but do not know the joint distribution. This can happen for a number of reasons: We know how many households of different sizes and workers, but not how many large households have multiple workers. We have a forecast of truck volumes at external roads, but do not know how many trucks go between the roads. In these situations, a convenient technique is iterative proportional fitting. This technique updates a joint distribution to match two or more marginal distributions within a particular tolerance. IPF is complicated to explain but easy to demonstrate, so let’s go straight into an example. Let’s say we have a forecast for AADT at three external stations in the future. We can assume that the two-way AADT is roughly even in each direction, so the inbound volume at each station is half the AADT. Let’s also say we have an estimate of where the trips coming at those stations today go today. This matrix is called a seed. # AADT projections volumes &lt;- tibble( Station = LETTERS[1:3], Volume = c(20000, 30000, 35000), AADT = Volume * 2 ) volumes ## # A tibble: 3 x 3 ## Station Volume AADT ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 20000 40000 ## 2 B 30000 60000 ## 3 C 35000 70000 # observed distribution for a seed seed &lt;- matrix(c( 0, 7501 ,12500, 8956, 0, 11879, 9146, 21044, 4687), nrow = 3, ncol = 3, byrow = TRUE) rownames(seed) &lt;- colnames(seed) &lt;- volumes$Station seed ## A B C ## A 0 7501 12500 ## B 8956 0 11879 ## C 9146 21044 4687 Note that the row and column sums of the matrix do not match the forecast. But we can multiply each row in the matrix by the new volume estimate and the proportion of the seed matrix row that is in that cell. This gives us a new estimate of the cell’s value. Mathematically, \\[\\begin{equation} S_{n+1, ij} = \\frac{m_i * S_{n, ij} }{\\sum_{i}S_{ij}} \\tag{1.6} \\end{equation}\\] Where \\(S\\) is the seed matrix and \\(m\\) is the marginal vector. We then repeat with the other marginal, \\[\\begin{equation} S_{n+2, ij} = \\frac{m_j * S_{n, ij} }{\\sum_{j}S_{ij}} \\tag{1.7} \\end{equation}\\] In this example case, the first row is \\[\\begin{align*} S_{1, 1 1} &amp;= 20000 * 0 / 20001 &amp;= 0\\\\ S_{1, 1 2} &amp;= 20000 * 7501 / 20001 &amp;= 7500.62\\\\ S_{1, 1 2} &amp;= 20000 * 12500/ 20001 &amp;= 12499.38\\\\ \\end{align*}\\] And the whole row iteration is # get factor to multiply each row by row_factor &lt;- volumes$Volume / rowSums(seed) # multiply across rows, see ?sweep() sweep(seed, 1, row_factor, &quot;*&quot;) ## A B C ## A 0.000 7500.625 12499.38 ## B 12895.608 0.000 17104.39 ## C 9178.255 21118.215 4703.53 We can write a function that does a complete round of row, then column fitting. ipf_round &lt;- function(marginal1, marginal2, seed) { # multiply the first marginal through the rows (MARGIN = 1) seed_rows &lt;- sweep(seed, MARGIN = 1, marginal1 / rowSums(seed), &quot;*&quot;) # multiply the second marginal through the columns (MARGIN = 2) seed_cols &lt;- sweep(seed_rows, MARGIN = 2, marginal2 / colSums(seed_rows), &quot;*&quot;) # return seed_cols } ipf_round(volumes$Volume, volumes$Volume, seed) ## A B C ## A 0.000 7862.609 12751.752 ## B 11684.052 0.000 17449.749 ## C 8315.948 22137.391 4798.499 If we repeat this process for several iterations, we can see that the change between successive values shrinks. We can use this change to set a tolerance for when we want the process to stop. change &lt;- vector(&quot;numeric&quot;) joint &lt;- seed for(i in 1:5){ # update joint table new_joint &lt;- ipf_round(volumes$Volume, volumes$Volume, joint) # calculate absolute error at this iteration print(sum(abs(new_joint - joint))) # update joint joint &lt;- new_joint } ## [1] 10947.1 ## [1] 1550.274 ## [1] 325.7515 ## [1] 109.2708 ## [1] 37.10879 # final estimate new_joint ## A B C ## A 0.000 7748.922 12251.164 ## B 11945.229 0.000 18047.050 ## C 8054.771 22251.078 4701.787 A few notes: IPF is not guaranteed to progressively converge. Meaning, it is possible to get stuck in a loop where the successive change between iterations does not get smaller. It is important to set a maximum number of iterations. IPF can work in any number of dimensions. A two-dimensional matrix is easy to visualize and works as a good example, but that’s by no means an upper limit. Just keep repeating. If a cell in a seed matrix has a zero value, all successive iterations will have zero. If the seed table has a structural zero, keep it in. Otherwise, you might want to consider overriding the value with a small number. The process is not consistent: A different seed matrix will lead to a different outcome. If you are uncertain about your seed matrix, you could consider taking the average of multiple potential seed matrices. 1.5.3 Regression Analysis We often want to know what will happen 1.5.4 Numerical Optimization Let’s say you have a function with a Homework Some of these questions require a completed run of the demonstration model. For instructions on accessing and running the model, see the Appendix With the TAZ layer and socioeconomic data in the demonstration model, make a set of choropleth and / or bar chart maps showing the following socioeconomic variables: total households household density (per acre) total jobs job density share of manufacturing vs office vs retail employment Compare your maps with aerial imagery from Google Maps or OpenStreetMap. Describe the spatial patterns of the socioeconomic data in the model region. Identify which zones constitute the central business district, and identify any outlying employment centers. With the highway network layer, create maps showing: link functional type; link free flow speed; and link hourly capacity. Compare your maps with aerial imagery from Google Maps or OpenStreetMap. Note that the hourly capacity is not on the input network, so you will need to use a either the loaded highway network that is output from the model, or an intermediate network after the initial highway capacity calculations. Identify the major freeways and principal arterials in the model region. Find the shortest free-flow speed path along the network between two zones. Then find the shortest distance path between the same two zones. Are the paths the same? Do the paths match what an online mapping service shows for a trip in the middle of the night? Open the highway assignment report, which shows vehicle hours and miles traveled by facility type. What percent of the region’s VMT occurs on freeways? What percent of the region’s lane-miles are freeways? Create a map of the highway links showing PM period level of service based on the volume to capacity ratios in the table below. How would you characterize traffic in Roanoke? Which is the worst-performing major facility? LOS V/C Color A &lt; 0.35 Blue B 0.35 - 0.54 LightBlue C 0.55 - 0.77 Green D 0.78 - 0.93 Yellow E 0.94 - 0.99 Orange F \\(\\geq\\) 1.00 Red Lab Demographers for the Commonwealth of Virginia — like those in other states — forecast a certain number of residents, households, and jobs for many years into the future. But this data alone is not sufficient: We believe that larger households will make more trips than smaller households. But we only know how many households and persons there are, and not how many households of each different size. For example, let’s say that we know there are three households in a zone and nine persons. Does that mean that there are three 3-person households? Or two one-person households and one 7-person household? The number and types of trips generated by these two different scenarios could be very different. A household classification model turns these raw counts of households and persons into distributions of households by size, number of workers, etc. The Roanoke classification model works in two steps: The model makes a guess at the distribution by multiplying the average persons, vehicles, or workers per household in a zone by a marginal distribution determined from Census tables. The model then adjusts this initial guess (using IPF) so that the joint distribution of households matches what is currently there. The version of the Roanoke model you have installed contains a classification model, but the marginal and joint distribution tables the model uses contain nonsense. Your task for this lab is to correct the values in these tables so that the model will calculate reasonable estimates of the current and future population. Marginal Distributions The United States Census Bureau attempts to count every person in the United States every ten years. It also surveys a 1% sample of the population each year for additional statistical questions (in a program called the American Community Survey), like income and vehicle ownership. In order to protect respondent’s privacy, Census makes different types of information available at different geographic levels. For every Census tract in the country, we can get these marginal distribution tables: Number of households by household size Number of households by workers per household Number of households by vehicles available. Using these data, we can calculate how the average distribution of households changes based on the average household size in the tract. Figure 1.7 shows precisely this distribution, using a cubic polynomial to fit the average distribution. Each set of four points represents one census tract; for instance, the lowest average household size is in tract 51770001100 with 1.37 persons per household on average. This tract shows 71% of its households having 1 member, 25% having 2, and about 2 percent each of 3 and 4+ person households. Figure 1.7: Distribution of households by size, based on average households size. For the most part the distributions make sense, with a few exceptions. It makes sense that the proportion of 1-person households decreases with average size, and that the proportion of 4-person households increases. Each of these curves turns in a weird direction at the end; however, the wide margin on the distribution means that we’re not really confident about where the actual proportion is. We’ll need to do some manual adjusting to these curves. I have provided you with initial marginal distribution curves for the three marginal distributions of interest. These files are available on Box, and the starting values are shown graphically in Figure 1.8. Figure 1.8: Raw marginal distribution curves from Roanoke region. Rules you need to follow when adjusting the curves: If the average household size is one, then 100% of households must have one person. The same rule applies to zero workers and zero vehicles. If the average household size is four, there will still be some households with only one, two, or three people. This is because the 4 category really contains all households with four or more members. The total proportions across all four marginal curves at each \\(x\\) value must sum to 1. Joint Distribution In the section above we created curves to get the marginal distribution of household size, vehicles, and workers based on the average in a zone. In order to make sure that the joint distribution of all of these variables is correct, we will use IPF with a joint table as the seed. Census does not independently publish 3-dimensional tables, but the Census Transportation Planning Package (CTPP) is a partnership between AASHTO and the Census Bureau and it publishes key tables not available in other places. The CTPP data is available through the AASHTO website, or by searching “CTPP”. Find table A112305 for the residence counties where your group members are from. What do you see in this table? What makes sense and what does not? I have already gathered the data and done some preliminary preparation for you to compute the shares that belong in the household seed file. Run the code below in an R session (with the tidyverse library loaded) to calculate the necessary shares. Follow along with the code to understand what is happening. raw_counts &lt;- read_csv(&quot;https://byu.box.com/shared/static/03t5b6g9cw59aroqxarst55t2khnhmcz.csv&quot;) ## Parsed with column specification: ## cols( ## RESIDENCE = col_character(), ## persons = col_character(), ## workers = col_character(), ## vehicles = col_character(), ## output = col_character(), ## count = col_number() ## ) joint_distribution &lt;- raw_counts %&gt;% # the data contain both &quot;estimates&quot; and &quot;margin of error&quot;. We only want to # keep the estimates rows. filter(output == &quot;Estimate&quot;) %&gt;% # The data records size as text, with `0 workers`. We want to extract the # numeric information from these fields. This line of code says, for the variables # persons, workers, and vehicles, take all non-numeric characters and replace # them with nothing. It looks like gibberish, but it came from lots of Stack # Overflow searching. I can&#39;t come up with this stuff from thin air, you know. mutate_at( c(&quot;persons&quot;, &quot;workers&quot;, &quot;vehicles&quot;), function(x) {as.numeric(gsub(&quot;([0-9]+).*$&quot;, &quot;\\\\1&quot;, x))} ) %&gt;% # the rows that say &quot;total households&quot; get turned to NA by the above process, and we # don&#39;t want to keep those in anyways. filter(!is.na(persons), !is.na(workers), !is.na(vehicles)) %&gt;% # the roanoke model only includes households with 3+ workers, not 4+. So if a # row is for more than 3 workers, we group it as 3 mutate(workers = ifelse(workers &gt; 3, 3, workers)) %&gt;% # but this creates a problem where we now have multiple rows with # 3 workers. So we need to group and add up. group_by(persons, workers, vehicles) %&gt;% summarize(count = sum(count)) %&gt;% ungroup() %&gt;% # Finally, we want to turn the numbers into a share. So we divde the # counts by the total count in the whole table. mutate(share = count / sum(count)) ## Warning in (function (x) : NAs introduced by coercion ## Warning in (function (x) : NAs introduced by coercion ## Warning in (function (x) : NAs introduced by coercion ## `summarise()` regrouping output by &#39;persons&#39;, &#39;workers&#39; (override with `.groups` argument) Now we can see the joint distribution of size, workers, and vehicles. joint_distribution %&gt;% group_by(persons) %&gt;% select(-count) %&gt;% pivot_wider(names_from = vehicles, values_from = share, names_prefix = &quot;vehicles &quot;) %&gt;% knitr::kable() persons workers vehicles 0 vehicles 1 vehicles 2 vehicles 3 1 0 0.0424577 0.0937149 0.0193590 0.0040148 1 1 0.0129243 0.1195635 0.0273885 0.0073146 1 2 0.0000000 0.0000000 0.0000000 0.0000000 1 3 0.0000000 0.0000000 0.0000000 0.0000000 2 0 0.0079746 0.0269485 0.0441076 0.0211739 2 1 0.0088545 0.0369580 0.0531271 0.0283235 2 2 0.0019249 0.0120993 0.0742460 0.0378930 2 3 0.0000000 0.0000000 0.0000000 0.0000000 3 0 0.0031348 0.0067096 0.0040148 0.0029698 3 1 0.0041248 0.0192490 0.0178740 0.0114394 3 2 0.0008250 0.0051697 0.0323383 0.0255736 3 3 0.0002750 0.0005500 0.0038498 0.0146842 4 0 0.0015949 0.0033548 0.0010999 0.0024749 4 1 0.0025299 0.0140792 0.0231537 0.0153992 4 2 0.0015399 0.0087445 0.0382779 0.0257936 4 3 0.0001540 0.0021999 0.0027499 0.0237037 This table serves as the seed for the IPF process in the household classification Save this data table as a DBF file that you can put into the model at params/classification/hh_seed.dbf. The write.dbf() function is part of the foreign library. as.data.frame(joint_distribution) %&gt;% foreign::write.dbf(&quot;data/hh_seed.dbf&quot;) Report For your homework, you should have a complete run of the uncalibrated, “bare” RVTPO model. Open the “Trip Generation” submodule by double-clicking on the yellow box labeled Trip Generation. This submodule has two steps: the household classification model and the trip productions model. You can run just this step of the model (and not the entire travel model) by clicking the “Run Application” button in the top-left of the Cube window and then selecting the “Run current group only” option in the dialog. Open the classified socioeconomic data file that is an input to the trip productions step (the file is named se_classified_2012A.dbf), and make a tabulation report counting up the number of households in each district in the following three categories: W3V0 Households with three or more workers and no vehicles. W0V3 Households with no workers and three or more vehicles. P2V2 Households with two people and two vehicles. The marginal classification curves and the joint distribution seed table are stored in the params/classification folder. Each file is a .dbf with the following format: hh_size_lookup.dbf: [AVG, PERSONS_1, PERSONS_2, PERSONS_3, PERSONS_4] hh_workers_lookup.dbf: [AVG, WORKERS_0, WORKERS_1, WORKERS_2, WORKERS_3] hh_vehicles_lookup.dbf: [AVG, VEHICLES_0, VEHICLES_1, VEHICLES_2, VEHICLES_3] hh_seed.dbf: [WORKERS, PERSONS, VEHICLES, SHARE] Open these files in Cube, and observe that the data contained them is nonsense. Replace the files in the params/classification folder with the distributions you created above, including the marginal distribution curves you calibrated by hand and the household seed you constructed from CTPP. Run the trip generation submodule again, and recreate the tabulation report you previously created. Do these numbers make more sense? You can edit the DBF files in Cube by hand; its copy-paste feature is not what you would expect from a software program in 2020 (or 2005). A more efficient way might be to use the write.dbf() function in R. # read the file you edited; there is also a readxl() function for excel # spreadsheets read_csv(&quot;data/raw_size.csv&quot;) %&gt;% # need to make sure the names on the columns match what Cube is expecting to # get; you&#39;ll need to change this for the workers and vehicles rename(AVG = average, PERSONS_1 = `1`, PERSONS_2 = `2`, `PERSONS_3` = `3`, PERSONS_4 = `4`) %&gt;% # read_csv makes a modern data frame called a `tibble`. The old DBF function # has no idea what do to with this. So we need to convert the tibble to an # old-fashioned `data.frame` as.data.frame() %&gt;% foreign::write.dbf(&quot;data/hh_size_lookup.dbf&quot;) The classification model incorporates a zone-level IPF process executed in R; the script for this process is in R/classification.R. Open the script and find the place where the maximum IPF iterations are set. Change this parameter to 50, save the R file, and and re-run both the model and your tabulation report. Change it again to 100 and note the execution time and any changes in the tabulation report. How many iterations should you run? Write a lab report describing the household classification model you have developed. Describe how you developed your marginal disaggregation curves, including any assertions you employed in smoothing /adjusting the curves. Include plots of each curve. Describe how you determined the number of iterations of IPF to run in your model. Compare the distribution of households by classification category to the joint distribution you obtained from the CTPP. References "],
["chap-tripgen.html", "Chapter 2 Trip Generation 2.1 Trip Production 2.2 Trip Attraction 2.3 Trip Generation Homework 2.4 Trip Generation Lab", " Chapter 2 Trip Generation The purpose of the trip generation model is to turn socioeconomic data into a certain number of trips. Every trip has two ends: an origin and a destination. But because we do not yet know where trips are going, at this stage of the model we instead forecast trip productions and trip attractions. Imagine a basic day where you travel from home to work, and then back to home. You made two trips. The first trip originated at your home and was destined for work: your second trip originated at work and was destined for home. But your home produced two trips, and your workplace attracted two trips. This distinction is critical. In a trip production model, we use the attributes of households to figure out how many trips each zone produces. In a trip attraction model, we use socioeconomic data to determine how many trips are attracted to the zone. It would be oversimplifying to say that all trip production happens at households: businesses produce commercial trips, and households can also attract trips from other households. But in this class, we are mostly going to concern ourselves with household trip production. Trip generation models are separated by trip purpose; trip purposes are defined by the kind of activity occurring at the production and attraction ends of a trip. Common trip purposes include: Home-based work (HBW): trips produced at a home and attracted to a work place. Home-based school / college (HBSchool): trips produced at a home and attracted to a school or college. Home-based shopping / recreational (HBShop): trips produced at a home and attracted to a shopping or recreational activity. Home-based other (HBO): trips produced at a home and attracted to any activity not-defined above. Non Home-based (NHB): trips produced somewhere other than a home and attracted to anywhere. The specific purposes included in a model are a function of the data available and the needs of the region. If there is no university or college in a region, then the HBSchool purpose might be sufficient; otherwise, there might need to be a HBSchool and a HBCollege. Or, if there is no data on trips by students, this purpose might just get folded into HBO. The next two sections give details of how to construct a cross-classification model of trip production as well as a regression model of trip attractions. More details of trip Generation models are given in Section 4.4 of NCHRP Report 716. 2.1 Trip Production The trip production model is a cross-classification model. What this means is that we will classify households into different bins based on their household size, income, vehicle ownership, etc. We will then calculate the average number of trips made by households in each group. We will need to get the data for households and trips. Use the records for households in MSA size 02, that completed the survey on a weekday, and then filter the trips to include only those records. We can also select only the data columns that have the information we will use to classify the model. We might need to create some or modify variables that we need to use to cross-classify; for instance we should cap the household size category at 4 people and the vehicles at 3. library(tidyverse) library(nhts2017) hh &lt;- nhts_households %&gt;% # filter to MSA size 2, travel on weekday filter(msasize == &quot;02&quot;, !travday %in% c(&quot;01&quot;, &quot;07&quot;)) %&gt;% # select the columns we care about. select(houseid, wthhfin, hhsize, hhvehcnt, numadlt, hhfaminc, wrkcount) %&gt;% mutate( hhsize = ifelse(hhsize &gt; 4, 4, hhsize), hhvehcnt = ifelse(hhvehcnt &gt; 3, 3, hhvehcnt), wrkcount = ifelse(wrkcount &gt; 2, 2, wrkcount) ) hh ## # A tibble: 10,381 x 7 ## houseid wthhfin hhsize hhvehcnt numadlt hhfaminc wrkcount ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr+lbl&gt; &lt;dbl&gt; ## 1 30000019 279. 2 2 2 03 [$15,000 to $24,999] 0 ## 2 30000288 103. 1 2 1 05 [$35,000 to $49,999] 0 ## 3 30000289 244. 3 3 2 07 [$75,000 to $99,999] 1 ## 4 30000463 348. 2 2 2 06 [$50,000 to $74,999] 2 ## 5 30000465 133. 4 2 2 08 [$100,000 to $124,999] 2 ## 6 30000478 120. 2 0 2 03 [$15,000 to $24,999] 0 ## 7 30000545 35.7 2 3 2 06 [$50,000 to $74,999] 2 ## 8 30000770 130. 1 1 1 06 [$50,000 to $74,999] 1 ## 9 30000983 147. 4 3 4 09 [$125,000 to $149,999] 1 ## 10 30001177 304. 2 0 2 04 [$25,000 to $34,999] 2 ## # … with 10,371 more rows The next step is we need to calculate how many trips the members of each household in the data took. To do this, we can use summarise to count the number of trip rows for each household. Then, we can pivot_wider to spread the trips out by purpose. trips &lt;- nhts_trips %&gt;% # filter to households in the data filter(houseid %in% hh$houseid) %&gt;% group_by(houseid, trippurp) %&gt;% # count up how many trips each household took summarise(trips = n()) %&gt;% # &quot;spread&quot; the data, filling zero if no trips were taken pivot_wider(id_cols = houseid, names_from = trippurp, values_from = trips, values_fill = 0) ## `summarise()` regrouping output by &#39;houseid&#39; (override with `.groups` argument) trips ## # A tibble: 9,518 x 7 ## # Groups: houseid [9,518] ## houseid HBO HBSHOP NHB HBSOCREC HBW `-9` ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 30000019 4 0 0 0 0 0 ## 2 30000288 1 1 4 0 0 0 ## 3 30000289 2 4 3 1 1 0 ## 4 30000463 7 2 6 0 0 0 ## 5 30000465 8 2 9 0 1 0 ## 6 30000478 2 0 0 0 0 0 ## 7 30000545 0 2 2 4 2 0 ## 8 30000770 0 3 1 0 1 0 ## 9 30000983 4 0 2 1 2 0 ## 10 30001177 3 2 0 2 2 0 ## # … with 9,508 more rows Now, we will join the trips data frame to the households data frame so that everything is in one place. Note that when we do this, there will be some households that never made any trips; we need to change their trip counts from NA to 0. # function to change NA to 0 nato0 &lt;- function(x) {ifelse(is.na(x), 0, x)} tripprod &lt;- hh %&gt;% # join tables by id field left_join(trips, by = &quot;houseid&quot;) %&gt;% # change all NA values in columns from the trips data to 0.a mutate_at(vars(names(trips)), nato0) tripprod ## # A tibble: 10,381 x 13 ## houseid wthhfin hhsize hhvehcnt numadlt hhfaminc wrkcount HBO HBSHOP NHB ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr+lb&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 300000… 279. 2 2 2 03 [$15… 0 4 0 0 ## 2 300002… 103. 1 2 1 05 [$35… 0 1 1 4 ## 3 300002… 244. 3 3 2 07 [$75… 1 2 4 3 ## 4 300004… 348. 2 2 2 06 [$50… 2 7 2 6 ## 5 300004… 133. 4 2 2 08 [$10… 2 8 2 9 ## 6 300004… 120. 2 0 2 03 [$15… 0 2 0 0 ## 7 300005… 35.7 2 3 2 06 [$50… 2 0 2 2 ## 8 300007… 130. 1 1 1 06 [$50… 1 0 3 1 ## 9 300009… 147. 4 3 4 09 [$12… 1 4 0 2 ## 10 300011… 304. 2 0 2 04 [$25… 2 3 2 0 ## # … with 10,371 more rows, and 3 more variables: HBSOCREC &lt;dbl&gt;, HBW &lt;dbl&gt;, ## # `-9` &lt;dbl&gt; Now we can count up the number of trips by grouping the variables we care about and taking the average. For instance, we can get the average HBO trip rate for households by size and vehicle count. Remember to weight! hbo_tripprod &lt;- tripprod %&gt;% group_by(hhsize, hhvehcnt) %&gt;% summarise( n = n(), # number of households in category HBO = weighted.mean(HBO, wthhfin), # average HBO trips per hh ) ## `summarise()` regrouping output by &#39;hhsize&#39; (override with `.groups` argument) hbo_tripprod ## # A tibble: 16 x 4 ## # Groups: hhsize [4] ## hhsize hhvehcnt n HBO ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 0 345 0.414 ## 2 1 1 2263 0.558 ## 3 1 2 541 0.540 ## 4 1 3 207 0.339 ## 5 2 0 55 0.420 ## 6 2 1 782 1.55 ## 7 2 2 2472 1.15 ## 8 2 3 1282 1.12 ## 9 3 0 25 3.98 ## 10 3 1 161 1.90 ## 11 3 2 422 2.04 ## 12 3 3 561 1.93 ## 13 4 0 12 3.32 ## 14 4 1 120 5.28 ## 15 4 2 556 4.31 ## 16 4 3 577 4.19 2.2 Trip Attraction Trip attraction models estimate how many trips will be attracted to a particular zone. This is a function of how many jobs of different kinds are in a zone, in addition to other elements of a zone. Trip attraction models are often a linear regression model. The NHTS cannot be used to estimate trip attraction models because we do not know how many trips went to each TAZ; we only see the household side of the survey. So we will use a file that I have prepared from the Puget Sound Regional Council (PSRC, Seattle) household travel survey. This file is available on Box. You can download it and read it directly into an R session with the read_csv() function, psrc_attractions &lt;- read_csv(&quot;https://byu.box.com/shared/static/7ci8vomip719bdno7xl5ftjj940dausm.csv&quot;) ## Parsed with column specification: ## cols( ## attr_tract = col_double(), ## HBO = col_double(), ## HBShop = col_double(), ## HBW = col_double(), ## NHB = col_double(), ## tothh = col_double(), ## retl = col_double(), ## manu = col_double(), ## offi = col_double(), ## gved = col_double(), ## othr = col_double(), ## totemp = col_double() ## ) psrc_attractions ## # A tibble: 643 x 12 ## attr_tract HBO HBShop HBW NHB tothh retl manu offi gved othr ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.30e10 8.92e3 1.02e3 6151. 4.70e2 3779 508 121 680 28 199 ## 2 5.30e10 2.58e4 1.48e2 0 1.03e4 3654 146 0 605 264 117 ## 3 5.30e10 3.90e1 1.30e1 0 8.50e2 1187 50 0 574 0 0 ## 4 5.30e10 1.22e3 4.78e2 83.1 2.79e3 3760 385 0 773 211 99 ## 5 5.30e10 5.08e3 3.53e2 0 5.30e3 2437 97 53 981 27 124 ## 6 5.30e10 0. 0. 0 9.89e1 1251 0 0 96 2 93 ## 7 5.30e10 9.79e3 4.22e3 1937. 3.22e4 3413 997 0 4017 527 101 ## 8 5.30e10 5.66e3 3.24e1 178. 4.20e3 2222 315 0 967 19 89 ## 9 5.30e10 1.65e3 1.08e0 0 2.22e3 1067 0 0 33 61 15 ## 10 5.30e10 2.07e3 0. 0 4.25e0 868 0 0 80 0 5 ## # … with 633 more rows, and 1 more variable: totemp &lt;dbl&gt; This file has, for every tract in the Seattle metro region, how many trips were attracted to the tract by purpose as well as the households and jobs by type in that tract. Let’s look at the relationship between HBW trips and total employment: ggplot(psrc_attractions, aes(x = totemp, y = HBW)) + geom_point() + stat_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 1 rows containing non-finite values (stat_smooth). ## Warning: Removed 1 rows containing missing values (geom_point). We can estimate a linear regression model with the lm function. In this function we specify the model as y ~ x + .... lm_hbw_totemp &lt;- lm(HBW ~ totemp, data = psrc_attractions) summary(lm_hbw_totemp) ## ## Call: ## lm(formula = HBW ~ totemp, data = psrc_attractions) ## ## Residuals: ## Min 1Q Median 3Q Max ## -41447 -1955 -1180 -699 56593 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 530.34960 300.96059 1.762 0.0785 . ## totemp 0.98197 0.04386 22.388 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6902 on 640 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.4392, Adjusted R-squared: 0.4383 ## F-statistic: 501.2 on 1 and 640 DF, p-value: &lt; 2.2e-16 Let’s estimate a more complex for HBO trips with many predictors. hbo_rates &lt;- lm(HBO ~ tothh + retl + manu + offi + gved + othr, data = psrc_attractions) summary(hbo_rates) ## ## Call: ## lm(formula = HBO ~ tothh + retl + manu + offi + gved + othr, ## data = psrc_attractions) ## ## Residuals: ## Min 1Q Median 3Q Max ## -24039 -6216 -4083 -818 227788 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -394.4522 2045.5441 -0.193 0.84715 ## tothh 3.0939 0.9421 3.284 0.00108 ** ## retl 2.8846 1.4143 2.040 0.04180 * ## manu 1.3558 1.5214 0.891 0.37320 ## offi 0.3982 0.2207 1.804 0.07165 . ## gved 0.5974 0.4671 1.279 0.20137 ## othr 0.0726 0.9776 0.074 0.94082 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17030 on 635 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.07254, Adjusted R-squared: 0.06378 ## F-statistic: 8.278 on 6 and 635 DF, p-value: 1.199e-08 The \\(R^2\\) statistic is not particularly good, but it really never will be with this kind of data. More important is the relationship with the residuals. It’s also not very good; there are a few outliers and quite a bit of heteroskedasticity. But there may be things we can try to make it better. plot(hbo_rates, which = 1) 2.3 Trip Generation Homework Calculate the trip rates for each purpose by household size, and by income group. Do the rates make sense? Why or why not? Calculate the trip rates for each purpose by the number of household workers and the vehicle availability. Do the rates make sense? Why or why not? Calculate the variance or standard deviation in work trip rates by household size / vehicles and by number of workers / vehicles. Which classification should be used for work trips? Calculate the number of households in each classification (size / vehicles), (workers / vehicles). What information does this give you about the estimated trip rates? Estimate trip rate attraction models for all the trip purposes. Present models with only significant or influential factors (try a few different specifications until you are satisfied with your models’ performance) Explain your attraction rate models; do the rates make sense? Which models have the best fit in terms of \\(R^2\\) value? Why? Remove the intercept from your model estimations. In R, you can do this by adding a -1 to the formula, as in lm(y ~ x - 1). Do the rates change? By how much? Should you keep the intercept in or remove it? 2.4 Trip Generation Lab In this lab you will implement and calibrate trip generation rates for the RVTPO model. 2.4.1 Trip Production The trip production rates are stored in the params/trip_prod/ folder, with a dbf file for each trip purpose in the model. We are going to calibrate the following trip purposes: HBW: cross-classification model of workers and vehicles available HBO: cross-classification model of household persons and vehicles available HBShop: cross-classification model of household persons and vehicles available The household travel survey for the RVTPO region reported the following total weighted trips in these trip purposes: Purpose Weighted Survey Trips HBW 118,653 HBO 267,987 HBShop 129,614 Begin by running the RVTPO model through the Trip Generation submodel. The trip productions for each trip purpose are recorded in Base/outputs/HH_PROD.dbf. Using this file, create a report that sums the trips produced in each of these three purposes. You can read this file in R using the read.dbf() function in the foreign library, and then sum all columns in this table using the summarize_all function. # read roanoke household trip productions rvtpo_productions &lt;- foreign::read.dbf(&quot;data/HH_PROD.DBF&quot;) %&gt;% as_tibble() # show first 10 rows rvtpo_productions ## # A tibble: 267 x 7 ## TAZ HBWP NHBWP HBOP HBSCP HBSHP NHBOP ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 199. 113. 199. 112. 199. 384. ## 2 2 38.6 22.0 38.6 21.8 38.6 74.5 ## 3 3 88 50.2 88 49.7 88 170. ## 4 4 265. 151. 265. 150. 265. 512. ## 5 5 116. 65.9 116. 65.3 116. 223. ## 6 6 69.0 39.4 69.0 39.0 69.0 133. ## 7 7 26.7 15.2 26.7 15.1 26.7 51.7 ## 8 8 174. 99.2 174. 98.2 174. 336. ## 9 9 82.2 46.9 82.2 46.5 82.2 159. ## 10 10 177. 101. 177. 100. 177. 343. ## # … with 257 more rows rvtpo_productions %&gt;% summarize_all( sum ) ## # A tibble: 1 x 7 ## TAZ HBWP NHBWP HBOP HBSCP HBSHP NHBOP ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 35778 28200. 16093. 28200. 15933. 28200. 54513. Obviously the bare model is not very close to the targets. Replace the bare model rates with rates you estimated from the NHTS during your homework assignment. Run your tabulation report again, and compare the total productions to the regional targets. Adjust the trip rates so that they replicate the regional survey targets within an acceptable margin of error. Ensure that the comparative relationship between the trip rates is maintained (i.e. households with more workers must make at least as many work trips). 2.4.2 Trip Attraction The trip attraction rates enter into the destination choice model, but it is helpful to compare the total forecasted attractions using the rates you estimated. Apply the rates to the zonal socioeconomic data file and calculate the total number of attracted trips for each of the three purposes. Adjust the rates so that the total attractions for your three estimated purposes match the regional totals. 2.4.3 Report Prepare a technical report describing the process by which you estimated household trip production and attraction rates, and calibrated the trip rates to reproduce regional totals. Calculate the margin of error for all purposes (not just the purposes you calibrated). Discuss how well the calibrated trip generation models replicates the survey targets, and justify your residual error. Use formatted tables to display results instead of screenshots of output. You will be graded on the overall readability, flow, formatting and grammar in addition to how clearly you articulate the process of your work. "],
["chap-distribution.html", "Chapter 3 Trip Distribution Homework", " Chapter 3 Trip Distribution Homework The figure above4 presents a simple three zone system, the link travel times for this system (for internal trips, \\(t_{ii}=2\\) globally), and the zonal productions and attractions. Assume a gravity model of the form. \\[T_{ij}=\\frac{P_iA_j^*(t_{ij})^{-b}}{\\sum_{j&#39;}A^*_{j&#39;}(t_{ij})^{-b}}\\] where \\(A_j^*\\) is a ``modified attraction term’’ defined by the algorithm shown in the figure below. This algorithm constrains the predicted trips to a given zone to equal the true zonal attractions \\(A_j\\). Question 1: Write a computer program or build a spreadsheet that computes the O-D flows for this system using the algorithm (with \\(\\varepsilon=1\\)). Find the value of \\(b\\) (to a single decimal place) which provides the “best fit” with the observed O-D matrix in the table below. Discuss how you determined which \\(b\\) value gave the “best fit.” \\(i\\) 1 2 3 1 80 5 15 2 80 40 80 3 40 5 55 Question 2: The region represented in Question 1 has begun improvements to the link between zones 1 and 2 that will reduce the travel time from 5 to 3. Using your program and the value of \\(b\\) estimated above, determine the effect of this improvement on the predicted trip distribution matrix. Is the response reasonable? This problem is adapted with permission from Urban Transportation Planning: Second Edition by Michael D. Meyer and Eric J. Miller.↩︎ "],
["chap-modechoice.html", "Chapter 4 Mode and Destination Choice Homework 4.1 Lab", " Chapter 4 Mode and Destination Choice Homework Let’s first start with a couple of practice problems before using data to estimate multinomial logit models. Calculate the Non-motorized Travel Time, Auto Utility, Non-motorized Utility, Auto Probability, Non-motorized Probability, and the Mode Choice Logsum using the following information: \\(\\beta\\)1 = -0.025, \\(\\beta\\)2 = -0.06257, \\(\\alpha\\)1 = 1, \\(\\alpha\\)2 = -1.2258 \\[Highway Time = \\begin{vmatrix}0.77 &amp; 1.55 &amp; 21.60\\\\ 1.55 &amp; 0.77 &amp; 20.51\\\\22.02 &amp; 20.93 &amp; 1.21 \\end{vmatrix}\\] \\[ Highway Distance = \\begin{vmatrix}0 &amp; 0.72 &amp; 12.87\\\\ 0.72 &amp; 0 &amp; 12.49\\\\12.82 &amp; 12.44 &amp; 0 \\end{vmatrix}\\] Calculate the Utility, Probability, Trips, and Destination Choice Logsum. Check your answer to make sure the rowsum of the Trips is equal to the productions for each row. The Mode Choice Logsum found in Question 1 should be used as the impedance. Use the following information: \\(\\beta\\)2 = 1.1657, \\(\\alpha\\)2 = Attractions n Productions 1 0.6 2 1.8 3 1.3 n Attractions 1 125 2 180 3 210 Now, for this week’s assignment, you will use data from the 2000 Bay Area Travel Survey to estimate multinomial logit models that predict mode choice for work trips. The data is available on on Box. The data are listed as worktrips_logitdata.rds. We will also need to load the mlogit library package, which contains the tools necessary to estimate multinomial logit models. library(mlogit) ## Warning: package &#39;mlogit&#39; was built under R version 4.0.2 ## Loading required package: dfidx ## ## Attaching package: &#39;dfidx&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## filter worktrips &lt;- read_rds(&quot;data/worktrips_logitdata.rds&quot;) Because multinomial logit models are so different from other models, the data are stored in a special type of data frame. You can see the first several rows of this data below; Person 1 in household 2 has five alternatives, and they chose to drive alone. Person 3 chose to take transit. head(worktrips[,1:8], n=12) ## HHID PERID CASE ALTNUM NUMALTS CHOSEN IVTT OVTT ## 2-1. Drive Alone 2 1 1 Drive Alone 5 TRUE 13.38 2.0 ## 2-1. Share 2 2 1 1 Share 2 5 FALSE 18.38 2.0 ## 2-1. Share 3+ 2 1 1 Share 3+ 5 FALSE 20.38 2.0 ## 2-1. Transit 2 1 1 Transit 5 FALSE 25.90 15.2 ## 2-1. Bike 2 1 1 Bike 5 FALSE 40.50 2.0 ## 3-1. Drive Alone 3 1 2 Drive Alone 5 FALSE 29.92 10.0 ## 3-1. Share 2 3 1 2 Share 2 5 FALSE 34.92 10.0 ## 3-1. Share 3+ 3 1 2 Share 3+ 5 FALSE 21.92 10.0 ## 3-1. Transit 3 1 2 Transit 5 TRUE 22.96 14.2 ## 3-1. Bike 3 1 2 Bike 5 FALSE 58.95 10.0 ## 5-1. Drive Alone 5 1 3 Drive Alone 4 TRUE 8.60 6.0 ## 5-1. Share 2 5 1 3 Share 2 4 FALSE 13.60 6.0 Now that your data is cleaned and formatted, you can estimate multinomial logit models. To do this, use the mlogit() function, in a manner sort of like you would use the lm() command. One thing to look out for: the difference between generic and alternative-specific variables.5 Generic Variables: These are coefficients with a single estimated parameter. That is, the \\(\\hat{\\beta}\\) coefficient for these variables has the same value in the utility equation for every alternative. These estimates come from variables that vary naturally across the alternatives, like the cost of travel. Alternative-Specific Variables: This type of coefficient has a unique estimate for each alternative. That is, \\(\\hat{\\beta}_{DA}\\) is different from \\(\\hat{\\beta}_{Walk}\\). This type of estimate comes from variables that are constant across alternatives, like the distance of the trip. To specify the model, we use the following construction. fit.mnl &lt;- mlogit ( CHOICE ~ Generic | Alt.Specific, data = logitdata ) To examine the model output, the standard summary() command will produce a coefficients table and key test statistics. The the texreg package will produce convenient model comparison tables that can be included in a report or pasted into Excel for further formatting. For your homework, please include a model comparison table rather than a print out of each model summary. Question 1: Calculate the likelihood of a model with no covariates (equal-shares) and a model with constants only (market shares). Estimate a model with only the travel time, and calculate the McFadden pseudo-\\(R^2\\) statistic with respect to the equal shares model and the market shares model. Which statistic is reported by the summary() command? Why is this important? Question 2: Estimate a model with just the total travel time (TVTT) and the cost of the trip (COST). These two parameter estimates will allow you to calculate the value of time for the sample population as \\[VOT = \\frac{60\\hat{\\beta}_{TVTT}}{100 \\hat{\\beta}_{COST}}\\] Report the value of time you calculate. Is this reasonable? Question 3: Estimate a model with the out-of-vehicle travel time (OVTT), and the in-vehicle travel time (IVTT). What is the ratio of these parameters? What does this tell you about how people feel waiting for the bus? Question 4: Estimate a model with the residential population density (RSPOPDEN) and the workplace employment density (WKEMPDEN), controlling for the affordability of the trip (COSTINC). Does land use at the origin or the destination of the trip affect the mode choice problem more? Is it different by mode? Question 5: Estimate a nested logit model including cost, travel time, out-of-vehicle travel time, and workplace employment density. Group car alternatives into one nest, and non-car alternatives into another. Constrain the nesting parameter to a single value (un.nest.el = TRUE). What is the estimated value of the nesting parameter? What are the implications of this parameter value for across-nest substitution? Question 6: Estimate another nested logit model with the same nests, but this time segment the data on income; include households making less than $50k/year in one segment and households making at least $50k in the other. Add vehicles per worker as a covariate (VEHBYWRK). Comment on how the two segments respond to the different covariates. Which matters more to which group? Question 7. Of the models you estimated, which is the preferred in terms of model likelihood? What about in terms of behavioral sensitivity? Hint: A tool to compare models is the screenreg() function in library(texreg). 4.1 Lab In practice, small-sample surveys have a difficult time generating estimates of choice parameters that are both precise and rational. As a result, it is common to assert choice coefficients that have worked well in comparable cities and then calibrate the mode-specific constants and distance decay parameters to match your targets. In this lab you will calibrate the mode choice models and the destination choice models for the following trip purposes: Home-based Work Home-based Other Home-based Shopping In the Roanoke mode choice model, HBO and HBShopping get combined for mode choice. So you will calibrate three purposes in the destination choice model, but only two in the mode choice model. Even though trip distribution comes first in the standard four-step process, we end up using the mode choice logsums mode choice to inform destination choice. Mathematically they happen simultaneously, but in the model it goes skims &gt; mode choice logsums &gt; destination choice &gt; mode choice As a result you will need to calibrate the models iteratively: first adjust the mode choice constants, then the distance decay parameter, then mode choice, etc., until you are satisfied that the model meets the targets. 4.1.1 Mode Choice Calibration The coefficients are found in the ./Params/mc/MC_Coefficients.csv file. Record the coefficients in a table in your lab report accompanying a description of the purpose of each coefficient and any notable values. These coefficients are fixed; they should not change as part of this exercise. read_csv(&quot;data/rvtpo_data/MC_Coefficients.csv&quot;) %&gt;% pander::pander() ## Parsed with column specification: ## cols( ## `;N` = col_double(), ## Name = col_character(), ## HBW = col_double(), ## HBO = col_double(), ## NHB = col_double(), ## HBSC = col_double() ## ) ;N Name HBW HBO NHB HBSC 1 CIVTT -0.025 -0.015 -0.02 -0.015 2 CSWAIT -0.05625 -0.03375 -0.045 -0.03375 3 CLWAIT -0.025 -0.015 -0.02 -0.015 4 CXWAIT -0.0625 -0.0375 -0.05 -0.0375 5 CCOST -0.00158 -0.00237 -0.00253 -0.18 6 CDRIVE -0.05625 -0.03375 -0.045 -0.03375 7 CTERML -0.0625 -0.0375 -0.05 -0.0625 8 CWALK -0.0625 -0.0375 -0.05 -0.0375 9 CWALK1 -0.0625 -0.0375 -0.05 -0.0375 10 CWALK2 -0.09375 -0.05625 -0.075 -0.05625 11 CBIKE1 -0.0625 -0.0375 -0.05 -0.0375 12 CBIKE2 -0.09375 -0.05625 -0.075 -0.05625 13 DWalkBIKE 1 1 1 1 14 NC1 0.5 0.5 0.5 0.5 15 NC2 0.5 0.5 0.5 0.5 16 NC3 0.5 0.5 0.5 0.5 17 CBD 0 0 0 0 18 NXFER 0 0 0 0 19 AUTOCOST 13.6 13.6 13.6 13.6 20 SHAREFAC 2 2 2 2 The mode-specific constants are in a separate file, ./Params/mc/MC_Constants.csv. The reference alternative in the choice model is Drive Alone, and the alternatives with their nesting structure are: Trips |-- Auto | |--Drive | |--Share | |-- Transit | |--Local | |--Premium | |-- Non-motorized read_csv(&quot;data/rvtpo_data/MC_Constants.csv&quot;) %&gt;% pander::pander() ## Parsed with column specification: ## cols( ## `;N` = col_double(), ## Name = col_character(), ## HBW = col_double(), ## HBO = col_double(), ## NHB = col_double(), ## HBSC = col_double() ## ) ;N Name HBW HBO NHB HBSC 1 K_SR -1.17 0.0164 -0.0336 -1.169 2 K_TRN -0.3903 -1.981 -2.271 0.3261 3 K_NMOT -1.226 -0.3834 -0.8655 -1.25 4 K_PREM 0 0 0 0 5 NA 0 0 0 0 6 NA 0 0 0 0 7 NA 0 0 0 0 8 NA 0 0 0 0 9 NA 0 0 0 0 10 NA 0 0 0 0 There is currently no premium service in the model, so changing its constant will not result in more people taking it. But the structure for it exists, if the MPO wants to look at some future transit options. For initial values for the alternative-specific constants, you can use parameters you estimated in the homework. After you run the destination choice model and adjust its calibration (see below), run the mode choice model and look at the mode choice report. Calculate new alternative specific constants using the bias adjustment formula, \\[\\alpha_{n+1} = \\alpha_n + \\log(A_j / S_j) \\] Where \\(A_j\\) is the population share (target) and \\(S_j\\) is the model share. The population shares (targets) are given below. # Population shares (targets) read_xlsx(&quot;data/rvtpo_data/mc.xlsx&quot;) %&gt;% knitr::kable(digits = 1) Mode HBW HBO Drive Alone 87.5 44.1 Share 8.4 45.5 Local Bus 2.7 0.5 Premium 0.0 0.0 Non-motorized 1.4 9.9 4.1.2 Destination Choice Calibration A destination choice model has three basic components: A size term: these are the trip attraction rates An impedance term: this is the mode choice logsum Calibration constants Return to your trip generation lab where you estimated and adjusted attraction rates for different land uses. These attraction rates are the size terms of the destination choice model; replace the dummy rates in the model with your estimated rates. The coefficient on the logsum term should match the nesting parameter in the mode choice model. This link is what allows for a simultaneous mode and destination choice. Instead of alternative-specific constants, a destination choice model includes calibration constants based on distance-decay functions. By adjusting these parameters, we can make the modeled trip length frequency distribution match the observed distribution. The roanoke model has three basic versions that can be combined (but really don’t need to be) A distance polynomial (\\(\\beta_d d + \\beta_{d2} d^2 + \\beta_{d3} d^3\\)) A logarithmic decay function (\\(\\beta_d \\log(d)\\)) A set of bins for trips within specific mile ranges, 0-1, 1-2, etc. There is also an intrazonal constant, which we can leave alone for this lab. Remember that most of the work should be done by the logsum and the size term, and that the constants shouldn’t generally violate basic travel behavior theory. These constants are really just here to nudge the distribution in one direction or another. The table and figure below show the observed trip length frequency distributions for the purposes you need to calibrate. An Excel file with the values is on LearningSuite. tlfd &lt;- read_xlsx(&quot;data/rvtpo_data/tlfd.xlsx&quot;) knitr::kable(tlfd) HIGH MID LOW HBW HBO HBSH 1 0.5 0 3.9039039 19.6131359 13.9606519 2 1.5 1 5.0050050 7.9734108 10.0849391 3 2.5 2 8.3083083 11.9601161 23.5531056 4 3.5 3 10.0100100 12.9567925 13.8690052 5 4.5 4 10.3103103 8.9700871 8.5542211 6 5.5 5 9.0090090 6.9767344 7.1484507 7 6.5 6 13.0130130 7.5098897 6.7094465 8 7.5 7 9.5095095 4.7840465 4.3326161 9 8.5 8 7.6076076 5.0830494 2.8884107 10 9.5 9 5.8058058 4.9833817 3.0809714 11 10.5 10 4.2042042 4.0863730 2.4070089 12 11.5 11 2.0020020 1.7940174 0.4464608 13 12.5 12 1.9019019 1.8936851 2.2144482 14 13.5 13 3.0030030 0.3986705 0.0000000 15 14.5 14 1.9019019 0.6976734 0.3851214 16 15.5 15 0.5005005 0.0996676 0.1093873 17 16.5 16 1.0010010 0.0897009 0.0772576 18 17.5 17 0.3003003 0.0797341 0.0590860 19 18.5 18 0.8008008 0.0000000 0.0402657 20 19.5 19 0.0000000 0.0498338 0.0304333 21 20.5 20 1.0010010 0.0000000 0.0210982 22 21.5 21 0.9009009 0.0000000 0.0276142 ggplot(tlfd %&gt;% gather(purpose, share, HBW:HBSH), aes(x = LOW, y = share, color = purpose)) + geom_line() + xlab(&quot;Distance&quot;) + ylab(&quot;%Trips&quot;) The modeled TLFD for each purpose can be had on the model home screen as an output of the trip distribution model. Calculate the error between your observed and modeled TLFD. Determine which calibration curve you will use (you may decide to use different curves for different purposes). Find the coefficients of the curve that will compensate for the error in the model. Re-run the destination choice model with your new coefficients, run the mode choice model, and adjust the coefficients over there. 4.1.3 Report Your lab report should describe the trip distribution and mode choice models; include discussions of the coefficients and your process to calibrate the mode and distance constants. Provide results of the calibration including observed and modeled trip length frequency distributions. This can be confusing for many students; just remember that the difference between generic and alternative-specific is in the coefficients, not the variables.↩︎ "],
["chap-assignment.html", "Chapter 5 Network Assignment and Validation Homework 5.1 Laboratory Assignment", " Chapter 5 Network Assignment and Validation Homework Network The figure above6 represents a simple four-node network where 7000 vehicles travel from A to D, and 5000 travel from B to D (there are no additional trips from C to D). Link travel times for the network are given by the functions below. \\[ \\begin{aligned} t_{AD} =&amp; 20 + 0.01 q_{AD}\\\\ t_{AC} =&amp; 10 + 0.005 q_{AC}\\\\ t_{CD} =&amp; 12 + 0.005 q_{CD}\\\\ t_{BC} =&amp; 7.25 + 0.005 q_{BC}\\\\ t_{BD} =&amp; 20 + 0.01 q_{BD} \\end{aligned} \\] Question 1: Solve for the user equilibrium (UE) link flows and travel times (HINT: write and solve a set of simultaneous equations that explicitly define the UE conditions). Demonstrate that your solution is the user equilibrium by showing through example that all UE conditions are satisfied. Question 2: Perform four iterations of All Or Nothing (AON) assignment on the network and O/D volumes. Show the link flows and travel times at the end of each iteration. Question 3: Perform an incremental assignment, using trip table increments of 25% for each step. Show the link flows and travel times at the end of each incremental assignment. Question 5: Assign trips using the FHWA assignment heuristic. Show the link flows and travel times for four assignments, and the final average assignment and resulting travel times. Question 6: Compare these four traffic assignment heuristic approaches to the UE assignment and to each other. How do the resulting flow patterns differ (cite specific differences)? Which one comes closest to the UE flows? Question 7: State in words the general theory underlying each of the heuristic approaches. Which one do you prefer and why? Consider accuracy, ease of computation and the underlying theory. 5.1 Laboratory Assignment The highway volume-to-capacity curves in the Roanoke Model have already been largely calibrated7. They use the Bureau of Public Roads (BPR) format, \\[T_c = T_0 * (1 + \\alpha (V / C)^\\beta)\\] Create a plot showing the values of these curves for varying VOC ratios and discuss the implications of the different curves on different facility types in your report. Note that there are 5 facility types in the BPR table but 11 facility types in the model network. The assignment script files have comments that build a crosswalk between the two facility type definitions. For this lab, you will create a model validation report where you examine the following: Root mean squared error (RMSE) by facility type, area type, volume group, and by screenline. Are there certain classes that are outperforming others? Observed vs Modeled link volume scatterplots: an X-Y fit line by facility type as well as a maximum desirable deviation plot defined in NCHRP 765. Geographic distribution of link error. Comment on the Roanoke model’s calibration. This is an adaptation of a homework assignment from Dr. John Ivan at the University of Connecticut.↩︎ To be specific, VDOT has values that they assert for all of their models↩︎ "],
["chap-process.html", "Chapter 6 The Planning Process", " Chapter 6 The Planning Process "],
["app-demomodel.html", "A Demonstration Model A.1 Running the Model A.2 Files and Reports A.3 Network and Zone Maps", " A Demonstration Model In this class we will use the model for the Roanoke Valley Transportation Planning Organization (RVTPO), the MPO responsible for transportation planning in Roanoke, Virginia. The model is written for the CUBE travel modeling software package, the same software used by the Wasatch Front Regional Council model. The model code and files are available on Box. A few key parameters files have been reset to default values, rather than the calibrated values used in the actual model. The homework assignments and lab activities in this course will walk you through re-calibrating the model to use in your term assignments. A.1 Running the Model The model files are available from Box as a compressed file called rvtpo_bare.zip. Extract this file to a folder on your local computer. I prefer to keep my models in a folder on the C:\\ drive called C:\\projects. It may be that the C drive is not available to you, but you should place the model at a path that makes sense and that will not change from session to session. It is possible that the J: drive will not have enough space for multiple runs of the model. The path that you choose must have no spaces, from the drive letter to the final folder, i.e., C:\\folder\\folder\\rvtpo_bare. If there are any spaces your model will crash. Figure A.1: RVTPO model home folder Double-click on the roanoke.cat Cube catalog file. This will open the model application interface in Cube. On this interface you can see the steps the model will execute, as well as access the input / output files for each step. Some steps actually contain several sub-steps, and double-clicking the yellow step box will expand that application. Figure A.2: RVTPO model application interface Run the base scenario of the model by pushing the large blue “Run” button in the upper left-hand corner of the Cube application. A window will appear first asking you to confirm which scenario you are running, and then showing you the progress. This model takes approximately 15 minutes to run on my laptop.8 Complete instructions are included in the model user’s guide (in the usersguide/ folder). I have also made a YouTube video showing these steps. Note that the video shows you getting the model from Canvas; get it from Box. A.2 Files and Reports You can access files in the model in multiple ways: Through the application manager in Cube (input / output boxes) In the catalog windows on the side Directly in the File Explorer The model has a few prepared reports that you can run at any time. These are found in the “Reports” drop-down in the catalog along the left-hand side of the Cube window. These reports include: Highway vehicle miles traveled and vehicle hours traveled by facility type Mode choice by purpose Transit route-level boardings in Peak and Off-peak periods You can also make tabulations of any report. The video below has an example. A.3 Network and Zone Maps You can use Cube to create maps of network and zone data that you can use for debugging and analysis. You can also include these graphics in reports and presentations. For example, Figure A.3 shows the network links by facility type. The video below shows how to do this. Figure A.3: Facility types in the Roanoke region. A.3.1 Shortest Paths You can use Cube to measure the shortest path between two points in your model network. You can also make isochrone maps of the travel time to various destinations from a specific origin point, like the map shown in Figure A.4. The video below shows how to do this. Figure A.4: Isochrone map using network speed information. A.3.2 Working with Matrices A.3.3 Writing Custom Scripts It’s a small model with only about 250 zones; a larger model like WFRC will take many hours. Generally model run time increases with the square of the zones.↩︎ "],
["app-rstudio.html", "B R and RStudio Help B.1 Installing B.2 RStudio Orientation B.3 R Packages B.4 Working with Tables B.5 Graphics with ggplot2", " B R and RStudio Help R is a powerful, open-source statistical programming language used by both professional and academic data scientists. It is among the computer languages most suited to modern data science, and is growing rapidly in its user base and available packages. Some students may not feel comfortable working in a programming language like R or a console-based application like RStudio, especially if they have used applications primarily through a GUI. This appendix provides a basic bootcamp for R and Rstudio, but cannot be a comprehensive manual on RStudio, and it certainly cannot be one for R. Good places to get more detailed help include: R help manuals Stack Overflow Some of the sections in this appendix are text-based, and some contain little more than links to YouTube videos created by me or someone else. B.1 Installing There are two pieces of software you should install: R https://cran.r-project.org/: this contains the system libraries necessary to run R commands in a terminal on your computer, and contains a few additional helper applications. Install the most recent stable release for your operating system. RStudio https://rstudio.com/products/rstudio/download/ is an integrated application that makes using R considerably easier with text completion, file management, and some GUI features. Both software are available for Windows, MacOS, and Linux. The videos and screenshots of the application I post will use MacOS; the R code for all systems is the same, and the RStudio interface all systems is very similar with minor differences. B.2 RStudio Orientation The video below gives a very basic introduction to RStudio. There is also a very useful cheat sheet for working with RStudio on the Rstudio website. B.3 R Packages One of the strengths of R is the ability for anyone to write packages. These packages make it easier to read manipulate, and vizualize data; to estimate statistical models; or to communicate results. There are a number of ways to install additional packages. The most straightforward is to use the install.packages() function in the console. The problems in this book are solved with two additional packages9: install.packages(&quot;tidyverse&quot;) # a suite of tools for data manipulation install.packages(&quot;mlogit&quot;) # discrete choice modeling RStudio also contains a GUI interface to install and update packages. Sometimes you want to use a package that has not yet been pushed to CRAN, the international repository of “approved” R packages. This may be because the package is in development, or for one reason or another does not meet CRAN’s standards for completeness, etc. Oftentimes, the package has been made available on GitHub. You can install a package directly from GitHub with the remotes library. One package you will want for the problems in the book is the nhts2017 package on the BYU Transportation GitHub account. This package contains datasets from the 2017 National Household Travel Survey. install.packages(&quot;remotes&quot;) # tools for installing development packages remotes::install_github(&quot;byu-transpolab/nhts2017&quot;) You only need to install a package once on your computer. But every time you want to use a function in a package, you need to load the package with the library() function. To load the tidyverse packages, for instance, library(tidyverse) ## ── Attaching packages ────────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.2.9000 ✓ purrr 0.3.4 ## ✓ tibble 3.0.3 ✓ dplyr 1.0.0 ## ✓ tidyr 1.1.0 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0 ## ── Conflicts ───────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() If you get errors when you run the command above, it means that for some reason you did not install the package correctly. And if you ever get an error like kable(tibble(x = 1:2, y = c(&quot;blue&quot;, &quot;red&quot;))) ## Error in kable(tibble(x = 1:2, y = c(&quot;blue&quot;, &quot;red&quot;))): could not find function &quot;kable&quot; It often means you didn’t load the library. In this case, the kable() function to make pretty tables is part of the knitr package. library(knitr) kable(tibble(x = 1:2, y = c(&quot;blue&quot;, &quot;red&quot;))) x y 1 blue 2 red You can also use a function from a package without loading the library if you use the :: operator, like you did in the remotes::install_github() command earlier. This is handy if you only want to use one function from a package, or if you have two functions from different packages with the same name. For example, when you loaded the tidyverse package, R told you that dplyr::filter() would mask stats::filter(). So if for some reason you wanted to use the filter function from the stats package, you would need to use stats::filter(). B.4 Working with Tables Most data you will work with comes in a tabular form, meaning that the data is formatted in columns of variables and rows of observations. B.4.1 Reading Data Tabular data is often stored in a comma-separated values .csv file. To read a data file like this in R, you can use the read_csv() function included in tidyverse. trips &lt;- read_csv(&quot;data/demo_trips.csv&quot;) ## Parsed with column specification: ## cols( ## houseid = col_double(), ## personid = col_character(), ## trpmiles = col_double(), ## trippurp = col_character() ## ) print(trips) ## # A tibble: 924 x 4 ## houseid personid trpmiles trippurp ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 30182694 01 13.6 HBW ## 2 40532989 02 9.38 HBSOCREC ## 3 40729475 01 5.06 HBSHOP ## 4 40290784 01 0.509 HBSOCREC ## 5 30118876 02 0.599 NHB ## 6 30352119 01 20.3 HBO ## 7 30085077 01 22.5 NHB ## 8 30180962 02 0.581 HBSOCREC ## 9 40155356 01 2.74 HBO ## 10 30069734 01 4.65 NHB ## # … with 914 more rows This function will make a guess as to what the columns types should be. Often we want to keep ID values as characters, even if they are numeric (this preserves leading 0 values, etc.). We can tell read_csv() what types we expect with the col_types argument. trips &lt;- read_csv(&quot;data/demo_trips.csv&quot;, col_types = list(houseid = col_character())) print(trips) ## # A tibble: 924 x 4 ## houseid personid trpmiles trippurp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 30182694 01 13.6 HBW ## 2 40532989 02 9.38 HBSOCREC ## 3 40729475 01 5.06 HBSHOP ## 4 40290784 01 0.509 HBSOCREC ## 5 30118876 02 0.599 NHB ## 6 30352119 01 20.3 HBO ## 7 30085077 01 22.5 NHB ## 8 30180962 02 0.581 HBSOCREC ## 9 40155356 01 2.74 HBO ## 10 30069734 01 4.65 NHB ## # … with 914 more rows You can also write tables back to .csv with the write_csv() command. B.4.2 Modifying and Summarizing Tables In much of this section, we will work with the nhts_trips dataset of trips from the 2017 National Household Travel Survey in the nhts2017 package you installed from GitHub above. library(nhts2017) trips &lt;- nhts_trips trips ## # A tibble: 923,572 x 62 ## houseid personid tdtrpnum strttime endtime trvlcmin ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;dbl+lb&gt; ## 1 300000… 01 1 2017-10-10 10:00:00 2017-10-10 10:15:00 15 ## 2 300000… 01 2 2017-10-10 15:10:00 2017-10-10 15:30:00 20 ## 3 300000… 02 1 2017-10-10 07:00:00 2017-10-10 09:00:00 120 ## 4 300000… 02 2 2017-10-10 18:00:00 2017-10-10 20:30:00 150 ## 5 300000… 03 1 2017-10-10 08:45:00 2017-10-10 09:00:00 15 ## 6 300000… 03 2 2017-10-10 14:30:00 2017-10-10 14:45:00 15 ## 7 300000… 01 1 2017-10-10 11:15:00 2017-10-10 11:30:00 15 ## 8 300000… 01 2 2017-10-10 23:30:00 2017-10-10 23:40:00 10 ## 9 300000… 01 1 2017-10-10 05:50:00 2017-10-10 06:05:00 15 ## 10 300000… 01 2 2017-10-10 07:00:00 2017-10-10 07:15:00 15 ## # … with 923,562 more rows, and 56 more variables: trpmiles &lt;dbl+lbl&gt;, ## # trptrans &lt;chr+lbl&gt;, trpaccmp &lt;dbl+lbl&gt;, trphhacc &lt;dbl+lbl&gt;, ## # vehid &lt;chr+lbl&gt;, trwaittm &lt;dbl+lbl&gt;, numtrans &lt;dbl+lbl&gt;, tracctm &lt;dbl+lbl&gt;, ## # drop_prk &lt;chr+lbl&gt;, tregrtm &lt;dbl+lbl&gt;, whodrove &lt;chr+lbl&gt;, ## # whyfrom &lt;chr+lbl&gt;, loop_trip &lt;chr+lbl&gt;, trphhveh &lt;chr+lbl&gt;, ## # hhmemdrv &lt;chr+lbl&gt;, hh_ontd &lt;dbl+lbl&gt;, nonhhcnt &lt;dbl+lbl&gt;, ## # numontrp &lt;dbl+lbl&gt;, psgr_flg &lt;chr+lbl&gt;, pubtrans &lt;chr+lbl&gt;, ## # trippurp &lt;chr+lbl&gt;, dweltime &lt;dbl+lbl&gt;, tdwknd &lt;chr+lbl&gt;, ## # vmt_mile &lt;dbl+lbl&gt;, drvr_flg &lt;chr+lbl&gt;, whytrp1s &lt;chr+lbl&gt;, ## # ontd_p1 &lt;chr+lbl&gt;, ontd_p2 &lt;chr+lbl&gt;, ontd_p3 &lt;chr+lbl&gt;, ontd_p4 &lt;chr+lbl&gt;, ## # ontd_p5 &lt;chr+lbl&gt;, ontd_p6 &lt;chr+lbl&gt;, ontd_p7 &lt;chr+lbl&gt;, ontd_p8 &lt;chr+lbl&gt;, ## # ontd_p9 &lt;chr+lbl&gt;, ontd_p10 &lt;chr+lbl&gt;, ontd_p11 &lt;chr+lbl&gt;, ## # ontd_p12 &lt;chr+lbl&gt;, ontd_p13 &lt;chr+lbl&gt;, tdcaseid &lt;chr&gt;, ## # tracc_wlk &lt;chr+lbl&gt;, tracc_pov &lt;chr+lbl&gt;, tracc_bus &lt;chr+lbl&gt;, ## # tracc_crl &lt;chr+lbl&gt;, tracc_sub &lt;chr+lbl&gt;, tracc_oth &lt;chr+lbl&gt;, ## # tregr_wlk &lt;chr+lbl&gt;, tregr_pov &lt;chr+lbl&gt;, tregr_bus &lt;chr+lbl&gt;, ## # tregr_crl &lt;chr+lbl&gt;, tregr_sub &lt;chr+lbl&gt;, tregr_oth &lt;chr+lbl&gt;, ## # whyto &lt;chr+lbl&gt;, gasprice &lt;chr&gt;, wttrdfin &lt;dbl&gt;, whytrp90 &lt;chr+lbl&gt; B.4.2.1 Select, Filter, and Chains This table is pretty overwhelming. But there are two functions that can help us pare it down: select() lets you select columns in a table using the names of the columns. filter() lets you select rows in a table that meet a certain condition. Let’s practice this by selecting our trips dataset to only include the id columns, the trip length, and the trip purpose. select(trips, houseid, personid, trpmiles, trippurp) ## # A tibble: 923,572 x 4 ## houseid personid trpmiles trippurp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl+lbl&gt; &lt;chr+lbl&gt; ## 1 30000007 01 5.24 HBO [Home-based trip (other)] ## 2 30000007 01 5.15 HBO [Home-based trip (other)] ## 3 30000007 02 84.0 HBW [Home-based trip (work)] ## 4 30000007 02 81.6 HBW [Home-based trip (work)] ## 5 30000007 03 2.25 HBO [Home-based trip (other)] ## 6 30000007 03 2.24 HBO [Home-based trip (other)] ## 7 30000008 01 8.02 HBW [Home-based trip (work)] ## 8 30000008 01 8.02 HBW [Home-based trip (work)] ## 9 30000012 01 3.40 HBSOCREC [Home-based trip (social/recreational)] ## 10 30000012 01 3.40 HBSOCREC [Home-based trip (social/recreational)] ## # … with 923,562 more rows Let’s also practice filtering the trips dataset to only include trips of the purpose “HBO” (home-based other). Notice how the number of rows in the table trips is much smaller. filter(trips, trippurp == &quot;HBW&quot;) # use double equals as comparison ## # A tibble: 117,368 x 62 ## houseid personid tdtrpnum strttime endtime trvlcmin ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;dbl+lb&gt; ## 1 300000… 02 1 2017-10-10 07:00:00 2017-10-10 09:00:00 120 ## 2 300000… 02 2 2017-10-10 18:00:00 2017-10-10 20:30:00 150 ## 3 300000… 01 1 2017-10-10 11:15:00 2017-10-10 11:30:00 15 ## 4 300000… 01 2 2017-10-10 23:30:00 2017-10-10 23:40:00 10 ## 5 300000… 01 5 2017-10-10 09:00:00 2017-10-10 09:20:00 20 ## 6 300000… 01 7 2017-10-10 15:30:00 2017-10-10 16:05:00 35 ## 7 300000… 01 1 2017-10-10 08:00:00 2017-10-10 08:20:00 20 ## 8 300000… 01 2 2017-10-10 18:00:00 2017-10-10 20:00:00 120 ## 9 300000… 02 3 2017-10-10 09:00:00 2017-10-10 11:00:00 120 ## 10 300000… 02 4 2017-10-10 18:30:00 2017-10-10 20:30:00 120 ## # … with 117,358 more rows, and 56 more variables: trpmiles &lt;dbl+lbl&gt;, ## # trptrans &lt;chr+lbl&gt;, trpaccmp &lt;dbl+lbl&gt;, trphhacc &lt;dbl+lbl&gt;, ## # vehid &lt;chr+lbl&gt;, trwaittm &lt;dbl+lbl&gt;, numtrans &lt;dbl+lbl&gt;, tracctm &lt;dbl+lbl&gt;, ## # drop_prk &lt;chr+lbl&gt;, tregrtm &lt;dbl+lbl&gt;, whodrove &lt;chr+lbl&gt;, ## # whyfrom &lt;chr+lbl&gt;, loop_trip &lt;chr+lbl&gt;, trphhveh &lt;chr+lbl&gt;, ## # hhmemdrv &lt;chr+lbl&gt;, hh_ontd &lt;dbl+lbl&gt;, nonhhcnt &lt;dbl+lbl&gt;, ## # numontrp &lt;dbl+lbl&gt;, psgr_flg &lt;chr+lbl&gt;, pubtrans &lt;chr+lbl&gt;, ## # trippurp &lt;chr+lbl&gt;, dweltime &lt;dbl+lbl&gt;, tdwknd &lt;chr+lbl&gt;, ## # vmt_mile &lt;dbl+lbl&gt;, drvr_flg &lt;chr+lbl&gt;, whytrp1s &lt;chr+lbl&gt;, ## # ontd_p1 &lt;chr+lbl&gt;, ontd_p2 &lt;chr+lbl&gt;, ontd_p3 &lt;chr+lbl&gt;, ontd_p4 &lt;chr+lbl&gt;, ## # ontd_p5 &lt;chr+lbl&gt;, ontd_p6 &lt;chr+lbl&gt;, ontd_p7 &lt;chr+lbl&gt;, ontd_p8 &lt;chr+lbl&gt;, ## # ontd_p9 &lt;chr+lbl&gt;, ontd_p10 &lt;chr+lbl&gt;, ontd_p11 &lt;chr+lbl&gt;, ## # ontd_p12 &lt;chr+lbl&gt;, ontd_p13 &lt;chr+lbl&gt;, tdcaseid &lt;chr&gt;, ## # tracc_wlk &lt;chr+lbl&gt;, tracc_pov &lt;chr+lbl&gt;, tracc_bus &lt;chr+lbl&gt;, ## # tracc_crl &lt;chr+lbl&gt;, tracc_sub &lt;chr+lbl&gt;, tracc_oth &lt;chr+lbl&gt;, ## # tregr_wlk &lt;chr+lbl&gt;, tregr_pov &lt;chr+lbl&gt;, tregr_bus &lt;chr+lbl&gt;, ## # tregr_crl &lt;chr+lbl&gt;, tregr_sub &lt;chr+lbl&gt;, tregr_oth &lt;chr+lbl&gt;, ## # whyto &lt;chr+lbl&gt;, gasprice &lt;chr&gt;, wttrdfin &lt;dbl&gt;, whytrp90 &lt;chr+lbl&gt; One extremely useful feature of the tidyverse functions is the chain operator, %&gt;%. This operator basically does the opposite of the assigment operator &lt;-. While assignment says “take the thing on the right and put it in the thing on the left,” chain says “take the thing on the left and pass it as the first argument of the function on the right.” What this means in practice is we can chain R commands together. So we can do the select and the filter statements in sequence, trips %&gt;% select(houseid, personid, trpmiles, trippurp) %&gt;% filter(trippurp == &quot;HBW&quot;) ## # A tibble: 117,368 x 4 ## houseid personid trpmiles trippurp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl+lbl&gt; &lt;chr+lbl&gt; ## 1 30000007 02 84.0 HBW [Home-based trip (work)] ## 2 30000007 02 81.6 HBW [Home-based trip (work)] ## 3 30000008 01 8.02 HBW [Home-based trip (work)] ## 4 30000008 01 8.02 HBW [Home-based trip (work)] ## 5 30000012 01 4.29 HBW [Home-based trip (work)] ## 6 30000012 01 6.82 HBW [Home-based trip (work)] ## 7 30000039 01 11.5 HBW [Home-based trip (work)] ## 8 30000041 01 73.7 HBW [Home-based trip (work)] ## 9 30000041 02 77.9 HBW [Home-based trip (work)] ## 10 30000041 02 77.8 HBW [Home-based trip (work)] ## # … with 117,358 more rows Notice that we didn’t have to tell the select and filter functions the name of the table we were selecting or filtering. The %&gt;% chain operator did that for us. Once we have the table we want, we can assign it to a new object called mytrips In this case, let’s get HBO and HBW trips. mytrips &lt;- trips %&gt;% select(houseid, personid, trpmiles, trippurp) %&gt;% filter(trippurp %in% c(&quot;HBW&quot;, &quot;HBO&quot;)) # use %in% for multiple comparisons. B.4.3 Mutate, Summarize, and Group Sometimes we want to calculate a new column in a table, or recompute an existing column. We can do that with the mutate function, and we can put more than one calculation in a single mutate statement. mytrips %&gt;% mutate( tripkm = trpmiles * 1.60934, # convert miles to km. longtrip = ifelse(tripkm &gt; 50, TRUE, FALSE) # is trip longer than 50 km? ) ## # A tibble: 307,390 x 6 ## houseid personid trpmiles trippurp tripkm longtrip ## &lt;chr&gt; &lt;chr&gt; &lt;dbl+lbl&gt; &lt;chr+lbl&gt; &lt;dbl+lbl&gt; &lt;lgl&gt; ## 1 30000007 01 5.24 HBO [Home-based trip (other)] 8.44 FALSE ## 2 30000007 01 5.15 HBO [Home-based trip (other)] 8.29 FALSE ## 3 30000007 02 84.0 HBW [Home-based trip (work)] 135. TRUE ## 4 30000007 02 81.6 HBW [Home-based trip (work)] 131. TRUE ## 5 30000007 03 2.25 HBO [Home-based trip (other)] 3.62 FALSE ## 6 30000007 03 2.24 HBO [Home-based trip (other)] 3.61 FALSE ## 7 30000008 01 8.02 HBW [Home-based trip (work)] 12.9 FALSE ## 8 30000008 01 8.02 HBW [Home-based trip (work)] 12.9 FALSE ## 9 30000012 01 4.29 HBW [Home-based trip (work)] 6.91 FALSE ## 10 30000012 01 6.82 HBW [Home-based trip (work)] 11.0 FALSE ## # … with 307,380 more rows Other times we want to calculate summary statistics like means. For this we can use the summarize() function. mytrips %&gt;% summarize( mean_trip = mean(trpmiles), sd_trip = sd(trpmiles), max_trip = max(trpmiles), min_trip = min(trpmiles) ) ## # A tibble: 1 x 4 ## mean_trip sd_trip max_trip min_trip ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9.81 32.1 5699. -9 Finally, we sometimes want to calculate summary statistics for different groups. We can tell tidyverse to group our tables with the group_by() function. mytrips %&gt;% group_by(trippurp) %&gt;% summarize( mean_trip = mean(trpmiles), sd_trip = sd(trpmiles), max_trip = max(trpmiles), min_trip = min(trpmiles) ) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 5 ## trippurp mean_trip sd_trip max_trip min_trip ## &lt;chr+lbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 HBO [Home-based trip (other)] 7.73 32.0 5699. -9 ## 2 HBW [Home-based trip (work)] 13.2 31.9 2927. -9 As you might expect, work trips are on average longer than other kinds of trips. But some people report very long trips! You might want to filter your data more carefully for real analyses. B.5 Graphics with ggplot2 The ggplot2 package included in the tidyverse is a very powerful graphics engine with a relatively easy-to-learn grammar. In fact, the gg stands for “grammar of graphics” as it implements the grammar defined by Wilkinson (2012). The basic structure of a ggplot2 call is constructed as follows: ggplot(data, aes(data aesthetics like x and y coordinates, fill color, etc.)) + geom_(geometry style like point, bar, or histogram) + other things like theme, color, and labels For instance, we can create a histogram of trip lengths in the NHTS by giving the x aesthetic as the trpmiles column in the mytrips dataset. ggplot(mytrips, aes(x = trpmiles)) + geom_histogram() ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. This ends up not being very informative because some trips are very long. We could filter out the long trips within the data argument (Note that we still have the -9 values from the missing information). ggplot(mytrips %&gt;% filter(trpmiles &lt; 50), aes(x = trpmiles)) + geom_histogram() ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. If we wanted to see the difference between lengths of different trip purposes, we could add a color aesthetic to the plot. By default this stacks the two categories on top of each other. ggplot(mytrips %&gt;% filter(trpmiles &lt; 50), aes(x = trpmiles, fill = factor(trippurp))) + geom_histogram() ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. You could also show this with a statistical density (the integral of a density function is 1). Note that the alpha statement for fill opacity is not included as an aesthetic, because it doesn’t vary based on any data elements in the way that the x and fill variables do. ggplot(mytrips %&gt;% filter(trpmiles &lt; 50), aes(x = trpmiles, fill = factor(trippurp))) + geom_density(alpha = 0.5) ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. ggplot2 also excels at building statistical analysis on top of visualization. For example, we can see the odometer reading for cars still on the road in 2017 by make. set.seed(15) # so that we pull the same random records each time # sample 15k vehicles built after 1980 with 0 to 500k miles vehicles &lt;- nhts_vehicles %&gt;% # convert numeric make to its labeled name, and then group into manufacturers mutate( make = as_factor(make, levels = &quot;labels&quot;), vehtype = as_factor(vehtype, levels = &quot;labels&quot;), make = case_when( make %in% c(&quot;Toyota&quot;, &quot;Lexus&quot;, &quot;Subaru&quot;) ~ &quot;Toyota&quot;, make %in% c(&quot;Ford&quot;, &quot;Lincoln&quot;, &quot;Mercury&quot;) ~ &quot;Ford&quot;, make %in% c(&quot;Chevrolet&quot;, &quot;GMC&quot;, &quot;Pontiac&quot;, &quot;Buick&quot;, &quot;Cadillac&quot;, &quot;Saturn&quot;) ~ &quot;GM&quot;, make %in% c(&quot;Volkswagen&quot;, &quot;Audi&quot;, &quot;Porsche&quot;) ~ &quot;VW&quot;, grepl(&quot;Jeep&quot;, make) | grepl(&quot;Chrysler&quot;, make) | make %in% c(&quot;Ram&quot;, &quot;Dodge&quot;, &quot;Plymouth&quot;) ~ &quot;Chrysler&quot;, make %in% c(&quot;Honda&quot;, &quot;Acura&quot;) ~ &quot;Honda&quot;, make %in% c(&quot;Nissan/Datsun&quot;, &quot;Infiniti&quot;) ~ &quot;Nissan&quot;, TRUE ~ &quot;Other&quot; # all other makes ) , vehtype = case_when( grepl(&quot;Car&quot;, vehtype) ~ &quot;Car&quot;, grepl(&quot;Van&quot;, vehtype) ~ &quot;Van&quot;, grepl(&quot;SUV&quot;, vehtype) ~ &quot;SUV&quot;, grepl(&quot;Pickup&quot;, vehtype) ~ &quot;Pickup&quot;, TRUE ~ &quot;Other&quot;, ) ) %&gt;% filter(vehtype != &quot;Other&quot;) %&gt;% filter(vehyear &gt; 1980) %&gt;% filter(od_read &gt; 0, od_read &lt; 500000) %&gt;% sample_n(15000) ggplot(vehicles, aes(x = vehyear, y = od_read, color = make)) + geom_point() ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. This is pretty unreadable. But we can add a few things to the figure to make it a little bit easier to understand, like smooth average lines and point transparency. ggplot(vehicles, aes(x = vehyear, y = od_read, color = make)) + geom_point(alpha = 0.5) + stat_smooth(method = &quot;loess&quot;) ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. ## `geom_smooth()` using formula &#39;y ~ x&#39; Let’s break this out by vehicle type. ggplot(vehicles, aes(x = vehyear, y = od_read, color = make)) + geom_point(alpha = 0.5) + stat_smooth(method = &quot;loess&quot;) + facet_wrap(~vehtype) ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. ## `geom_smooth()` using formula &#39;y ~ x&#39; And let’s clean it up a little bit. This is a figure that you could put in a published journal article or thesis, if it showed something you cared to show. ggplot(vehicles, aes(x = vehyear, y = od_read, color = make)) + geom_point(alpha = 0.5) + scale_color_discrete(&quot;Manufacturer&quot;) + stat_smooth(method = &quot;loess&quot;) + facet_wrap(~vehtype) + xlab(&quot;Vehicle Model Year&quot;) + ylab(&quot;Odometer Reading&quot;) + theme_bw() ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. ## Don&#39;t know how to automatically pick scale for object of type haven_labelled. Defaulting to continuous. ## `geom_smooth()` using formula &#39;y ~ x&#39; "]
]
